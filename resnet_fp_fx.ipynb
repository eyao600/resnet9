{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eyao600/resnet9/blob/main/resnet_fp_fx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6ikW2JiYwp0"
      },
      "source": [
        "##The Resnet Research paper can be accessed from here https://arxiv.org/pdf/1512.03385v1.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V6WeCzrhCPQ"
      },
      "source": [
        "# **Checking availability of GPU. Google Colab provides a NVIDIA Tesla\n",
        "\n",
        "K80 GPU for free for 12 hours.**\n",
        "\n",
        "##Some Features of Tesla K80\n",
        "\n",
        "1.4992 NVIDIA CUDA cores with a dual-GPU design\n",
        "\n",
        "2.Up to 2.91 teraflops double-precision performance with NVIDIA GPU Boost\n",
        "\n",
        "3.Up to 8.73 teraflops single-precision performance with NVIDIA GPU Boost\n",
        "\n",
        "3.24 GB of GDDR5 memory\n",
        "\n",
        "4.480 GB/s aggregate memory bandwidth\n",
        "\n",
        "5.ECC protection for increased reliability\n",
        "\n",
        "6.Server-optimised to deliver the best throughput in the data center\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W--FTN0j2VB"
      },
      "source": [
        "#**CUDA®**\n",
        "CUDA® is a parallel computing platform and programming model developed by NVIDIA for general computing on graphical processing units (GPUs). With CUDA, developers are able to dramatically speed up computing applications by harnessing the power of GPUs.\n",
        "\n",
        "In GPU-accelerated applications, the sequential part of the workload runs on the CPU – which is optimized for single-threaded performance – while the compute intensive portion of the application runs on thousands of GPU cores in parallel. When using CUDA, developers program in popular languages such as C, C++, Fortran, Python and MATLAB and express parallelism through extensions in the form of a few basic keywords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OI_L93eYy-D",
        "outputId": "b0ca75e5-aecd-4481-c19e-8205e42c9b9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "print(torch.__version__)\n",
        "\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "  print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "  print('CUDA is available!  Training on GPU ...')\n",
        "\n",
        "if torch.cuda.device_count() >= 1:\n",
        "  print(f'Using {torch.cuda.device_count()} GPUs ...')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n",
            "CUDA is not available.  Training on CPU ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVwz1WydnN_7"
      },
      "source": [
        "#**Downloading the CIFAR10 datset and loading the data in Normalized form as torch.FloatTensor datatype and generating a validation set by dividing the training set in 80-20 ratio**\n",
        "#**CIFAR10**\n",
        "The CIFAR10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.\n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
        "\n",
        "Here are the classes in the dataset:\n",
        "1. airplane\n",
        "2. automobile\n",
        "3. bird\n",
        "4. cat\n",
        "5. deer\n",
        "6. dog\n",
        "7. frog\n",
        "8. horse\n",
        "9. ship\n",
        "10. truck\n",
        "\n",
        "The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks.\n",
        "\n",
        "More can be read from their page at https://www.cs.toronto.edu/~kriz/cifar.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U2Vn2qhvrIi"
      },
      "source": [
        "#**Image Augmentation**\n",
        "In this cell, we perform some simple data augmentation by randomly flipping and cropping the given image data. We do this by defining a torchvision transform, and you can learn about all the transforms that are used to pre-process and augment data from the [PyTorch documentation](https://pytorch.org/docs/stable/torchvision/transforms.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5WmV1je_1kr",
        "outputId": "32efe083-9a8e-40d9-9ecb-bce1634a095f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 20\n",
        "# percentage of training set to use as validation\n",
        "valid_size = 0.2\n",
        "\n",
        "# convert data to a normalized torch.FloatTensor\n",
        "print('==> Preparing data..')\n",
        "#Image augmentation is used to train the model\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "#Only the data is normalized we do not need to augment the test data\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# choose the training and test datasets\n",
        "train_data = datasets.CIFAR10('data', train=True,\n",
        "                              download=True, transform=transform_train)\n",
        "test_data = datasets.CIFAR10('data', train=False,\n",
        "                             download=True, transform=transform_test)\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# prepare data loaders (combine dataset and sampler)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "    sampler=train_sampler, num_workers=num_workers)\n",
        "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "    sampler=valid_sampler, num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n",
        "    num_workers=num_workers, shuffle=False)\n",
        "\n",
        "# specify the image classes\n",
        "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck']\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 29.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdIarqe4tnBs"
      },
      "source": [
        "#**Visualizing the Data**\n",
        "Obtaining a batch of training data and plot the same with its lables using matplotlib library. You can also see how the transformations which you applied in the previous step show up in these visualizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhhpI2ntAB8f",
        "outputId": "31891bb7-ff07-470e-e0c5-744d7d4ae7c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# helper function to un-normalize and display an image\n",
        "def imshow(img):\n",
        "  img = img / 2 + 0.5  # unnormalize\n",
        "  plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\n",
        "\n",
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "images = images.numpy() # convert images to numpy for display\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "# display 20 images\n",
        "for idx in np.arange(20):\n",
        "  ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
        "  imshow(images[idx])\n",
        "  ax.set_title(classes[labels[idx]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.71453285..1.2427812].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.71453285..1.3891084].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.71453285..1.7885619].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.71453285..1.8183348].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.71453285..1.7279667].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.71453285..1.4565778].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.71453285..1.6915178].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.71453285..1.5354356].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.71453285..1.6795044].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.71453285..1.5728873].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.71453285..1.8768656].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.71453285..1.7492281].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.71453285..1.5549458].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.71453285..1.7110282].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.71453285..1.4371929].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.71453285..1.6915178].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.71453285..1.8768656].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.71453285..1.8768656].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.71453285..1.7988244].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.71453285..1.6915178].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2500x400 with 20 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAB40AAAFeCAYAAACRuIkTAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzs3Xl8VOX1P/CTGYaZDDMMCUNCCISJMRADFAQEQWRRccdSBVy+VnDD3dpqW1t/FcSt1bba6letXdSq9aug1VoVBUURpQiyFIiRGAiBGBJD4pAwmWEyc39/ICHnnAcyCXvm8369+qrnznOXuXOf7d5wT4plWRYBAAAAAAAAAAAAAAAAAEBSsh3pAwAAAAAAAAAAAAAAAAAAgCMHD40BAAAAAAAAAAAAAAAAAJIYHhoDAAAAAAAAAAAAAAAAACQxPDQGAAAAAAAAAAAAAAAAAEhieGgMAAAAAAAAAAAAAAAAAJDE8NAYAAAAAAAAAAAAAAAAACCJ4aExAAAAAAAAAAAAAAAAAEASw0NjAAAAAAAAAAAAAAAAAIAkhofGAAAAAAAAAAAAAAAAAABJDA+ND7PZs2dTSkoK1dTUHOlDgST24YcfUkpKCn344YfHxHYBAODwQj8BcOwJBAJ0/vnnt1rOVA9nzJhBgUDg0B0cJKXly5fT6NGjqUuXLpSSkkKrV68+0ocEcNQaP348DRw4sNVyZWVllJKSQs8+++yhPyiADgL3YqEj2HMdA3Q0aKOPPkn30PjTTz+l2bNn07fffnukDwUAAJLc119/TbNnz8ZNVACAo9QTTzyBG/MA7RCNRmnq1KlUW1tLjzzyCD3//PPUt2/fI31YAABwBOBeLAAAwLGj05E+gMPt008/pXvuuYdmzJhB3bp1O9KHA3BEjB07lhobG6lz585H+lAAktrXX39N99xzDwUCARoyZMiRPhyAZugnAHZ74oknyO/304wZM470oRw0qN9wOJSWltLmzZvpz3/+M11zzTVH+nAAOoy+fftSY2MjORyOI30oAAnDvVgAAIBjR9L9S+NExeNxCofDR/owAA4Jm81GLpeLbLb9NwGhUOgwHREAABxN0E8AdFyJ1m+AA1FdXU1E1OrDgZ07dx6GowHoOFJSUsjlcpHdbj/ShwJw0OFeLAAAHGyWZVFjY+ORPoxjSlLdKZg9ezb99Kc/JSKi3NxcSklJoZSUlOacMDfffDO9+OKLNGDAAHI6nTR//vx95t7bVx6Z4uJimjZtGvXo0YNSU1Opf//+dNddd+33uDZv3kzHH388DRw4kKqqqg7mV4Yks3nzZrrxxhupf//+lJqaSt27d6epU6dSWVkZK2e6rvfkUPr8889p7Nix5Ha76Ze//CUR7c2R995779GQIUPI5XJRYWEhvfbaa60e08cff0xTp06lnJwccjqd1KdPH/rxj3+sGusZM2aQx+OhiooKmjx5Mnk8HurRowfdcccdFIvFWNl4PE6PPvooDRgwgFwuF2VmZtJ1111HdXV17TtxAO1QUVFBV199NfXq1YucTifl5ubSDTfcQLt27aLa2lq64447aNCgQeTxeKhr1650zjnn0Jo1a5rX//DDD+mkk04iIqIrr7yyuU/Ca1DhUEI/gX6iI0v0+t5XPrBnn322eW5AtPu6Xr9+PX300UfNbfT48eOby2/cuJGmTp1K6enp5Ha76eSTT6a33nqLbXNPXXrllVfonnvuoezsbPJ6vTRlyhQKBoMUiUTotttuo4yMDPJ4PHTllVdSJBJh22hqaqJ7772X8vLyyOl0UiAQoF/+8peq3B6t1cNEc4ujHkF7zZgxg8aNG0dERFOnTm2uO3va8dLSUjr33HPJ6/XS//zP/xDR7ofHt99+O/Xp04ecTif179+ffvvb35JlWWzbjY2NdOutt5Lf7yev10sXXHABVVRUUEpKCs2ePftwf1WAhNXX19Ntt91GgUCAnE4nZWRk0MSJE2nlypWsXFFREU2YMIHcbjdlZ2fTQw89xD433YvaU7c2btxIZ511FnXp0oV69epFc+bMUXUI4HDDvViAtluyZAmddNJJ5HK5KC8vj/70pz+pMonOEeLxOM2ePZt69epFbrebJkyYQEVFRRQIBDrU25Tg2Pftt982v5HC5/PRlVdeyf6hQqLX/J77U++++y4NHz6cUlNTm+vQggULaMyYMdStWzfyeDzUv3//5vtae0QiEZo1axYdf/zxzfeofvazn+1z/t0RJdXrqS+88ELasGEDvfTSS/TII4+Q3+8nIqIePXoQEdEHH3xAr7zyCt18883k9/spEAi0Kd/Gf//7Xzr11FPJ4XDQzJkzKRAIUGlpKb355pt0//33G9cpLS2l0047jdLT02nBggXNxwTQHsuXL6dPP/2ULrnkEurduzeVlZXRk08+SePHj6eioiJyu937XX/79u10zjnn0CWXXEKXX345ZWZmNn9WUlJCF198MV1//fU0ffp0euaZZ2jq1Kk0f/58mjhx4j63OXfuXAqFQnTDDTdQ9+7d6bPPPqPHHnuMtm7dSnPnzmVlY7EYnXXWWTRy5Ej67W9/SwsXLqTf/e53lJeXRzfccENzueuuu46effZZuvLKK+nWW2+lTZs20eOPP06rVq2iTz75BK/qgkPu66+/phEjRtC3335LM2fOpIKCAqqoqKB58+ZRKBSijRs30uuvv05Tp06l3Nxcqqqqoj/96U80btw4Kioqol69etEJJ5xAc+bMobvvvptmzpxJp556KhERjR49+gh/O+jI0E+gn+jIDvT6lh599FG65ZZbyOPxNN943HPNV1VV0ejRoykUCtGtt95K3bt3p+eee44uuOACmjdvHv3gBz9g23rwwQcpNTWV7rzzTvrqq6/oscceI4fDQTabjerq6mj27Nn0n//8h5599lnKzc2lu+++u3nda665hp577jmaMmUK3X777bRs2TJ68MEH6YsvvqB//vOfbD/trYcmqEfQXtdddx1lZ2fTAw88QLfeeiuddNJJlJmZSS+++CI1NTXRWWedRWPGjKHf/va35Ha7ybIsuuCCC2jRokV09dVX05AhQ+jdd9+ln/70p1RRUUGPPPJI87ZnzJhBr7zyCv3whz+kk08+mT766CM677zzjuC3BUjM9ddfT/PmzaObb76ZCgsLafv27bRkyRL64osvaOjQoUREVFdXR2effTZdeOGFNG3aNJo3bx79/Oc/p0GDBtE555yz3+3HYjE6++yz6eSTT6aHHnqI5s+fT7NmzaKmpiaaM2fO4fiKAEa4FwvQNmvXrqUzzzyTevToQbNnz6ampiaaNWsWm3sTJT5H+MUvfkEPPfQQTZo0ic466yxas2YNnXXWWfhX/XDUmTZtGuXm5tKDDz5IK1eupL/85S+UkZFBv/nNb4iobfPiL7/8ki699FK67rrr6Nprr6X+/fvT+vXr6fzzz6fvfe97NGfOHHI6nfTVV1/RJ5980rxePB6nCy64gJYsWUIzZ86kE044gdauXUuPPPIIbdiwgV5//fXDeUqOHCvJPPzwwxYRWZs2bWLLiciy2WzW+vXr2fJFixZZRGQtWrSILd+0aZNFRNYzzzzTvGzs2LGW1+u1Nm/ezMrG4/Hm/541a5ZFRNY333xjffHFF1avXr2sk046yaqtrT0o3w+SWygUUsuWLl1qEZH197//vXmZ6boeN26cRUTWU089pbbRt29fi4isV199tXlZMBi0srKyrBNPPHG/2zUd04MPPmilpKSwujJ9+nSLiKw5c+awsieeeKI1bNiw5vjjjz+2iMh68cUXWbn58+cblwMcCldccYVls9ms5cuXq8/i8bgVDoetWCzGlm/atMlyOp3sGl++fLnqSwAOJfQT6Cc6skSv7z3jcemZZ55R84QBAwZY48aNU2Vvu+02i4isjz/+uHlZfX29lZubawUCgeY+YM81P3DgQGvXrl3NZS+99FIrJSXFOuecc9h2R40aZfXt27c5Xr16tUVE1jXXXMPK3XHHHRYRWR988EHzsgOph9OnT2f7RT2CA7XnOps7d27zsj3t+J133snKvv766xYRWffddx9bPmXKFCslJcX66quvLMuyrM8//9wiIuu2225j5WbMmGERkTVr1qxD82UADgKfz2fddNNN+/x8zzirZX8ViUSsnj17WhdddFHzMtO9qD1165ZbbmleFo/HrfPOO8/q3Lmz9c033xzcLwPQRrgXC5C4yZMnWy6Xi13TRUVFlt1ub57DJDpH2LZtm9WpUydr8uTJrNzs2bMtIrKmT59+aL8MQAL2tNFXXXUVW/6DH/zA6t69u2VZ7ZsXz58/n5V95JFHmvuCfXn++ectm83G5vmWZVlPPfWURUTWJ5980q7veKxJqtdTt2bcuHFUWFjYrnW/+eYbWrx4MV111VWUk5PDPjO9/m7dunU0btw4CgQCtHDhQkpLS2vXfgFaSk1Nbf7vaDRK27dvp+OPP566deumXntl4nQ66corrzR+1qtXL/avZrp27UpXXHEFrVq1irZt25bQMe3cuZNqampo9OjRZFkWrVq1SpW//vrrWXzqqafSxo0bm+O5c+eSz+ejiRMnUk1NTfP/hg0bRh6PhxYtWtTq9wQ4EPF4nF5//XWaNGkSDR8+XH2ekpJCTqezOVdkLBaj7du3N7/2JJG6CHCooJ9AP9GRHej13RZvv/02jRgxgsaMGdO8zOPx0MyZM6msrIyKiopY+SuuuIL9y9yRI0eSZVl01VVXsXIjR46kLVu2UFNTU/N+iIh+8pOfsHK33347EZF6HXZ766GEegSHUss3QxDtvs7tdjvdeuutbPntt99OlmXRO++8Q0RE8+fPJyKiG2+8kZW75ZZbDuHRAhwc3bp1o2XLltHXX3+9zzIej4cuv/zy5rhz5840YsQINs7Zn5tvvrn5v/e89nfXrl20cOHC9h84wCGGe7EAe8ViMXr33Xdp8uTJ7Jo+4YQT6KyzzmqOE50jvP/++9TU1ISxExwTTPd6tm/fTjt27GjzvDg3N5fVGaLdYzEiojfeeIPi8bjxGObOnUsnnHACFRQUsHnwaaedRkSUNPNgPDRuITc3t93r7hnEDxw4MKHykyZNIq/XS++++y517dq13fsFaKmxsZHuvvvu5lxgfr+fevToQd9++y0Fg8FW18/OzqbOnTsbPzv++OPVoLtfv35ERCpXYEvl5eU0Y8YMSk9Pb84/uSfHmTwml8vV/IqiPdLS0ljuvJKSEgoGg5SRkUE9evRg/2toaKDq6upWvyfAgfjmm29ox44d+23v4/E4PfLII5Sfn8/q4n//+9+E6iLAoYJ+Av1ER3ag13dbbN68mfr376+Wn3DCCc2ftyRvZPp8PiIi6tOnj1oej8ebj3fz5s1ks9no+OOPZ+V69uxJ3bp1U/tpbz2UUI/gUOnUqRP17t2bLdu8eTP16tWLvF4vWy7r0576IOftsn4AHI0eeughWrduHfXp04dGjBhBs2fPVg+De/furdpwOc7ZF5vNRscddxxb1p72H+Bww71YgL2++eYbamxspPz8fPVZy7lHonOEPf8vy6Wnp+OPJuCoI+fMe67Rurq6Ns+LTX3LxRdfTKeccgpdc801lJmZSZdccgm98sor7AFySUkJrV+/Xs2B94ypkmUenFQ5jVvT8l8n7GH6yzSi3X/5cyAuuugieu655+jFF1+k66677oC2BbDHLbfcQs888wzddtttNGrUKPL5fJSSkkKXXHLJPv+CpiVTHTgQsViMJk6cSLW1tfTzn/+cCgoKqEuXLlRRUUEzZsxQx2S321vdZjwep4yMDHrxxReNn8uHCQBHwgMPPEC/+tWv6KqrrqJ7772X0tPTyWaz0W233ZZQXQQ4VNBPoJ/oyBK9vg/V+H5/9nXt7mu5ZVks3tcxHyqoR3CotHwbC0AymTZtGp166qn0z3/+k9577z16+OGH6Te/+Q299tprzfmKE+0TADoS3IsFaL/DPUcAOJQSGQcles2b+pbU1FRavHgxLVq0iN566y2aP38+vfzyy3TaaafRe++9R3a7neLxOA0aNIh+//vfG7cr/+i7o0q6h8ZtbUz3/EXDt99+y5bLv17Y8xed69atS2i7Dz/8MHXq1IluvPFG8nq9dNlll7XpuABM5s2bR9OnT6ff/e53zcvC4bC6ftvjq6++IsuyWB3asGEDEREFAgHjOmvXrqUNGzbQc889R1dccUXz8gULFrT7OPLy8mjhwoV0yimnHPSHFwCJ6NGjB3Xt2nW/7f28efNowoQJ9Ne//pUt//bbb8nv9zfHGODD4YZ+AjqyRK/vluP7Pa+oItLje6J9t9N9+/alL7/8Ui0vLi5u/vxg6Nu3L8XjcSopKWn+V5dERFVVVfTtt9+q/bSnHpqgHsHh1LdvX1q4cCHV19ezf20s69Oe+rBp0yb2L3C++uqrw3vAAO2UlZVFN954I914441UXV1NQ4cOpfvvv7/5ofGBiMfjtHHjxuZ/CUPUvvYf4FDAvViAxPTo0YNSU1OppKREfdZy7pHoHGHP/3/11VfsX15u3749obdYABwt2jov3hebzUann346nX766fT73/+eHnjgAbrrrrto0aJFdMYZZ1BeXh6tWbOGTj/99KS+Z5t0f+LbpUsXItIDj33p27cv2e12Wrx4MVv+xBNPsLhHjx40duxY+tvf/kbl5eXsM9NfhaakpNDTTz9NU6ZMoenTp9O//vWvNnwLADO73a6ut8cee+yg/MuZr7/+mv75z382xzt27KC///3vNGTIEOrZs+c+j4eI1wHLsugPf/hDu49j2rRpFIvF6N5771WfNTU1HZQHHwD7Y7PZaPLkyfTmm2/SihUr1OeWZRnr4ty5c6miooIta2ufBHCg0E+gn+jIEr2+8/LyiIjY+H7nzp303HPPqW126dLFeM2ce+659Nlnn9HSpUvZNp5++mkKBALtzs1n2g8R0aOPPsqW7/nL5/POO48tb089NEE9gsPp3HPPpVgsRo8//jhb/sgjj1BKSkrzA7U9ecnkXPyxxx47PAcK0E6xWEylScjIyKBevXpRJBI5aPtpWYcsy6LHH3+cHA4HnX766QdtHwDtgXuxAImx2+101lln0euvv86u6S+++ILefffd5jjROcLpp59OnTp1oieffJKVk2MugKNdW+fFJrW1tWrZkCFDiIiax2PTpk2jiooK+vOf/6zKNjY20s6dO9ty2MespPuXxsOGDSMiorvuuosuueQScjgcNGnSpH2W9/l8NHXqVHrssccoJSWF8vLy6N///rfx/eV//OMfacyYMTR06FCaOXMm5ebmUllZGb311lu0evVqVd5ms9ELL7xAkydPpmnTptHbb7/dnFQboD3OP/98ev7558nn81FhYSEtXbqUFi5cSN27dz/gbffr14+uvvpqWr58OWVmZtLf/vY3qqqqomeeeWaf6xQUFFBeXh7dcccdVFFRQV27dqVXX331gP6abdy4cXTdddfRgw8+SKtXr6YzzzyTHA4HlZSU0Ny5c+kPf/gDTZkypd3bB0jEAw88QO+99x6NGzeOZs6cSSeccAJVVlbS3LlzacmSJXT++efTnDlz6Morr6TRo0fT2rVr6cUXX1R5xvLy8qhbt2701FNPkdfrpS5dutDIkSMPKK8TwP6gn0A/0ZElen2feeaZlJOTQ1dffTX99Kc/JbvdTn/729+oR48e6objsGHD6Mknn6T77ruPjj/+eMrIyKDTTjuN7rzzTnrppZfonHPOoVtvvZXS09Ppueeeo02bNtGrr7560F6/O3jwYJo+fTo9/fTT9O2339K4cePos88+o+eee44mT55MEyZMYOXbUw9NUI/gcJo0aRJNmDCB7rrrLiorK6PBgwfTe++9R2+88QbddtttzX/oMWzYMLrooovo0Ucfpe3bt9PJJ59MH330UfO/pkzmfw0AR7f6+nrq3bs3TZkyhQYPHkwej4cWLlxIy5cvZ2/HOBAul4vmz59P06dPp5EjR9I777xDb731Fv3yl79ESgE44nAvFiBx99xzD82fP59OPfVUuvHGG6mpqYkee+wxGjBgAP33v/8losTnCJmZmfSjH/2Ifve739EFF1xAZ599Nq1Zs4beeecd8vv9GDvBMaOt82KTOXPm0OLFi+m8886jvn37UnV1NT3xxBPUu3dvGjNmDBER/fCHP6RXXnmFrr/+elq0aBGdcsopFIvFqLi4mF555RV69913afjw4Yf66x55VhK69957rezsbMtms1lEZG3atMkiIuumm24ylv/mm2+siy66yHK73VZaWpp13XXXWevWrbOIyHrmmWdY2XXr1lk/+MEPrG7dulkul8vq37+/9atf/ar581mzZllEZH3zzTfNy0KhkDVu3DjL4/FY//nPfw7Jd4bkUFdXZ1155ZWW3++3PB6PddZZZ1nFxcVW3759renTpzeXW7RokUVE1qJFi5qXjRs3zhowYIBxu3379rXOO+88691337W+973vWU6n0yooKLDmzp3Lypm2W1RUZJ1xxhmWx+Ox/H6/de2111pr1qxR9Wf69OlWly5d1L731Bnp6aeftoYNG2alpqZaXq/XGjRokPWzn/3M+vrrrxM7WQAHaPPmzdYVV1xh9ejRw3I6ndZxxx1n3XTTTVYkErHC4bB1++23W1lZWVZqaqp1yimnWEuXLrXGjRtnjRs3jm3njTfesAoLC61OnToZ+xWAgwn9BPqJjizR69uyLOvzzz+3Ro4caXXu3NnKycmxfv/731vPPPNM89xgj23btlnnnXee5fV6LSJibXhpaak1ZcqU5nH/iBEjrH//+99sP3uueVkX9uxr+fLlbLlprhCNRq177rnHys3NtRwOh9WnTx/rF7/4hRUOh9m6B1IPp0+fbvXt21edU9QjaC/Ttb+vdtyyLKu+vt768Y9/bPXq1ctyOBxWfn6+9fDDD1vxeJyV27lzp3XTTTdZ6enplsfjsSZPnmx9+eWXFhFZv/71rw/pdwJor0gkYv30pz+1Bg8ebHm9XqtLly7W4MGDrSeeeKK5zL7GWbJ93nP/yjRGKi0ttc4880zL7XZbmZmZ1qxZs6xYLHYovxpAwnAvFiBxH330kTVs2DCrc+fO1nHHHWc99dRTat6b6ByhqanJ+tWvfmX17NnTSk1NtU477TTriy++sLp3725df/31h/urASimNtqyLDU/b+u8WHr//fet73//+1avXr2szp07W7169bIuvfRSa8OGDazcrl27rN/85jfWgAEDLKfTaaWlpVnDhg2z7rnnHisYDB7cL3+USrEsw/s6AABaCAQCNHDgQPr3v/99pA8FAACOQugnAADgSFm9ejWdeOKJ9MILL9D//M//HOnDATjsZsyYQfPmzaOGhoYjfSgAAHAM+PbbbyktLY3uu+8+uuuuu4704QDAUSbpchoDAAAAAAAAwLGnsbFRLXv00UfJZrPR2LFjj8ARAQAAABy99jV2IiIaP3784T0YADgmJF1OYwAAAAAAAAA49jz00EP0+eef04QJE6hTp070zjvv0DvvvEMzZ86kPn36HOnDAwAAADiqvPzyy/Tss8/SueeeSx6Ph5YsWUIvvfQSnXnmmXTKKacc6cMDgKMQHhoDAAAAAAAAwFFv9OjRtGDBArr33nupoaGBcnJyaPbs2Xi1IgAAAIDB9773PerUqRM99NBDtGPHDsrMzKQf/ehHdN999x3pQwOAoxRyGgMAAAAAAAAAAAAAAAAAJDHkNAYAAAAAAAAAAAAAAAAASGJ4aAwAAAAAAAAAAAAAAAAAkMQSymkcj8fp66+/Jq/XSykpKYf6mOAYZ1kW1dfXU69evchm65h/l4A6AW2BOgHAoU4AcKgTABzqBACHOgHAoU4AcKgTABzqBADXljqR0EPjr7/+mvr06XNQDg6Sx5YtW6h3795H+jAOCdQJaA/UCQAOdQKAQ50A4FAnADjUCQAOdQKAQ50A4FAnALhE6kRCD429Xm9CO/R7PSyuqW9IaL0DddL3Bqtl6/67hsUOSmVx4bCRap0hp4xmcZ9+A1SZMy85n8WfrfiCxS/OeUStU7xyLT+WrvxYemb3VOvsCO5kcSweUmXKv/6axZ2c/Pw31RerdeQbyXsfx7/j1o2b1RrX33wni2tj+lheefI+tSzR6+ZY1JG/Gxw6Hfm66cjfDQ6djnzd7PluW7Zsoa5dux7ho4Gj3Y4dO6hPnz5JUScA2qIjXzd7vtu8j/5Mbo+biIh6985W5XaU17B40fP/UmWmXHQZi3sNGSVKlBuOwMGiKG0zlMgVS5pE7CHNZVjGbad1fN9ltSzuGRjb6jYSsWLFK2pZ/nF9WexLz2RxONyo1vnPvz9icW4gX5XZurGKxZ+vXc/iqCOi1on25efvwVv1vQQpGeoEQFt05OumI383OHQ68nVzsL5bMBg8KNs5uoX1ou38WQml5vHY7T90h3MEYI4NYJbIdZPQQ+NE/3m77Qj9M/hOdrtaJo8kRSzpZNdf3enkE1xXqluV8Ygbv6ld+ESvU6fOah1bCj8+m43v227nE3YiIps4Psv0U6XwB8ApKfo8GFYS+5Hr6N9QnpfOsXgC+0n8ujkWdeTvBodOR75uOvJ3g0OnI183e75b165d8dAYEpYMdQKgLTrydbPnu7k9bury3UNjb1f9EDbu4X+w63Lq+abX04XFut8x3RiQD431H3w7SG4nmsB2W39ovEs8bI56+QPVg9Vvdumi5/Ndu8pzxb9D5856Tt3Fzb+Tt0sXQxn+h+Eup5PFdoel1rGnOtWy1iRDnQBoi4583XTk7waHTke+bg7Wd0uO+bkeL9IuMX5xi3Gcu2OeF9QJAC6R66ZjvtAdAAAAAAAAAAAAAAAAAAASktC/NDYx/U1xXkEhi6s/W9bezbdJVL8tmfQivmTJZwtViWf/MZfFEYf+K+kH7/0ji99f+D6LvUH9+oeaRv5KMWrkf0lduc10nvhrsrIDObpIhP81+Ckj+eu1P1zMX4lFRJRG/K+Z/W7+V96V6q/HicrW89eH+fsF9LEAABBRMHgdde26p50pMpQ4V8Ty9YOmf52iUwW01R/++JBa9v47n7C4vp63fzvqKtU6oSh/fWJVTbUqU1dneA1QGznEeYiSoQ+gDQe8H3vPcWpZLMS/IzWIHjWu/9UO0VrDMgAAAGhN3EYU/+4ft3Z16T62T34Bi4Nj5KunTS8g3ClinypRT3wMUxHarsoUuGWuK3l8rf+rYpMI8TdX9czV6ZoOBnuG4W/k0+UyEbv0OqdfcDaLozX1qsyKZStZHHLE+Dpufa7WbijRxwcAAADQmqr3eFyzmsd+Pfajpfz11IuXlLF47G/fPvDjOkgWG5bJRB8TDWWiYfN/A0Db4F8aAwAAAAAAAAAAAAAAAAAkMTw0BgAAAAAAAAAAAAAAAABIYnhoDAAAAAAAAAAAAAAAAACQxPDQGAAAAAAAAAAAAAAAAAAgiXVqS+Es2vuUecKkkerzngPOZnFB3lAWp3VxqnVCFGJx3dYdqozT5mBxMBhk8agzxqt1PK/zhO9N0TiLu2fnqHXy8rqxuHhrTJVZ9Z9lLK4o38RiRw3/PrvVibj1Z/WBgYNZPKggoMo4xGZ21NS0ul0S57tqawWL3WRXa8Tra1lctyGawH6Sw4l/CpI9tSsREdkd+nObOJ0OUcZmuBTs9raXUfttxzpHUkxUtXjcXK6t24mKS9W03biu5nybhnUi4dbLtNxurHEHFd3s04U6pPFE5P7uv28zfO4RcY+Dstey8q9YfNMNt7J43ZoNap1omFeKcIRfDI0h2XYTheO1apnm5aEtjcfx8gS2IdgMF2o760lLsajulyki9yMbt6O4MQGADmfzghIWx+v0WNuTwdspp103kN6MDL6g38Hpf1oV+51a9Jur/8jiSPZlLO4/5HS9nXo+zvd5eV8zeOiJapWyTRtZXF6u+x+fl/dRHp+bxXW1lWodl0v0nyE+4ArVi4ESEdWIeUpmZqYqM+Xi7zf/944dO8jXu68q0xE5PV3I6elCRETpcgxBRGTnY6fCQYNUkZffnM/irPUrWTz2zFFqnbI4H+dU1uvfuqDfmeaDbpNv1JIqqmZxGvVkMb8K96W+1RLH5earZeUxPv/1i0mS3TDkcrmzWezI0eN6f5afxW4xdntvxVK1zoIP5qtl0HZ3bLP4AsPtikSGzfKnl0NgObcnSmBUbJiXx8RKbsM15xUHLO8jRA33HuS+ZBG5DSL9nUxzY3Ve5OemubDcjzg207w8JufuuggT3rGDHsxKljk2ACSXrTxc9owuUsrHeuQUreagiXqdQj4eHFvLx+e1T85Qq6Tf8Ow+jvHg+o2I7yzVZdL4Ixj6qaETLtmwq/m/dzXu0gUAICH4l8YAAAAAAAAAAAAAAAAAAEkMD40BAAAAAAAAAAAAAAAAAJIYHhoDAAAAAAAAAAAAAAAAACSxNuU07tuNqFPK7v+eeKbOtbWmXCRJ6cLzi4yfzHMeExF9+NEiFkdJ5yQ5cdAQFm+r4vmW3IacWEMnjGVx4QCe+ylmc6l1Fi//msVOry5z7/2zWFxaVsZiP+n8kHVBni9KZvmyGx7dO108k5PPoxPQnHvBOSwO1fG8Tu4/6NybxRt5zgOHyFBjUxlriMpKillcU51I7mQ4WEy5iE05iw90ncPpYORXTiTvcXv201qOYyJzPqv2bKdjOpeIuh7SPTz467+pZS/9H8/xEqrnOS+dniy1TkwkPPO6eZvv9aSrdcIRnhOzISoTABOFwuLHb1JFWhWVbbEtgWRh7ZCfV6iWlXwmc17KxHCmJGoAyUNm1A0aysjquclQpmXm0oYDOqKOLSdPLtHjc8r1iwU67zGRbtNbJ/JkUkoC6/CR/vtPPqxKfPhFFYsnFvBxvyOsE3JGGvhV4s7k36euVo/Py8t5PrR4VA8G7WJO1BTlV291NZ/H7Ma/o0Nsw2b4u2SHSKbplzmmiSjasLdPje5MnhxkpZs2UWqXVCIiKrTrubDPx8cw/lzdd0ccPDfuirVrWDx8zHC1TlZWxn7j3eTv0NlQpjV64BwK85bT7dI5uVun57orq1bx/dh1fu2icj63HTR4KItH2AfrPVV+yeJ0R5oq43Hz+wA2kfv7g4+XqHVoS9JOGA4ql+gWYoahaiLTQplrWDaZ7Zljm4brDnG8phzebtENeOTUwHAscXEs8vu05xwQETWKY4nI/Mqm7bSyH9N5iSWS97jFhiOoPgDQIVTpRe+J+UPQkOC3toLHJWLGWWfYblB0QH5xf+ujl9Uq6VN/KtYZoLfbDrLvuPM9sUDeliIivxj6/XK0YcMNLcaq4faMWwGACP/SGAAAAAAAAAAAAAAAAAAgqeGhMQAAAAAAAAAAAAAAAABAEsNDYwAAAAAAAAAAAAAAAACAJNamnMaRMFHTd6m8SkvL1Oc5/SazODuP5/UJGhKkVNTxPFlRmciEiD4p3cDieIy/+T7k1nmEq6M8l9ngDJ73q7LSkJe3mucD8AZ1MpyYyM6SFwiwOMPlUetERW4wTwbPgbRqJc+9RERUVLSexcNH6dxKMi1zdibPRXDzXSLvABHNuPpSFudl9WZx3fZqtU5xNc/7dBDSaCYtmfPoYOUeTiR378HII3yoHE35lhMh80WZ8he3zDNlHcXn/kiLhfjJm33/HarMm+8tZXGoXueldNi9PBbJwsKGpFcRkTMyJvqNeFxn6LI5eNYxQxFS2VlsIu9xIvm4RaIymX/5YMnurZKFUslnplygLR2aYwHYH1PeYEmO2kw5AltjGB2q7LitxSYNhmrT1OKAdyawjaSl8hV3S2Cl9uQvNkkkh7HE+5+sftmqRGWI5xir2MpzD2elDVLruD0i77G44EtLS9Q6dnEsfr88l0RuL5+7NNTzuZnDpudZNgdf5vfxPLxxQyJKt5sfv8eta+jij/bmfN0ZSqRmdQzheJRS4run5dVbK9XndRv4srBM4EpEFTX8moqJwWk8qn+TbJfIYxfS80ByFIv4e7pMq3R9HOOa2o7tSH3VkvxMnvv7g/WLVZnGet4gl1fy7+32rVPr+GUbntZTlRk+YSyLHeK3nO3W9wne+NcCFq94+V+qDLTOKdrDhO5XmOZvck4qtmua0snNyDm33ZR3N7rfkIiI6kUsW0SbabtymdywKQ+yWCdmOHlyUSiB49cnRnycQD5i4+/YYuEu3Jhq9uvbzmFxg321KpNWwPuODB+/L/mlof/p7+W5SzOz9I2bng7+Q+Tl8Bz1brfhhwrzylUe1CP9smo+fho7SmzHp9tUssurMdOwb76v2qD+Tunp4oJ1yDGL3nf1Jp63PiNniCoTjPIkrStW8mNpiOhjKa3l/VpDnSpCnu8OJxzaRXdd/VddAI4y/+XhygWGMvJa9hnK8J5h2b9Xs7ju8WV6DbGZl0TVW2eorp+P/A2L3T/+u+FY2q5G9gNzd/E4qvMRl6xfyBfUnaE3vLnFdqK79OdgNCXDsFA0fQ5DsxsR10wszGOXfrRG4natamI9br2SQ0x+PR6vKuPuIg5Q3Fd12013iPh2beLf18aj4n4uETXs5KO0YI2+i1RWyiuXuP1M+QP48zgifR5CYb7duhpxcolITrsrt6oiVL5973/HiGijLmJ0jD2qAQAAAAAAAAAAAAAAAACAgwkPjQEAAAAAAAAAAAAAAAAAkhgeGgMAAAAAAAAAAAAAAAAAJDE8NAYAAAAAAAAAAAAAAAAASGKd2lK4WxpRp+8eM1ds1UmeX3znaRbfce89LF5XWqLWCRQUsrheJkInIm+mXyzhWZ59Tp3IOiYWOTPTWOyKyqTyRL4MXqas+EtVJsvfncWhIE9+XVqpz4vL5WRxWjbPLh6O6ETWSz9dyo+lqkyVIS9P1p2fFWBxYZrIcE9E539/Kosv/Z9rWPzDaeerdRzEE4eHyPAjJSsb0Z7TI/KrExGRXfxZhkxQbtzkQfhTjkS2cTD2k4hEvrPdcO5a204i6xwqtvj+Y73CITuUo1pN+Wa17PY757D4k6VrWRy21ap1HC4vi+OG9jsa4bH8SWJR/SM5XC4eO8TnDsMPJy88h0sViYV5v2AXRcLVerOKTbSzTcEEVmq7uhrdZxGJk6nafHGiANqoQsSmJlLW8gYRm67C7iLWo0NNVsdNhjKNIpaDZ9OxeEXsMhQKtahajRha7VPZsvdZHBh50WHc+04Rd2nHNvSPL5veukpxJQ7VtcLm97C4OljJ4lCDvoh87iyxxDD1i/F9Bet4H+Z0yauZKJDXh8XpPl7byjeVqXVCoRCLwxHZ1xA5HO4W/60PtaPq3tVHbk8qERG5SQ9wS8rLWfzAk39RZdbV8Gto2nlns7hSbIOIiHz8eghFtqsi7uyeLD76fxZeTzIyZR0gCgf5mMpRx79VMKTHmf3T+XkgV06rR+Lsx7cTLNe/7cU/nMLiFS//q9Xtgqb6WMPAIianAoYycTnMl9s1zPnUZsV2jVNW3hxSpZ7+kKe32I6YT5iGDWpfiYwtxPGq80T6noac+xrbhVYaC1MbLw/XNMWOtSiU4jQUSFIZE/g9xTVr9Cj4n3eWsvihh3gbVV2p27783nxMQD59j9Hty+YLbOJiDekfO+bj2/W7PKqMp1+d2JHYj10fix6BGyq6i6+X7jLNGOTcW1xsMT2Hzsg9SSwxnKtoFd+Li/fdIUeqWie+lTcG8jYBEVH0uzFhVE5a4Oiw9VMWlv36ahYHzhiq15FjeNPlvnIJC1/cyD9+07DKxaJaTRnM62fOWt0OLF3Ej99dfYcqM+qnv+QL0tMNe+eyRKc1xvE2i5d8pI+FJvNnGu76elUk1KVFnd51BG8cH2N8uhmmEjFFNXW7UXnrUvxsqYYmVt5FdYrHYrZ6/ds77HyZzaNHCVE3H2BF5LhOHwpFxYPIWJyXqqiUd6KIisr4OqZbvLKXSBNxQ6Zh8CcGYcFqfmIqDDtqpbciIn6+E3hE0yxJH2UAAAAAAAAAAAAAAAAAAAARHhoDAAAAAAAAAAAAAAAAACQ1PDQGAAAAAAAAAAAAAAAAAEhibcppHI0SWd89ZrbHdX6rs84axReIhC7ZeXlqnVFDT2RxyKHfrl0R5O/5joocWHWbZHY8ovyCfny7Uf5ec0MaSkr38+8U8ss3jhO53fzd5jU1PPdTmq+rWufEkTw/gcORwuKcHJ1r6cSTBvP9+vX5zirI58dby9+z7k7XSQ+GnjqW7zu/P4sD2QVqnXiYn++qoM7fUddkeBd7EmiR0rhdjmRe3qNZe3Icm9ZLJJ/ywSDzPElWEv15zqVXX0OO75JUbSzSeezJJnK8Z/CccHGHbndlLQs1GJJiiEQaGRk8d3xDww61Sr3If2IX/U8opPPN2+TfWhmSg8v+JeoIqTKticlEIAfU0uxbWflGw1KZ6UO274epYkGHlUgKugRSDyqylphyy1SJWGZV1S0FkWwJ5H4ChnUyRGxqBWpb7Dys07vCd9ZsKGNxMPSIKjN4gpiDkCE3WKuZsQ15s1SWIFlG5NczKK2sVMsi8gIXhxKK62NxO3jf54yLmmTIQxUVV2s4pC+0BrEsJnIpub26z83K4HOXSISfJ7tD91mnjDmFxWWGvMfbG/b2N6HGtvedxyp7NE726O7zHg3La45oU/lWFm8P6uvD7+fX4piRvE6MGinrCNGSyjV8uy6dF27FsrUsPns0v1gz6HtqncPltff+Ty2zi2n14EE693B/Dx9rprvGihI6dxkF+W9QXvyZKvLK+kUs9pw5iMUlDZvUOlvW6t8b2i6RMYJhyN7qhmRLZkoRLNK1k1e0xabdxsWGHIbuxyvLiA05DVMD2bXIVtRmWEfl9zOUiYhjiSWS3Fysk8jsQY63ZI5pIp4LOWVXAhtNEmVr+bw7sl6OeIkcUX5CK8r5xfq9Qfr+oS3I77Wm+fSN1DLRZrr8vF0rqtLjiOEFfF9uQ6ZJd1Qk9lbJfPWYpqTczxc4VqsytjAfl+Xl+lUZWTPeWM/vgfbJ0f2Eu4Hfny3I0t875prJ4rOH8vMwf/kKtU52b95nLVv9kirj/a6BiEST6MbTMYX/Lm/MLWbxj9yGieA5F/O4pk4Vqf2KX8ufiM/LDEfyGxFfXcs7oIsvk7m5iWrK17P4n88+qcr48vmYp/Cq6Ya9C1XfsPCN0/l5+uFWPgYlInp7Ks9pHFqmn5W4WwxvLbITUn0nJmoYi0REU+cwzDfltM8lOnybYagrH/25xL4zDPuRj8XcatRA5HDxfiIsdhSRCZiJKCT2vUZcdosNF5C8z6SvQiI5A3GK71SyXs91K5p4bBp3SnKoZEp/3nJ6FCN9T2xf0KMAAAAAAAAAAAAAAAAAACQxPDQGAAAAAAAAAAAAAAAAAEhieGgMAAAAAAAAAAAAAAAAAJDE8NAYAAAAAAAAAAAAAAAAACCJdWpL4dKavU+Zt3+qE5KfO2Mii4PBHSz+/g/PVOsUOBLYcSZPKV0jPn5quT6WSRdMYHEozBNMhxp0wmyK8DIF/fJUEaeDH3DPrCwWNxkyh0ciPKl9JMRTWWeLbRARjR0zhsVlleWqjEdkIK8M8lTctQ59cgePGsXi8jqekTx/8FC1TqShjsUDM9JUmXnznlPLkoHNtvt/RER2w59g2Oy6/KE6jpbsMhP6Idx3a9qz33hcLzN9p9bWO1TfWf7W8USy0yeJyobtZHfs7lq6Z+u2zU4+FtuoO4tdXv3jV9fwts0ZdasyhYOOY3HR+jUsDgZ5O0ZE5HLzNtLj5W2q2633YyMnix2hkCpT0yD6m7JKVabtDH1Wu/B+zePR/USoZw6Lo9u2ihK1B+lYIFnJWl5laEM3lvB46cKPWFy2dqla57TRY1k8/PLRqkyD6EtCYtgWMVS1mKgmstY7XHod2f14dBGyR83/DVysoZ7F5aWGE15fxMLBZ6TrMjViLC1+yNBW3R5W1PC+o3sev0DSc8v0fsK8zV+wTI/h68TvbfPx/iYvkK/WiUX5fMLj5v0p+XWftaWKj/Ojdl0mw8mvVqeD19ChJw1U63TiXSF9uWIdi7v79VwhGGpgcaXo24mI0jP8e/cRcqrPO6r1X20il3v39/3+uNPV565lq1n80G9n6Y2k8/lyYXYG/9ybrVbxuapYvLG2XpWpqOZ9flEpr2sZed/Tx0KWiIOGMt0My1qzi0WvvPqyKnH17VeyOECD9WZUE5IiYq8sQJVVW1j8h3/9S5V5u2gFi8c28HOXJW9iEJENQ6qDQrXeCcwbTWVk3y23a5iiUp34XWVTbJqOxsU1mKEvOfKLnVUv43GaoVrVie06CsR+/KTIqYChh6Wt4lgi4tyZzkuslTKm8yLPt2lo1HK9Nt3M7OBClXykGavVjUtoBz+jA7N4f+N16LH1xDP42HrxO2WqzCmj+b69BfzCC4bWq3VK1/IxTF7BSaoMOXry7VTxNvb9lfo71uVcw+JQ9QZVZnA+Pw/e2jJVJiOd96GVxXwMNu/lYrXOdVP5Ois9JaqMgyaxeFURH2cWr9T3LLbX8+ONhnTt2XPnfVdEfQRHAz+/3svFELi4eJNapeBMMSb7dIkq87d5/NowDDNa9eIW3mk1rNct7/0XnMPizyrfUmWW/eUZFhf+6X1eoE7fNws7+Vg/Ul7G4lG5/PkFEdEHoo8N+1QRCrVs2kJE9HddBrREbkc49M+oxhF+0f97DOOtTPG79Re3jgdn6nVy5PReLSAij7ixlC527tADoTdf5P3EJ41iFb0Xkne6TbNW2RxX8qlwYudbxKZ15K20hlbKtOXxBf6lMQAAAAAAAAAAAAAAAABAEsNDYwAAAAAAAAAAAAAAAACAJIaHxgAAAAAAAAAAAAAAAAAASaxNaUA8tDf1S4Y/oD4v/oLnbIhU8Rxe/jyRW4mI4kN5nqFoSCdhiDr4s+06kV/sVzfeptb57D2e0zgji+fNzM7kORuJiKLixd5TLrlMlYmEeS6zJpGv2GnIgRkVeY7lO8ldTv3287wcfnxxQ67kmMg4M6gfP5fVIucUEdHiklIWe8TvmN5Ln5etm/gb0RtCpjekJye7fW+uXZm/mKj1XMOmnLuJ5O5tD7nZw5WG92B9H5mv+GBtNyZORCJ5kFXuZFMipxasJPrznK6xVOpk293Kue06x4TN0ZXFUYdoMx26Dyj6lOcyDTfqNigv63oWB3w8ScYbq15T6+T0HMTitBzehroNuR/lBVJTo3/8uhrZXre99mUPPpPFFVvLdKHtOkdTa3J68uwbAUPeySwfPw8rtvGch46uOld1dIfOsQPavH99rpZdchEfa8Sa5O+qE93l9R/K4oGDdM7RQG4ui3NyAuJzHhMR5QT68TJ5fN8+Q6K74q08d+VnK5apMgve4zmNyjbx8WFN7Xa1Tk05H69Eqnlu7QynPhhPFc8vtm69zs0WGH0xiy+c3JvFYUPSmi0ilmkEq0iTIztTl+Vrsa9OpmQ5QEREdjmWMnTU1XW8X6heW6HKbN3Kr6m8PN4HlNXovmXFWp4vz1EuyizS+Ypjcd6PrdHVnmIiZV0gix+LaZ5SUryGxWlpvC2OR/VFJNITU329/o45+f3Fvnnf7c4yXL0h3o/JuU7JBp3Hzy5+SKehDqe2mEfFLZkXt+PKy84nd5fd3726Vud69mXytrhggL4+qkI8k51dZV/XScgKHbwvCckEY0R03CieVGxMps4vp5WJ2JSBq1sC25H4NX7ikEGqxNZiPv8t66fHom7i9yQySJzPKG8riIjKg7ylP2XkaFXGk8vzeMZjvL75Pfo8DLuM5wt94tGnVRlonUM2f6ahtpyHG4qoUb2cJxq2K5fJuaUpSbBNVMdIpS4Tk/n+xOWe9Zw+mIo4/1a+sTxfdx9Dcy5riCnNdkhcukFxvg0pDtX5leMih6FZaBDbRXrWxOX4+Hg27stTZWJOnofU5VvH4sWL9Ly2pIznEQ5k6Xu63gLZFvOxUeHQoaTJizGsSpTVLGbxsjX8+IrW6IsowE8DVZfnqjLzN/FKWV3wnirjy+Ttd6SB55ftXq+v+s8+5f23fYO+z2sL/pzFxTX8foPXq/u1qmreQJw7St+j2FY3gIiIwo2N6jPYh0rDAD1LJIKnLgdnX+Le/HF9+McOm2Ei+PJLLDz5L7p1lrVRZms1dC1qRBYWLfjLq1ardWrFst6qBFFupcjz3SD3pL+jS/QUWcTrmd3N70sREYVXyAWGg4m08jkYyb6eiGiriMM7dJlsEcdF0+fWaYQpTQw+XGLMU2OoEmliu96AYST3wzFigaglq/VzslWl/P6bbN0N6ZXFlarvFxG1fukZ0nErsldOJA+yaQgcaeXzfUmiRxkAAAAAAAAAAAAAAAAAACDhoTEAAAAAAAAAAAAAAAAAQBLDQ2MAAAAAAAAAAAAAAAAAgCTWppzGg0/oTY7v8lFNuvYm9XlafiGLvwzyt3pnZOr8F5PPn8TiksVvqTKOnjzH3uOP/y8vEJFvWSd68+Xn1bKWJo49Uy3bUcvzVrhUchyiqiBPQrallOc88hlyfNhEvg63nW+3sJCfNyKiE0/ieT+Oy5ZviSdatZaf37wBPKdhWa3OK/jS/81l8R2zH2bx2q90DrIVH74jlpiy1sAhY/rTDpkCRsaGddS771vJw2viaMefmcRML90X25GHYnrHflzuO5GX+SdwvHax84Te7y+2a0pF2XKzKYbPO6pobZSs767HoMr0QBS18zMccfKkGDZDgiuPOMH1jXq7ZSKH3dYv14oS+pfNz+O5nvKGDWdxOKjbukiYH1+sh85EkZUr+oHwEBYunWfon0RvnJPP8+lUVOm8me0RivJ+rqy6WBdy8hOeNpDn+nO49BVfLXPLQDMfy6+tcytOuvAKFpdt4tfl2lWyDyYq/fKj/cbJoNyQsuuvbzzX+oqpPF9k2R33sXjE2XxcSkTUIJI5y7RTHp1elBpEDkNTzhpPy7w8Ou0ZfCcU5Vmkww0yqzRRdRUfn5dX1agyoRDPLLRD1Mc0v87Xnj2AL1u6cj6LfR497s/vx8fwDofertcmsoxFeH9TVMzz+BERedL4RRVx8b6wpkr3n7UiX7jXqwdGtXU8B1+whh9bRlC3W9FoSMT83GZl6e/s93dncVlZmSqzpWJvXxcKJc98IzcvjzxeDxERlQf1XMyWlcbi+rDOTe2y8d8/I5MnEFux/E21jvskPu7JIZ0DMytT/pY6l6Imc0iasqTKbPCylTQkghUj6ssn6fa6vITn/g5u0vcJ3Lk8j3epyPoXten2o8HFz+/YLD1/p1AZC/++kufJHDNAn18qMCR6gzZzimF+3DDPla2f3ZDfN9zKdmyG+WdEVEe5julKDolubMVbX6gyY/0nsLhAjDWyXHo+5LfxnefbdfvdGt2rERWIav+h+FzXGH3HSM4esgwTaNkq1OkibDvI3rpXRZBfvLEehozQTp6ZsSbI2/eX3pLzZ6IfXMDbrSljdDtWvJKv5yA+rghk6FGwvbdHLNEXRHEpv54XfMorzvDeuhL7nBaLj8vW/c9/1vLtzHs/TZUZVcjPX9jDB+qu3rqmLF3JG4PPKnVGy1E5/NxEXbwPqK7Ux+Lvzc9fSXk/VSbdtzsnZ8yGTOCJ+ulUPYboXcnHAydee6sqM/ZyPncnt2jl0wfona1ZxMKCoSIrsCGx/Z0ih/EaVYLoleG8jX9pBV/HYZhfrhCXiHzSYGp3X503i8Xei6YaSomdXft3Hv9Fz23kTd3K/pezeOWPf6jW8Ir+M42+VWVq8rs1/7cVQl+RqO2GqVdFKzGRHgNUiGvMa1gpTSyTd2tNs0DZ4vf/ix59/Lz4XyyW+ZR/9ke93dfLeCx7LNOjB3n7xzTakjMQ2cuZvmO1YVlbGe87tfhv5DQGAAAAAAAAAAAAAAAAAICE4KExAAAAAAAAAAAAAAAAAEASw0NjAAAAAAAAAAAAAAAAAIAkhofGAAAAAAAAAAAAAAAAAABJrFNbCjdE7eSI737OvHjDe+rzfA/Pdj18HE8sv6Z4pVrHn5XJ4hLDfvNz01jcGDelZud8Hh47fTz7dXVIZOYmojSvm8XRaIPecGgZP7Ycnoo75jYcm0jUvm1DOYvnL5+rVilbMYAfS0xnsH9tEd/OpTfczuJH7n9ArTNqzBgWu91eFq9Yyr/fbqb03EBEu//s4gD+9MIuM7mbdmEoYz9Sf+6RwPHKrOoOQ5b1eCubSOTrmZLRt7bdhHaWwEbUKoYDtrX43sn01zlRspP13YViN1zg0Rj/5eqDvH3Jy8tW63gKhrC48rNKVWbll2+19VDJ5+vO9+PwsbiT06HWscd435HpcKsyUR9fr74+qMooLh46e/DzMur8k9QqS/9S3Pp2hewBOXyBRx9/ecMmFtfVFPECW+vbvF/Yo1YtefO1R1l88eW8L//dI79T6yxZ8gmL5/y/aw/80JJF4wYWPnbvNP75vYZ1Usex8Oq7f83iUXee3OpuUw3L0lo0kZ0S6V+TVCjO28OgYQwfIb6sqEy3jz4f7182VVeJmI+riYi8vFugsq/5/Cc7c4haZ1QGH2vbSPclaXxaQg1RPliqjenBSM/cDBbX1fDPN9ZUq3WcDr7vQYU9VRkK8n2Xb6rg29DdBFXXbGOxnE84XXreUlZWxndbq+dM2bmB5v/euXOn3nEH9emOL8kV391KFOTqcVB2znEsTrenqzIZYgxDxGN3+lq1TkWI/9Yhtx5dZ9H3jMfcFhU1a9Sy8CZ+AefliEqRqc9DfSkf/xUv09vN8/MLNpB7tipTLmYRiyv5uSnw6Yvel8PvWYTq9fU7OIcfc6CUr3Pi5NFqnY3BCrUM2k7+YqbpnJyP6ZaZyCH6YjkPrxHtLhHRJ+99zeLv+3qx2J+n16kWU4M+svoSUX8xh5bVMzC2m17J9KUOge4iDhvKpIk4Q8SDDOssTWDfhS3+23C3LnmJ397l8qgiIwsGs/j66y5kcU6/PmqdraV8bFFSq+8N1mwV83t7f77dGj2+ClTwK6JgZG9Vxi3a6rMH8UoxZQwfexARLaMUFpcZLhJnlI9Rir7Q23nz/zay2J/F2/cReXwcRETksPOKHArpPjUY4pP+cISXGZqtz1U03I/FWen6N3DT7v6mEfduE/bbT/Q9JensX/xRLXtKLBvKh2h0893XqHW2LOf32ZcW831nnzFcrVNDfJ1XZMNLRGtW8HsLXvGEx+nW13Yswu/lbL7wTF5ggl6HeovrauU/dJmwmIf86s88zvtKrfJU9vE8vox/XrRE7yYqhrP15d10obi1978bLf05GAUy9Q2JJTt4u+tXJXR/ni/iQlFHiIgmnspHbjVi2nfnPN2WvS/ilfqWAL0rHoOV6iKtkiN8Q41Q3zFgqJ+BXL7mpAsms/gPTzyv1vmt6FpM46vWGMe3Lf47xfD5viTTswwAAAAAAAAAAAAAAAAAABDw0BgAAAAAAAAAAAAAAAAAIInhoTEAAAAAAAAAAAAAAAAAQBJrU07j/3y1ufnd13Vf/q8ukMqXXX/Xiyx+6qG71SrjLzpPLNE5hAr78fwWtbHW8ylmBXiuJ1s630aIdI6P7DT+/vbvX6DzCgQ3vMliu8izVu/Qxxav5Hkr6tN5EpsGQ7rLSPgjFr/8jk6gU1EmFrh+ycKYIZ/oqafyvE5pHvFG+gjPZQn71zKlsc3wJxjyJzCVUdsUZRLJX9yeHMe2duQ8ihn2ExXf0SHKmHYjr0xZJpGvcyRzGMlUgzZT4qwWy0x5qTuqnZEY2b+7IG0OnYHBLnIAe0WeFYfdcMW4dXvdmryuvM0v3bFVlSkv28LimK+MFzD9rjGe0yPi0IVCxPNkxjuL/C6m5KYib9mXm3lWr8pSnd9IKgwMUcty8gv4bry8dkUdOlfImmKR275MFNCpKuEgevmF3+03JiIadOI5h+twkgAfo0266feqxMTJM1k84wzeqJvy3MgsZq11uUnUTbRZeWUZi+sM+fQcdjl/0Gdc5iDdVi6yHDl2qHXefWcRixct4uPk4YN1vrqLz7uDxYNOOEOVsbv54D9m43nrq4OFJO1Yy9vvGnEeitbo72yL8tHS0hUyGxRR8CueF3bQCTxLk8uQ07g2yPs1t5t3DLGYPi82MfAsHDBQlYm1HFClJE8Osv9EKsjh2N0WvfHOcvX5T8ZNZfFgXz9VpjWFeSPVsp5ixL2t1jDWiH/BwlqZO7te/9ar1vJrKhTXZbLd4nq1i3rt0tdzxfoiFpcuX6HKBIaKulO1TpX5e8lKFq/7dD2L3aP1uQoM5+PKqE8fX17vISx+YEAui5cSP34iouLSlWoZtJ1LDMfFcJ2I9HzZNN+0x1opY7gNdVwGz2PaR7SZbtOtKzFFGjNUJz6elKsWcW1vBg4a2S2EqnSZAE/pTSJU0wsiokpxX6zAkOu5Ze5E3WsnL4+dV4Jzx+pr6gcTeB7hBe/wtu+111epdQJD+K89OFffI41t5UlGQ3U8V3s0lqXWycqTy/S4Jy+7jJfI4sdfE9Q3UndEeYWrrJRXHtGoQXzs/0mpTozpyeJjRI+f5zTOyNXrfLmpjyjjUmWOEz/LyADvd8cO0bmd7/wHH2MtWK5zT5+Su7tv3hXGvws7mExZj+eLuCCX31N3hXQW1b/+jdeR+Y388/w8MbYiohIRr9muj0XmXs0Wt/fT8nQ7sPiT1Sy+7zU+Nil6S9/sWRp5lcWbPpR7JqLe3XicIz6/83iSrhfxDcU89pbquUD0CzFejOh2i0pb5G+PHMk7x8eWHxTqjMVvlfAO3pTTWLbe43vyOM1wsyRvwgAWB2rEvHYe759MDFehcdmByjYsSxNxhuFmTnqY90f+NP4dL/2fCWqdBb/j9x7kTMcwvFVMd89b7jmRbeyBHgUAAAAAAAAAAAAAAAAAIInhoTEAAAAAAAAAAAAAAAAAQBLDQ2MAAAAAAAAAAAAAAAAAgCTWppzGNkonW/NzZp1jl8R7+eMRkaxlh363/5qVa8QSnads21d8WWFI58SQZCqtQHaAxQ1xng+DiMhFPEnA23+5SZUZn72BxT7x3vKwTuFJEfGV3CIJjF8fCnlE/pYaQ97jpWU8/uzjxSyeMnWKWschcvdF6tryNvM9TJn32rOdY5/NvjdfrSGFtM5PnECOY9N2TPs92Iy/qjg+U4pX+dPLMmFDIsfWchqbd8QdrL94UfmJEzi38UR23qKMlUR/ntO1R0/q1KkzERF1kgmuiSgufv2IaKuDtTrnSDzEf6T87gWqjNfNt3vqmFEsXvA+bx+JiFYUvcPjYp73eMyYMWqdVCfP8RIxXKw1UZ7HKeISfZ+pCxPJNyrfFzk8dNoklUgjatM5A8tLeWKY7ByRhcSuE615RR2ol19R9PWwf/0HnkH273J1F615p5XSiVm76uBsp8PLmKwW3TJ7Fouvv2EIi00pu+WIV1ZHU07j1nIYE/G+EDmN962uSmYU02e8sor/Sr4eOo9cmo8PwGtDvM2MhvUcxJk+iMUX//BsFg/O07mI3G6ejy4tcIoqMzCHl6mr5McWqdaJHD1+npdvi30ni4vC+rz4ffyKzs7Rk443/nQfixcv5n13Rt4cfSw+3pd4fbxDcqqs3kQ1lfx3DBku+prg3s4w1Jg8nU3mNgd17rK71ag0JNArl/PlCXp8ojOpyzqgBx8NpTwzX7RU5zQui/KBfn09ryduh84X6ROTA5/NME8M8mXhWn78rrC+byCGenT6SToHZkYa/57VK/QJHZzGc/y9XcYzhn3o5uM4IqKrRw5lcZ4h/2Y98d/JS7z9KC9drdbx2ky9DrRV1HAPRhHj2ajhspQzF3lPqcFwCyzb34XFsvUOGdZxiGv5uLzOutBRRLYucrbml0mOiUhmZpU3Hk13jwp8PH9lIaW0fnBARERuD/8Rhg7VWRhXlvAJ55vLaln813/pHJJ3F/B2rGRlmSrTW1z0wwP8WIqiOgtmuahcdct0fvexw0W/Zed5X2s36LHSkhJ+ZZ0+tKsqU+Dk/cvAdN3fOCKnszhDjNvIpu9ZODJ5xXYHA6rMthq+L2c+L7OynP8mREQTT+f9RHjJSapMefX7RETUFEnO+7LtcfaJA9Sy+at4HZBPK0yOk+OO9cWqTKkY0sq8q+tKtpIkZw9n99f7LhaXrq+Aj/0e/HC1Xkl4xcmvmcH5uarMpf14fVD5i4mIAq3syPBMg8StqMvFqXtBNwtEFQ+IbRhm3VXH7f3vGHIaJ2rMaTpn/XlL32Kxx3C6xw/mcYaYgvz5Nb3O7BnLWSxnIDrLd2Lk9PJgtIgnGpYNyudxhmEcJI9l3t94bvCgYR3ZErTn+A2bZcua2rCtJHqUAQAAAAAAAAAAAAAAAAAAEh4aAwAAAAAAAAAAAAAAAAAkMTw0BgAAAAAAAAAAAAAAAABIYnhoDAAAAAAAAAAAAAAAAACQxDq1pbCbXGRrfs6cYyjBU1enqrTPmt/pZ7FMCE9ElO3nieUzXBkszhl4hlqnoDfPsl5eztNo10UMydDdfJ0XXt6gigy+nMc2L4/jeqtEjv2XiRhOUzzCY78pk7XwyZuvs/ih396nypQV89/orx+8x2JvaoFaZ9jIs1l8+VU3qDLXXNG/9QMExW747W3yTzkMZeyijK31qqbF9hvu3o9YaNqNQ1zQMXG9h0Rs2pkqYvhzFrlv01+8yGXxBLLGy3MZM1biVvZtOJhE9t0RdUrtQp0czt2BvDiIKNIQYnE0yj937lm3ha4+3ubXuspVmexMH4t3BHl7vrG6dJ/H3Cy+loVLFut1vM48FvfJ66fKBIn3NxVB0ZeY2nN5DUUMZSR+KimaZbjoomEWbiuvEbvlnxMROUX3WJ/AocC+ffbJq9S1a1ciIlqx9hv1+R8ef4zFL/zlaV4gXnXIju3Y0ptFp9/wU1XiB5fdwuJBw1NUmRwXj0UTRCWGPcvq2Cji8YZ1EtGyqu1s5zaSgcfNf3u3r48qk5bJf1hXmm5o3d40Fmf4+Zh3YOEotU5evx58G6YxjbCGN7O0+B9BVaZoLe8nzhszicWFYzLVOp+Irm9rTRcWp2XlqnWy+amjq2f2UmV6uv+Xxf/+3SwWv/Sv19Q6d1zzcxZ3dfDzvaOhUq3jsvPaVldZpso4nHu30ykma2fHVfXJOnI4d19cp5wxRn3+/bHy2ixWZYJhPov2uXg9Kfl0jVqndM0Kvo5TX+B+P683DjEij4b0OCLNy8dk0ZiuA/4sPr+Xc6JSMXcnIrL7svl2Hfp4t0X4WCjb8J3G5/DtrCoczOJaw3cqWV/G4hU1b6kyIwYMYnG6g3/v+FZ9/yEYCall0HahRMbNYpgcNcz5fOJyKRdD+MoKS63jcvCxRlD0ATWVep2om6/jylJFDpv5onqWGoadHy7+msXREG+fH7i1r1onVcSyJpqmQ/InMd6fMCwDoi83bGXxvDc/U2VWVvHfbdo5vJ8IhgaodT5ZyW94Rvrp9jxwXncWO2x8DDPcrX/JfOLjtkq34U6wmM9TnF9F3lyPWuX7dt4/bizVjcNKcWWNyPepMjUxPoA6Lq+QxZnuRWqduk38PkasXvdjYwt5A+EQ46eo4WaU38G3M3HMUlXmP/M3ERHRLrucpcC+XHvtFLVswY3rWZzILb0VRXzMe+7xejzut/EyJ4rPSyr1mPeKgTyW/RMR0aAhPF4Z4eMXfaVoI4YPZfEpp52uyrz9Ip8LrL33OX0st08XByMK5Bt2Lvq+/otq+YL/nWtYaYGIl+sifVqMt+TDFdinFz7V84s3RTM2aWJvVeZDcVP9zdf4pNV0f6U9ZP9vqp89RSx/fTFEM5JPxUzjFbdYmJOn+5Gg6MNCYshvOv6DMft1GZ70tjzcJmsfOzfAvzQGAAAAAAAAAAAAAAAAAEhieGgMAAAAAAAAAAAAAAAAAJDE8NAYAAAAAAAAAAAAAAAAACCJtSmnMdHXrRdpqb621SIu+VJv02aCq1nsFvkiKbRJrbNkIc9FWdckktgZ8jjKLJmDdYoMcoocxnbxHnNDaiXyiEfzIhUHmVJ2yTyfXXURGiQSxYR8PA/I7+++Uq1TUc2zU8rDvf6mqWqd7H4jWbxi7VpVBhKj8hW3dzsHIZmPyhlkyOvkEMfrNexXZqBtEO/GN2WQkK/PjyaQx1kentN0Do5U+ruj6ViOsCa7g8i+u2WpD21Xn9tt/Nf3uEWuvKg+mXYHXydkOLc1W3l/I/NHhBNN2sD3pJbUR3j7V10u+xaimpjoTWRqSlNSDJk4OEPEOn2UqlxlRToHCRxdhg/qoZY9/6c5+40rDL/9b+69l8WP/e5Rw95aH4MdLbIGnqeWPf7Hp1j8/Qk8f45p9Cgzh5UZyshM5bJvMfVZaSLWmdfbx7aP/wbu7Iv+H4v9WQFVxpPGk2L5/DqftVt0LwmkJ27V+1v1suumXc3i0qUyqRcRUYBFNeMuZvHiYp3lesFy3saHHCLPtyEP8rlniL0axisFmTxzU/oNN7H4qef+otbZuJhnScsby+cK2x26Jp04djiLa8o3qjJlZXuTaTbF9Vytozr9tHGU2mX3xM57ks6FV1nNc+HleLJVmdJl/GIsKlnG4rpKnai0IJvXG49M0EVEJPJORiO81YyE9W/tF2O7qGGyGxGteMzG9+316SSvby3hdWBLg+4JRgzn13NaTGcvc1fwZZeO5rkql5Xq3JqxIB+oOb1+VWZoFs8DWLqM15M6Q4deSjr/N7RddQJDHjnXrTOMr2Tq1Xrx87jdum+RU4xtlbztqt2qf2N7Dq/nlYZBzWBTdTxAxtmQGNS4vLrIqrW8vc7O4ck2eQ06EIbzCwlZK/KqOrvpmzunnHcKi3/2f4tZXLpet30RkaM+wzFIlfnPBt5P9MnhF++Wpbq9nDaSV8DqmB5dN1Tz9Z56h48jKuN6nayMXiz2hD5SZUrKeZ/kz9TfKSuT97NRMSRZuUHODogqgrw/HJ6jB139c3JYXF3N+yOPvHlMRMEK/j3dDn0/9uS8FURE1Ni4i15Rn4KJd2uFWibvuxsybZO8mrfV81J//lSv48vnDatf3GgtKtUdUlQOrwwTl4BIQz5/tS7Tmr9+8haLczL1ji7N4t+6IM9wMBs27H9HlbrO1LzFx0ljbbwz/NeFOi94zjn8OUfxv+SNM6KLQ3t/kx1Nu8inf2owyDDckJBX5gsLDJPfw0S2+IZHGGqZ6TaqlCfiUSI23R9aI269RmN6Tw7RBdSLsV61YezXnscIsjbWN+kyLR/+Gj7eJ9yjAgAAAAAAAAAAAAAAAABIYnhoDAAAAAAAAAAAAAAAAACQxPDQGAAAAAAAAAAAAAAAAAAgieGhMQAAAAAAAAAAAAAAAABAEuvUepG98tKIOn33mHnEEJn+nSg7dxCLC3qXsfjKS/updQqPD7B4nrtAlRk+IJPFjmg5iys2lqp1YiIePHAki9esW63W8XdtYHHZDrkVopCHxw43j+vVGkQOkTXbKeJEntzHvXrZXXfwuCJWxeKly/U6QZFHPl3kjPenL1HrfLa6hsW1oeH7PE44POLi0rTZzeX2JyrXMWSRd4ky4SpdJiqSt/fszeM0UUeIiEJiXzLZuyEfPMVFRYkZMsTL8xIzfCdJllHbaH0TCRZKFvbv/kfk9XZVnzpdDhY7doo4ymMiIlucN5o+n+5/lpa8wxdsS+RYD1xNg6GhbY3TsExeq/KaEv0GJI9sn172x9/+ar8xEdHSlVtZPHpYn4N6XPuWp5aMnflLFl9/zVUsPv8kvRXDsKfVz7eIuMHQNstFDtHPxQzrRESZDF2kXbJa/HeXg7TNjmjshNFH+hD2aXAorJbdQWksXtt1qCrzxA4+tl67eiWLvZW6n/OFeUdQs6mExUtr9HxouOtMFgcKdGVb8db7LF61/DUWV65Zq9b5q1iWUXsrP9YJej5XVs5rqC2uB3Iu996aHWvbNPWYNmXMxdS16+4x0xNbX1Cflxfz6yVi1yPlohX8eigr531An4wskhx2Pkh3OQwdDvEG0OHgv5vLqWeyNTX8eDOy9PXscPHruaGBz8NTPdlqnYYGvs7izzeqMudfPpnFfkeNKkMhfv1mDypkcSBNH++O8jIWnzvyMr1dca7yRp7Bt+vWx7K4crFhO9BW9z3N22Lj/RUbX1pX16DLfFvJwuwefF5yxdn6ftaWNatZvKKG33Cprtdzm6iDL1u6TE+yz75rmD6+FhYs+0Yt21BaweIrLhvCYtPUOODicb5uKugJD+/XKmr5llYYtos7RodXXZjfhPGk6R/yZ/cvZXF5sejf6wwbLkxn4XyvHgVHPPx6Lid+PV8/kl8/RETzN/EBt8On6+PfnuH92pInp4kSum7RiFdY+PAPPlVFppzB1/v3cj0eKS7md3YDA8RNrqiuTW4Hv1lcGdG/wZtL+HjP7+InPUS6Hw4EeL3OM9wLKY+lEhFRU6wdNwaTVNpHf1HL5Oj1M8N6cqYbDfFrec5i3ddfP5D/Zsdl8oa3cl1QrVPCh3HUXQ+tKZA9hMWh5at1oTb6UT89l/dtFb3q84Z7YO5lPM4W9d6rr01/jPfdY+OiXjkN49L/4ydm8AY9XqQtZS2CJv05GAWDhjHzYSKGIqbWncTjONItIVGliOVVZ7haVKsrbwfp2T5Rkbg/G1ynywTEztLEfeAsw7MSWftM4ytJbqbMUKbl+W1LjcC/NAYAAAAAAAAAAAAAAAAASGJ4aAwAAAAAAAAAAAAAAAAAkMTw0BgAAAAAAAAAAAAAAAAAIIm1KVnUiaOInN+9WNyTq991np3O32FfV7OIxaeN1m8ld4TeY/F1M85TZUJVPNfPutV/ZbFTvaWcKCTeQu518beUOzrpbHj5eTx3sie6QZVpLUOEzI1HRGRrLd+pKbmMeJxvep+7Q7xYPU28BH7SaXodp0jXERQpsbaH5RvgiSgs8yv01mXg2CMuKqchz2pU/PQv3HW/LrTuDR4HeG5zf/6JapWBJ09gcWb+AF7AkLrCydPpkN1QRqbDlfmKo4Y8yGobIjbmThYx/vpmL0/37tSp8+7GKGZouKLiR3B7eAYGhyHhr83Oz3heut7wGpnT+CjiCPGOwe/SySvq8nnepLDIP2/sJ0x5pwC+c8M11x/0bY49fbpaduPP7mNx/lg9RvCL8YlsvuXlTqTTkue3fnjUU8Qy1wwRkcwE6hYNuNPQbsk23pB+BpKUv1Ln+72+gOfwWrmpTJXJGzOCxfEhARYv/Zzn8SMiipWXs3h4HZ+LhWp0MqXoWp7z1X+BHtREn3+TxxGe/4+P6nYbLGLPej5/sGXqPH7Fdbyfc/l1TfI4987PIqZ+r4N66fN5lOrZnY+wLq4795pifn4ryspVmQwnP58eD5/rRkI6D3IwyAf6aW7dAAZrZJ5Jfg3Z7DrTl8PBG/3qSj2/9If4DxzIEnkyM/VcPZDNv2Nelc6T+WVJEYvdsa2qTMG4HF7GzbdTkKuvX38uz3tMtbWqzPvv83yhH67n7cPKyBq1TtZkWZugPYKfrhRLTLMzcaPGNC8Uk9+KEj5CeTus2+aNi15mcX29mMukm1rR1SwqCq1SJb73Kd9O8Uqe2S66rdiwXZ5J+J8bfs/i7DxxHRNRg8gk6PHrG1or14uRWkUZC08qD6h1vKKN7xTnbZDX8BPFRBsUd+l5YV3N3sGdFTHkpU5SIR9vQ6eMPl2VefCBO9q+4SLe1tV5dIbFN8TE9RfjAiwuq9cd+rYgvwAqSneoMkueXM3iwNiT+H5+Nl6tc///8Wto8Sbdl3Qfya+rynLd5ufn8evXLXLSrzCM7SjK+5LS9bo/L+zFE9NuDPN+orJc32cfPob/tg05+jcIZO3et7zvAPvmNNz6DojYlE9UZqCfMpr/ptFFum1esI7/rmvEkF2PZohs4v6n6dZPqchh/P8m84yoF5+pr3/5DMA9YCyLfZWmoxEryQcLRET1olEvFWWihom5vbUbuPJsE5Fs9x2GyXtGizod30V05FL1HlOa3Iab7KTzbR8KckhgeDyhcg2bHrUFRCyvDsNVqLYj65rpDMgyGYYyBUP53ruLg8l0i0pORKW1/Jpft51/bvrO8vhkfmgiXoNbe0TZEp51AAAAAAAAAAAAAAAAAAAkMTw0BgAAAAAAAAAAAAAAAABIYnhoDAAAAAAAAAAAAAAAAACQxNqU07hbGpGr8+7/jhrSh4TEe+4bRJm4ITloarSUl4kuUmWCDTzPQ414YbfMb0REtKWCv8XbJt5knpXuV+vY7fz4CnQR8okXl6eKV/s7DC8Pd4pH8/LUxQ3pAFI9PLYZXrzuEOfBJTYcNvxJQIM4XrtIeWEzrOPrzFeq31mvCyUrG+3/Ty/E+ZXZXBLJOGI3bN9m338Z0zvqozKNk3ifvs+QCqLozfV8wbpHDVsWCSLKlvNPy/QaHy6QO5OVLVev1GcgC925eapIWg7PJ5Kex/OEZfTmOcyIiDLSUvh2xaGZ6nSdOJdBQ949U71OBmFq0bHYddtsF7mpoiIHiem0OZ38R3B7DIWOYtFqXiOdaTpPTOFQfm2urBN1T6ci1EznxS9yNIXlxWvIEyP7x6D4VcKGxqLJkBQIDpk/PPkWi2+78fw2byOQP1ot++MTv2HxpDPGtHm7JjIDoCmXjNSev2qUWWEChuTD5TL9pmi/fYaOWWY6M9QASBYyadDj9+gypfyKj9Tp9vFCVx8WB+YM4wXKRUxEtFzEcuhkygEsE0IZru/nPA+zOHbPz0UJffxxkfzbkTOUFxg6Sa3zoaNCbNUwn4i26G/ibcm4dIzzdiby7P6xnKsN56WI53rMcsgM7kRdbfwC2BbkOSYra3U2vIDIGxwK6TKhOnHRO3gjmpGhB8rxOD+WohU6v9/44XzMTlGZZE/nSg6Ia/44n77o5z3/JIv7X3aeKkM5Mq8r/w5+6qbXoZ08TO+hSlSLex01FbzuDB8svjMR3TLydhb/lf6fYd/Qqmw5tjY0di45UDb05iKnMZXye1VrnvmdYecyx7gYfGzXOcipp6g321arImsrDOsxrY9G3l8m2u8KU5ZAWYcNZdwBHgfEKC2k62J9sWw7eFhnuvFEot4bcqazDUWTqJ9oxS2X9GbxnNdfMJSS90Jau8aIqKuIK3U+95iLN84lG/g9mJpSfQ+mIsgvCFuV4QbzcdkstGfztvuTTZlqFU9Wf35sEZ3M9MN/8X7W32+yKuP2b2CxrG1Rh55D11Xw9iLs0P35l2F+rkpfLhMl3iJpzdLpLB5+ob4xcPGY3WOscDhJb0K1wyqd7le23sY8pXKkVLyMj3H6GOafleLylqOiCYb92EVTvGKVLuPmlzsFTuJ9X+GYU/RKBWLM3sBn5pVz5qpVsorEggbTdSYfLog+12m4seoWXzIucxob2ni1a8OxtNyshTqROFM23CMjkVu+hipMfUQsv5Gp1zNNoVs7FtkOlBrK2Hy8j4o7+LWYP0HPUW4aysc9L/7kHyxO5K6r6Xhb9katfd+W8C+NAQAAAAAAAAAAAAAAAACSGB4aAwAAAAAAAAAAAAAAAAAkMTw0BgAAAAAAAAAAAAAAAABIYnhoDAAAAAAAAAAAAAAAAACQxDq1pbA9SmRP2f3fGT79uVckfI+IfOMZ6YaNBnkYsjeoIg6R+9zpEOuEdBrnEPFCNTW1LLbZ9PPysNhPebUqQqFcHkdlnnZDjvXyb3js6iH2y/Nc794PP1zyGzJ8x+W+ROw3nO8GcXpj4viD4liJiIpFRu911Ut0ITD+CYa9lT/LiO3/493bSGCZTSwwblcci6hGFDVkVF/50CyxpMa05XaQF6/cueFgtnzKwtAWXSQk4goaIpYYGi6pq2jIMo9TRXxnzGTxiPO+p7dj38d/d3Cert3I4UwlIiKby60+j4uLNRrhDaDT6VLr2O38ao1H5dV7bCn7Ul/f1VHRGZaLAk0JbFh3nxQYczFfIM5ddWWRWsfp4GVcGfw3ccSdap3yNc8lcIDQHpdddb9a9tIz/6/V9W689UEW/+8f7jxox9RWfUQsa7mpq9RXWdvlGJbJ2idHkKbWRR6L7AkNQz+KiDjfUAaOPWtfX8hiV6kceRDlxz0sDsiJDBGVffQ+L7NSFDBdvBe1cnDvGJbJwzNVNi/fmf3CW/nni15Tq9jrxM7efpnH116u1jl18gAW//ujr1SZjMptzf+9M7bTcLAdU7/ufamLtwsREW1Zu0h9Ht9cz+Jat24hg2JSHRKDgm2VeuxRVrmV7yeqx2COKB+nuW28jD9dX6zz33qbxVkZWapMToFcJsZBYTkQIiocksZir1tPoq/N42P09N65qkzN6o0s9g/JFCVM156+3yBl+HgP8j+TJ7O4zKZ/g0/K32t1u5CALmKy5TCMIuQcw/STOuRNF3H9rDD1+OLaJXkTxjCv3SaXmSaLBSKWx2aY8Wefx+Ohp4sCplGOWGY6Lz7xvcNilGO66WELiO22Xof04RnOd8tFkR2tbzNJlJTw9vyNv6zWhVKH8rhRt7OKPMWmU17pZ+HLi78UBd4wrCTrje5LfN+/je/GewqL3ywRN1aJyE5VLC764wuqzOkXX8HiwUN133fKGH5/x827H5p2mb7/s53vmopXqCIUFN3WOv8oFr/5v2V6JeLz7BV6WGZcBvsXNjSHPtF19JYTOiISt8cpKJ4bDO+q15G3cpbJbRiObzEfqlCaoUxEfIe6KC+V5eNjbyIisvO6tuLxm1jsMHRHvkF5vEw4T5WJihMRFQ9HnD59wl3DBvMF8sZ2gd4PncfbAeqn2wHWl+zYQeRL4D4wkM1wP/FwkVNW02hFjq4yDGXkMrcYntgMQxF5b6dMxKb7UvL4thnKrCrm/XKfAO8rHeW65q9cz9eRo0XTKFQeS52hTHuf5OBfGgMAAAAAAAAAAAAAAAAAJDE8NAYAAAAAAAAAAAAAAAAASGJ4aAwAAAAAAAAAAAAAAAAAkMTalNO4s53I+d0r5t1Rv/rcFuVvya7mr+KmomK9zbGDxAKHR5WRS1ziJd7b6vSxxIjv3OEczuLMNJ1r0+bgx29IO0mbxMvC08Vmag3vRy8q4bF8K3/EkCehaD2Ph5+ky2SItDZh8RJ4m06zptL7xOS5lAkaiCjLx3P5RNw6r8DKuoVqGeic0a3lOCZq319yxEVKI7shD4VdHItMTfTeP8RFR0TUcIz9rsfdwMKJN/OczOtKRaNERJWrP+MLKkQlCOqsAXEnz6ZgNyRcUDnHk0RTlJovYlM+FIdIKmFz6rZYkvlQKK5rSX5/nqOp5EuZIFKTh5dIjvGDQ18czlBvviCT5/oL1ZqyUIgLz65ztXipH4vdHt6jxn36RwqHeW6NNB/vY6OGPgsOnX/87a6Elh3NdKaw1h2MzOW6xSeKiOrnFmMpU86a1i75jYZlspbrzJu8DTqCKYSgDeYV80F9oFpfHflufsVnuXXbvKC6gsU3nDGexa/PnqvWCdxqyNnVUukmvaxG9B1jhusyg8VMa9REFobydP+z+OG3WOyh1XwTr+scgvbJPK96tLpMldnRolLstJtqTce0dXERpbpTiYho2SKdBLG6kk/qhg4eqco4RKsTCvK+3OHSc+wvN/MMXA7SYzJHuJbFUTG/3FqmJ8wVm3iezDMnnKjKUI5obYNiwyG+XyKiuIePRwIje6sylMvHgyv+NV8VWSlyiM0cMkWU0N+pcj2vs1kDRqsypwzNZnF1lNf9V97S+arzc/R9DGiHneJ6cRhGHvJmScw0YRZxTNSJwVfqdaoMN1D2u1EiColRQsgw+nCI7H12ObLQdYQC/BqkSn7dksMw73LInMaGGZG8qRHledYpbviOMTFfs8vfxDDSi4l2P2YagbXYrjyOJPb0k63nR3fn8fztoXXt2ZPOE085l/L9pIt5rEfntaeKpTyO6evOVseved/x/JoJhzardWq+kF9K3896/+WFIr5Clfm9WiK/g87BnOYR58ah71k0Eu8XUn08B62D5A1yoigZ7tPBAXMY7teJWyUUMDRB8ja7bFVPmaDH/cMbeBua9j5vu0w5SE/pyeM3DElT3xCXe/9zRDsb131LrJT3WWWlG1gcGGB4+DCGfyfHz/6sihyMuTscWTY6evpUw+MsCiSwnhyNZIo6nWm4UDdt57EI1TNJIn3PqMxQ5n0xpYuu5HPq0nmvqnXkPSHZTJlmDbLFKTeUae+9bvxLYwAAAAAAAAAAAAAAAACAJIaHxgAAAAAAAAAAAAAAAAAASQwPjQEAAAAAAAAAAAAAAAAAklibcho7HHtTnnjd+kXgQSpg8W/eMCQxFjzi7dtjz9Dvz3fEeN6h6hqevyjNNkqtUxl/k8U5/QIsLtskEg0TkUMk4Aw6dJ6KpkyeFybm4W8Gbwrp81Id5cueeIm/nZ1nsdgtryuPV/xDlykWqWXuGMvjfEO6EZ942XlEvPC9RKS9ISLq6ub5O2prgroQHDKmv+xQKYIMaYQklc1CpF8q+8v/M6x1uH5rWW9MeW5lQpHBqsSFv/hfFvcclMLinDE8jw8REV02jIVxsRuZYo2IKCxOpkxLJR2+XLlHXidHKjm+y5cVCusTYxM5sRwij1bckEdLLpE5domITr9oJotLHripla3oOmHKm3G4xGv4ecjIzWdxyKdzs0VjfJ3GOp3/MVbPv3dUZMVwGnJK19XyXH5Flbwvt5vyxME+hWjvYKv1DN7JyZTHSbYehmxoraowpeUR/aUcg+mRH5HM6Cp7xpoEGnljN9GiOWytH4Gjw8SpV7N43Suv60IhcUUb8mbOF7ko19R9xOKHn/+NWud/r/ktXyAblIIMfSxunt+yvmytKvLJpiIWl8V5NqU1xcvVOq+J45e98i+ee0atc3k976OqgmtUmdg1U5v/OxQ7kr3y4eV0xMjl2N2QfLhMnxeK8/7e79ctVbZozIpFjro0v25Fo1E+Jis35BH2u/nAePjoU1j82Uc6B3N2b74vh83QwEVE3mBxb2HlUn2tZg7m27XV6Vx9vkg1i9NydH7ImRfIewedRdxXrVNaupjFwTKdx7bMx3uH0jA/v9MuOketE68x5CKHtos08jjaZCgkrkO7IY+wnHjbRSa7PJ1PnNQ1JvP/Gg4lLLYbNyTZc4jBhUN8J4dpkCO2I3Kbk8PQrtoS+HckcfElbOI72g3HH5aDI5mpz7BOVMxl4nFdhpWX20xeEy8cx+Lehrnva6XynMs5nc7FSlQlYkObtZG3j6GNc43HyE3gYUahKlG3hPcvacX8Hm/v03l/RERkz+T9Y40pp3i77s7IXPcyJqprWCaWmLK88roUruP3p+x0pmGdoSJeaTpAaCNXVrpaZt/Kx0GmXKYjRJOZk89/57whvC4SEUUr+JjMXsjvr3T162slJ5fXR8/nckZK9KDIaVyylu/H169MrROs4XXak8EfHPgCJ6p1GqJ8bIV7Gh1TTp4pY65u6w4H09BJLms0lJF1tlBcrBmGr1grkhjLGXUfw37k04l1hjLy8ZpTDGlMIxjZY8le2fT8UM7wTKM6/QQ0MfiXxgAAAAAAAAAAAAAAAAAASQwPjQEAAAAAAAAAAAAAAAAAkhgeGgMAAAAAAAAAAAAAAAAAJDE8NAYAAAAAAAAAAAAAAAAASGKd2lL4mwYi13f52T0enQz73ZVtT5D9h/eDLB48bqEq44uWs9hu54naR5wp0z4TBaPn8gU2njK7PhRS6/TO5CmlAy79TD3NyZPPu+I8/XVXR7pap2iDTljfUp1hWdEOHuuj1f68mMdPTtDHYneF+YKol4UZBXq7QZuDxQ3RI5MIHVoQCdRlQni3zJ5ORI4Yj9/82yOixOuGHblEHDOUkXuXqdrlNoh0anlej1zjr1RrDD/1bBbXxnS9zxiUwmKRz57CMls9EcXl4Qsxt17W0Mo6RES2ls1HEv15TlPcThTffQFGo/qExyN8WcW3QVVGGvy9QSxuMLTfdge/HsaeezWLF7/9tFonkXb1cAk28X4uP+0UFse9mWqdUIjXx5pghSrjdYnGwsF7HHtMn4VonC9ziAYlGhf9COzXdiLa9d1/61aLyGFY1tHJJtQ0SpJXZpqITT1LtVxg6gtbOeENCRxLnegKo6KaERE5RLsfNvUb8VY+h6NOYXZnFn8Q0RdUaS2/QLINZXLEWGnsWD7uWbD4db3zm8UgfeokHq9coddJ58fyQZUewx8X5HOBtLiTxc5Yb7VONU1gcZCKWGw31OrK1x4WS7yqTOiCvWO9xnCj+ryj6pnVnbp4dg843UOHqs9DKzawOCPNr8p4XXw8FY3ylqu6Tv8m/XL5nLp60wZVpnc/Pp+023mDZ2pTs7LEOD9uGICL1jZYzo9vXaVujdPH8uP1ZA5UZcKi4c/zy96DiKiLYdn+jbnghywuXvQPVaauno/lTh9zBot9do9a56X3lrb5WMAgmkAnGhOdt90wr7WJDt0utmszXPAx8buqsUeTXkdWHDlRJyKSY3jibTPFTNe2GHw45DwgkUmpYVCjlonjjxkGXC55vIYyajfi+BJYBXa7ZdBEFrvTdX9f4+X97qW/4vdXfvnIfLVO2Sf/SmDvcxMoIy3iYbW+7gq/P5PFkTVvsTi7/DO1Tr2jlMU1xvtXh0sig/sqFsXoeUOZfiLW9wXkdqB1ffL0eGvxe/x5hKkJ6iOGOFk5fIZvj9SqdTxRPkuVd0ODG/S1YhvNj+/C2WNVmbo7/h+L3R5+89IW1vd6XE7efmefMJjFYbvu5/KHT1bLoOOxt3aj5DAyzRzk/X1T654qYjk8dBiaZXlfKSBiPesiMfMlMt3Vlt9BzgIyxDCJiMghnj94xfG6DTer5FcytVste3+LzPe8TJLoUQYAAAAAAAAAAAAAAAAAAEh4aAwAAAAAAAAAAAAAAAAAkMTw0BgAAAAAAAAAAAAAAAAAIIm1Kacx2aj5MbMpO8PitQd8PFQRKlfL5FvVY44lLM4elK3W8QZ5loDitdtYbHfot3w7nfzl4dHaZapMuFTkehQvP49HdS6Jonak5GpPrk35DvWSOp1LwSHemR4Wb1mPOvXb2m0i5ViOW5ep2bYpoWPsaOLx3f8jOoQpdwxphaKt7Mz01yDl4qX7lQtmtbrrwIUPsNjtlflUiL5czX/72FqRU+/MkWqdi3/I882WVfIr/rjCbmodv0gGamqDgiJpQNS2/5iIKC5TMsnECIbzb0vkz22SND9l8ZebyPZdw5jTV7fN4TA/wQ47b5QGDxmi1omKRBQlX+kcTRnp/ALJyuV5kDMy8tQ61dWlatmRw9vryhreZ/kM7a7ss5wunYPE6eR1q6ae5wwMNugch2GZ+8Yh8pSbKhLs03bam8/E1HR3FbHO8tnxyPFKImMema84x1BGXs2mZlhWE9nkm/LRtJYnxm2oEnJRvJWcxjJtIhyleHpXml+rMmlTUeNKFp9lyGbuKTyTxVf88HYWX7P4abXOgmduZPHErWLitZbvl4hUPsvCk8arIrELprI4fRCfQxUGddaj+A947sQVcx5jcXSLzm9YIWpSg8jrTESU6dmbs9Zu36k+76iiVpyi300oCk8apD5fsYL/tn6fzo07/lSek+6080ax+K9/e1mtEwrXs9jtc6sy2bm8td1ayefqTq8eewwaXMjijDy9XfLx9erK+fWRljdCrRJs4GPG8gpd/04cLnMU6p43KnI/llby8WBBFj93RETRGlFmgi7z17ufZPEpmUNYHDT0SgUj5XaeVGUgAU4xmoobJnAyn69NZrEjorho76JyTGwYpTlayWlsSqBnE4VMA0S3OL6oHMAYchp3Fus4xSjHlK5QnquY4XgjMrez+NyUH1pe7zKxoDHVrDhAUw7pll9xFwZPe6yr+YTFVTV67pht423xpRfdwOI+9oBa56EsXgdK14RVmaKSJWpZ232kt7uIX+OTRvP6Z6vU92t9pOf8h0Zvw7Kt7diOrPymirFBxGfqIsO/y3Mc20W0Svf3oI15fIFe9uNPWXzn6FNUGXGbhvIG8H4jw23of4bzcVGhjV+7FWt1W7bxX++x2O3WY6mr73+QH1sNvwYbDIcim3yXj4/7a2KGleyGcRx0OF29R8+dKNM9mURaWDmyKxeJkKN1eh15dcuc46bbnxWimphyBMu74Rk9eWxIH04xcTARcSLqDTuSuZNN99Za9iyW4fN9wZ1fAAAAAAAAAAAAAAAAAIAkhofGAAAAAAAAAAAAAAAAAABJDA+NAQAAAAAAAAAAAAAAAACSGB4aAwAAAAAAAAAAAAAAAAAksU5tKdzNRpRq3/3ffkPC5nJDQumWzj77ErVs/vz/Y3HUkLHZ4RNxOMbiHJ9ML01UY89kcZmtQuynXq0TrOFfKi1uSL1t52Gan8fumClxuN7X4RCs0sv84nijUX4uyV6j1tke3MDiTVtNKcnhSJIJ1COG5OgL7r9fLBHX5XFT1Trn/vjHLHZ59HYniazr1bU8jor6S0RkE8c7sLAziyMiqTwRUXVUL9MbFrHcjqi/RDwhvGmBYRUlrjZi2G6S6JHRk+ydU4mIKBTSDXp9PW8/CgoKWByP6R+6ZANvg3zpaapMzMWXVXxdzOKcnBy1TnV1qVp2tNge3MriUJ2uSPaYk8XZGW5VpiHI2/SaumpewGHozONiOxGxb7Ff2L/aKFFkz2VtON2yR5W/tB7hHHtkrZYjDUOTryRSRnQ/FDecb8MixjTCketki47B1N7L4802DA9b1qQdKa0cGBwR8rd9dyW/ypY2LlfrLBXx+1SpypwixlM/Gd5blNBt/ptiOxMXPCdK6Kt3hYjf3rJMlemTyfftdeez2GH4E+PySj6venvLmywuVrWeaIKIK1L9qszPrzqz+b937NhBdLPed0dUtOErcqW6iIgoK0Of8PyBhSxetXiBKpOVzgfkp4zmv+PdP5up1nn2H2+wuHuvLFWmNsavK5f4bSeeIX9ZouyCPL4gzzAZKONjsKIq3tLmDTlDrRIK8RoZN9w48NpbH7mHKcxifxYfI9aT3u6WDSUs7hnVvcngUSNZ7KIMFmdlifNCREUbluz/YCExLjF2DUd0GTnm7aLHzarH35Uqd6RXkc2dPSw+36HXUZNHw3UrrzG7aBtcYj9ERE7xneJiNBI1rOMQ38lpmPD7xHary3lcr+8hUUT0SWIOSFHDyM7F6ww5Dec70uI7mG4eJqmyKB8jvP6vtarMC7N/wheENrGwupbPn4mIsjJ5X/K3FwerMtf8gu/7jfcP0hx7x+ssfHN+IiutPzj7VkQdyC7URSq26mWtEm1BpwJdpEn+Lu+rInbb7nt5ltWU0JwJ9iFvNAt/XWWpIqV/uZfFNcWiH/cb2vORo3iczsdF2X49Pu9ezNvV7e++rspU2vn4+7jp97HY3qDb5prKMhZHbbydLSrW9dfp4Pd/ho6dpMqQ3dBewzHF5Ty67/PJR46mp29yZLdFxEWGBlKO6OXjlJhhHdnTmEaUGeJGnnxuY7j1TUExfDWMZhU5czSM9Awzm8TgXxoDAAAAAAAAAAAAAAAAACQxPDQGAAAAAAAAAAAAAAAAAEhieGgMAAAAAAAAAAAAAAAAAJDE2pTTeNVnRJ2/e8zcoNNDtsppyqEiRAzpfxvEo227eO93lm+AWseTzvNkbUzneSCiIZ1HKxriby73pqfr43PyRMFBcWw1NToXlMwde+6FPLfz26/xvM6JGjv+hyxe/OHzLM7Qh09u8Q51p4MXkjmPiYhC9fxN8StrdG42MJOphw6W1lL11hjyWatcRNk3sXDUz/6gVtkuUscGDe/c94m0Bx6Rls9U7eVpicrtJpBngAzpFuR2bCJFjCmXpdyVPDa7IVmlTaaHMmy35bJkyiuTm5tLDlcXIiKqMrQVXf08f4vby+NamcRhdykWOV26cQuF+VluiPJ1euYY8g6tWGTY19Eh3MD7ozSnzKlGJM9LtSGHtE1cnS6Rp0z2lURE1TVlfEGVyB5iSm4O+7QjTNT0Xdp2U1vRVTRMHfHsykyrss8ydZXyapZlDMNFlfPFlItV2i76KNMqBSJFU76hzIFq04AcDhtVZ2v4lVc4+BJZgkjkki8q031NUZD3j/5MWUJnRnpMxHmiZk3so/Pg/X0Ln/88ZsjA7XryRhaHn1RF2qzcsOxDEWc06lKPtPjvZMpU+fj/Pk+27yYNeSN13tvrfziZxb5q/TsGbfx6CNbya6wwQ+cVvvGGy1m8LajzMa5dynMnnj14OIsLCg35F0Oiha7T+f0+XMnzYnuy+Hadzp5qnfLiVfxYJk/V+05Ag8hW5id+YyNsGLkXDhLjSK8+nwGRba20lH/vnFo9CqgrN+SChbbzio7a1Xpua9Nckhzid7WLcXLM0DI1iPnOVyI3ZcU6vU5M1JEaw3XgEt8pWyTHUxNoIoqL46utbn2drACPc3XOWrKJPmnlhzwOGu4PyRtP2zeJAoZzmZorjsVwc8rRov2LNenPk9QLT69m8QM//aEqM3H06Sx+ey7PxTqqn/5N3n+H50Z+880KVSYzw5RZci9DVntypor20atvMFdHeV9XXmcaXRwuol+oeK9dW/F7eM7cu398KYtXqNES0d/vlTmN9Rgge9iVREQU37WTtn7+aruODRKTd82veBz6ihf4P/45EemfrEDkOJZ9DxG5xDOM7CpDO7uML0tf+Sj/fOo9apXswqEsDtfxm8eFo89W63izxHMO5C/umOLH1p1r0zBOzh5khu4V7diP6SnfDhEbHhtQieguPeKAvYZEyCExdIqIYZvPcNPIJoZCpuceLUdTcSKqNZQxwb80BgAAAAAAAAAAAAAAAABIYnhoDAAAAAAAAAAAAAAAAACQxPDQGAAAAAAAAAAAAAAAAAAgibUphdrnZXvz0P19Y9t3trFc57+QnnhGLztXpFWR+VqXvnytXsnD80EV5PP8KKGIztdRJXIErFmnE8NmZ/NcA+U1PH9USWnreYmiUdPbztuurs6UuHavxR/rZUOH8Lg+xt92XlSst5maxc/l0KEnqTIrP1m+32PpqGzU4i8vDK//j4ll7clxbMoqIDcTE++5zxbpgIiIfvKnH7NYpj8NGRIC1IjtRg3poVRNEpe3zbCOOg8yF7HhJfzxBPIe6x2J0FD1HK1sJ5GfzPgb2c3/3dHVfbudOjkbiYgoMytDfZ6dy/NxOUX+lgaZ/JSIhp/Ug8Wm0xmNWSwuKRXZLCpM2ZS4QOEF/FgMOYJryhayeEyGSkRJTlEh39+eaMaIvQoDPEfghRdNV2Vq7bzS/nvhAlUmWMPzE/qcPP9MKKavcK+PZ9V1ZvFkGyGZq5CIQkVqEXwnQnvbEVOO95hol3ofohRB8mo25QuV3YBsik11Tzar1YYyDSJWGfUMbbPMTxxMoB2VbXFPwzoyU59MGQjQkrw8Th/Tl8X5f3tJrVNSvJ7Fixe+qcqUbuVtc1hUSLuqAbqa3CYLbJE57xJjaJYOizp1dolem7uh+b8bQ7Ll6Lgq121u/u+KsP5FfFv5GObSk8eqMgNHDWLxjiif14Yi+poa3Ls3349P5zsNhfh23T5+LNXlMk8pkTOdj43K1ur5ceGoSSzOyB3J4gVv8VzKRET5ecepZVJ1eDXfrmuQKhOM8rGP08HrY5OhJ1tXVMLiUSOvUWUKfCNY/OJHr7H4jerX1Tpl1Pr9EUhAmhzBGBLFyTGBacIpRz7yvk3EMFHZuZLHJS/wuMbQNsvcwyaNIq4T30nmGSYiircjR/aOT3n85cuGQu24fyWPP6F1+PFn6OaFvJl726B43CJDkaT0wGPjWHyc16PKPP3q/7L4w7XpLH6tXvc/2bkTWOyO6ptG007+PotnjOb9jc2h65o7wvuouFPfNygN8foWqeP1Rs6hiIi2NPAyVUFdZ+s28Pa8fFOJKkP81NCHRVtFAVMeZ/kddJ9a08Dvt1Zv4hk342FDG5OA8j8/t/s/LEPecji03MfzOD1blykT/UBMjMnihruOMu9x9jZVJHtqPl8w+sF9HOS+ydE4psbJKyOt9XumRxPTyKRMxO25TVkgYtMzAUOGcUW25nIqZkhlTi6xs0idOJYEHlCYeqeWXVqMkNMYAAAAAAAAAAAAAAAAAAASgIfGAAAAAAAAAAAAAAAAAABJDA+NAQAAAAAAAAAAAAAAAACSGB4aAwAAAAAAAAAAAAAAAAAksU5tKdy9P1En++7/zjJkk64RcVTERSWlre6j3rDM4bGzOMPtFzuW6aWJQg0lLA7knc7i6uotap2Bg3i661Url6oy/Ud/n8VeF/+W3lx+rEREbyz9NYtLNrR+HhJRG6ze7+dvr2t92cyLc1hcXFql1rnijPEsdnp0Wu2Vnyzf77EkLZGZXSYt11dLq5sgIiKHiOV2IoaVKkRsc7e+b19cLJBxO8XEduziC5l2E5V/4mI6MZJYJyYbJV0koR/FtB2p5VdKSaI/z/F4upDD2YWIiMKhiPq8fBNvewv7pfO4gMdERD4Pj22G38jhTGHxcf1OYHHFspBa52Xi+xozYTyL3T7R1xDRa09Wsrg2rNvMsh21+gDbqOjLV1mcsTBflRl62tksPnfCSFWmopqvV76ZtwS+bj61Tu/cPBZ39/PzEInqCjC/6Gm1DHaLR3f/j4goamjcbOJ01rp4nGbYpvzVwoYydSKWfUDQ0IaK4RZlyXUM+2lIoExU7Msr9hMxnBfZ5svvI/e7eyUejpSdJcAByhCVL3WQLuPzD2BxWlq2KvNlCZ9I/fXx51kco63tO8BjyLXTr1fLslqMnna2bZp6bHMR0Z5hTI1u3d4s/4jFGXanKpNWOIHF/n79WPzZW2+pdWRfsmqrnqOedcnFLN6+ei2LgxV8XEREZLPzuWJg6Ai976xhallLp40dr5bZHXL88Y0qs3gRn7+PP0fXvywHn/OX0YcszlczLaKoW+77W1VmXTGfaJ94Bh+Xda3UY9yVn8reGdqlRpzHuGFgERODEYeuR+QSg7CYmD9EdB3xNfA64XTyY8nM0vOhte362cWxxBOYkCZg1Im8PmT5XarMawtWH5R9tVV1o2FZmWmkCVTGr+8iKldFKkP8BpA9/SQWu716DpuRk8lil0tfdxkO3mZmZfP+pyFimKn4RrGwLKTr44UBWY/Fb++TMxUiahDfQd54IiLqncFjm2HCULyKhW8vEnXfp8+DJ85/g2ffWaHK1Nv4jY3SIO/zfTbdZ00qPJPFbxa9r4+3aeV3/5HITTI4pCb/Vi9bci+Pl4kxmY/XMyLSNwHGXqnLpJ/XpkMD2B+7eop3dMs1LJP3jBJ5hCF7AFn1Ds4TPH3PrsHwADRNP25jQgl8IdMmWn7HtjyeSKJHGQAAAAAAAAAAAAAAAAAAIOGhMQAAAAAAAAAAAAAAAABAEsNDYwAAAAAAAAAAAAAAAACAJNamZFH/+fLAdhaj9uV5LKvh+RX75PAcFJOm8s+JiBydeZnuaTwnY1q6Tug6eChPTDb2zEmqTG4uzwHsEZvx5xmew9/LcxqXfVmsy7RDddX+852dPfwStSw7wN/W/v3LeI7mYRN03s8x43iehPqwfvH6bx+4e7/HAh2D/Qj+mYnMYxtPIF2LTH0bNxy/SnPTjjQwMlc1kU6blSy6eDzU2bU7p3EorBNRBet4LqKSDRtYHKrX7bnP24PFMgc2kU5FVFHJ+5vIVp08zO3h/URIJJytKde5oGrq1vNYH8oh8eGCX6tlHy99j8VnTZ2qyvh7F7K4UuQpyUgTCaOJKLs3z6XUGOK/Y1gmqIX9ijXtzYNuSrEXF9dutUh2Etep5Uj/apq8NutEihpTxpq4aDTlSKnO8NM3iO/kNtTPbLFdWaTOsI7MRyNHJ6Z0xRnIYQyHmddwzXn5VIEysrqpMgUFo1n8+z+WsDit+1C1TjzE+88cB89YFHPrnIENDp4rz+nV8x838TlSlpv3jRlZIvcfEWVk8txrWSJvsy9T5xn0ZfP95Pt03j63a2/O14adndXnHVVqv3xKse9uKENuQyaqT5az8KU3X1NFPPz00uX9bmLxwLE8JyIRUUnpahb7/XoM5vUPZ3GDm+cw3lpfptbxiWRgviy93dbYvfr3X7boQxbnZMisY0TD+/H8yX7qocosi25mcVExz1Y2eJC+B3DKIL6vN9c+o8p8uJznKh970mQWuzN0/Zs0+UIWL3j4VVUGEuCQg5FUQyExGAkZ8qw6xEBH/GRZfpkJnMgT5su2iVHa2ooqw7HIzqOVJHZERCTqUSd9LOQSDYGckNr1fKhUDETdbn2dEq1u5dj8hmWHapbUcmBsEZHOGZ2M/vr8Ehaff3qBKmOP82VBkZ8426cHNe4Iv+cYNZxuVxrPLOlP57eZHfW6vw85+TihqtZ0vYg2PlNsJ6uLXqVWlElP0WVqxP1MnW6eaCTvM8+VwzLTnKP6GxaOPf1iQyE+owlX8u9dHdUbri7l9faO+DWqTF1w9xgxFG6kyx641bBfOKLG/Gr/sUntOzxOP/vgHQ+AQWnppiN9CPsl7++bHk/Ie1zOBLYrW105UpL3pRIlj1eNrvTtcgqJL9CeRzD6qR4/L4nkeT6Q/QMAAAAAAAAAAAAAAAAAQAeBh8YAAAAAAAAAAAAAAAAAAEkMD40BAAAAAAAAAAAAAAAAAJJYm3IaHylLvuQ5L0nGtIQOhbTASLXMIfLNuGUyU7d8a7mJ6Q3jktyOTiQYbdx/npj+QyeqZbn9eMKOlSU8x44/TecgW7OW52DOMOTygeQg8wofSYnkND4Y22nvX9bYW5wrK4n+PMdu60T27xIMew3toUMkxnY5eAaJuhqeK4+IaFsFX9YzS7dTIZFgaUslX6dkie4nQg081+OCRYtYnBpLpK0+cmINK1n89jMr91GyJZ4gd4VH5810u3nmj1B1tShhyoYL+xKP781lbMp1HhXtQ1Bcdqaz7RJ5jk25WuTVGxH7jhna83qR3m+g2M8gwzrlYlmOLtIussY2iNiUi2XgQdo3wMFkSBFIPp7+j/74yHQWz76fx0REDaJ+5ohKXmuonztEvfeQpcpUu0S+P9noGNotm/hOLplO1NBwdRLb2WGoxCyVX70+1o6qccMmopTdv8PI625Qny9bsZrFoYjOxfr2W7zVdPgDLH74ofvUOiU5vPewBfXFWlNazmJnem++HzfPB0xENGj0KLGkmyrTmlhol1oWjfKLyG33qDJZeSe0uu3qKj5GvHKQzg8p2UWv6k7TOWhvuW4mi+c99xYvENIX/cCR+n4DtINL3NYyzVnF9UMhQzK5ejna4L97ZUTPU6hGDJZyeJ5q8huSwPpEDmCHIY9wjajnLnEPxmlIxKoS/oltBHV9rW4oY/H7ZYbj7TmBxxF5vIaczHUyl7PcruGeUv5xokimLuNpkbO2qZFoyc26TBKqDPL5cVlQ5zROc/BRek2YX98rvtE3LZw+XicKc/W1mpHDr+f6ML8QfemijhCRz1bC4ouHDFJlKE1cQw6Ro940STLlMJb84noNbdVl3OJc2A35k9U6ol6HTfcSeD/ryuW/W45P9xM5ot+tNWy3Ibi7baoP7Wz9OOHYkH5O62WK+djvr3/5B4srY7qdvfqS77M4a+TJbT826JBycvSdHB+tZXHwIO1L9gojuvN+Y/F2PQGVS1YZtivLyGGRKR29VN56kQSe2BF1FbEcKZm6sFDT/veje1MiOSs0bbfl8SGnMQAAAAAAAAAAAAAAAAAAJAQPjQEAAAAAAAAAAAAAAAAAkhgeGgMAAAAAAAAAAAAAAAAAJDE8NAYAAAAAAAAAAAAAAAAASGKd2lL4pttnkdO5O+2yze5Wn0dCNSxe9Xkxi4tLy9Q6wQhfx+7IVmWcMZ7GORTkaamjjRHD0dYalrVNXdmyA95G+5nSaLfNH56+2rC0lXTdziy1xphR41h82mljD+zA4JhhP0x/ViL3E2tLZvYDZBNVIn7gVS+pxeJNFIvvbrPt8uQSkcPm4Avi/MeOREN6m+I3CYm+Zvd6cVGmgcXFq1cZjpbvq76klMXjLz5brfHmxkWG7eyfS8RdDWWq27zV9grzsOFTVUKcOjhAsdjeazhuaNuiUVFelBEfExFRgxj2OAxttUMukNXRcCz5YminRwRaTgJl2iNdxI1yuKKbF/UVAY5V6XqapZel7zfcx7IUtcTfyrGEDY1QVHQlO2S7ZWq4ZFtnaLdaDhFSDuNY8EgbOHg42TvtnpaXfrRSfT78vPNYXPqpnqOWbKtg8T+ff4PFg8flqXUmnTeKxZWuoCrzs1vvY/HNV13O4oga5RCVlvNjyfMPUWVaY3d2VsvGTJjAFzj0vqXaWJVaNqn3ySyO0i4WV4bXqnUa6razOBTUF7CzdxqLL516MYvXLvlMrbPqvffUMmgHj2wg1ShILzLO+cRIItKw34+JiChHtKIOH4+jhpVCW3kc1vMfypKjMCcPXWI/REQ20dBG63jsG6DXiRfIBbpMjmj01eBVr0JBMViNikJer14nL4PHpjpub1HPdu0kWmLYdxKqifDfZEtpkSoz9OThLLZn8ft6oVrdt9TV8+vM5fKoMmUhvizLz+uE1yZ+VyI9QLE5dZmQqNfxzTzWtwSI+vXlsWk8ItsCZ29DoZ1iO9/yuJ7fkyYiogqxHUeaLuPpItbZxOOgPg+xKt43V9eUqTL27yaZuxoNbQkYpaToMXEy+NWjvz7ShwBHKcPt23b9S1PZxJqmdPKeaDTCxwiGEQKliniHoYwcjhh6FuVQtZpNIpbdkWm/8lZsIkNXOQI2/WYt992WKTb+pTEAAAAAAAAAAAAAAAAAQBLDQ2MAAAAAAAAAAAAAAAAAgCSGh8YAAAAAAAAAAAAAAAAAAEmsTTmNBxcWUKp799uy43adHCIW43lXBg7mOTMicZ1bJiTyrERsOkdGrIG/1dstnnVHDTlrIvU870Owjie8qKvVGSRraiv5duP6DePOGM+rEqnjZeoMeZNqang+l+pKnv8i3KhWIf0835SwQ+I5NNwZOktZVOSzjNbz8+/26SRq9SJ/aF1Qf0c4vGTe3YORe9i0DVNOg0NB5vA05RU+GHmO2/V9EshxbMpT2jIP7+HM0XykNYYaqSm++2JyO2XWCSK7OBexKG8f3S6ddcLm5hdnkyHprtvH8y2l+XmWjPyhJ6p14nU8z1DIw/Mt1RtykBX0GcLimoqtqkxNnLeZIguYiomIXDbeXvcfNIbFXr/OHFsf5luqa6hTZSqKeV6+WKTYsHc4lG4rNOScAwA4AhIY0igOQ2pQu1gmZw8Nho4uJKZVjlbyHjtkIqgObMwJg8nZ+bvxj0vnfswM8MxeZSrXKdFrz/HcuB6Rxevdt+apdQrO49sZ7p+gykS9/NeN23lsmocvXc7zMucNPU+VaU15yVdqWcXWjSwedcbphjX52C3drs9nLfEckiWlfGzU26fPL9Xz7/3lSn0vIeLgx9fT35MfmV+PB5wZiWRbg1bJRsnU2DnFuN5myJcr2yWvyElqSsYuVxKpfClkmiiKfdsMZdSEWOzHK3dE+hZSvVgQMTTo8lii9YZjEdtRx2s44W65Xfm5vu+nepOo4bywyWQSTbJb053npp54ss4Iacviv//AHH4NfVap575lEX6P9M0q3T7mlfP55hQX/x2XrC1T6wxO421f3kjDfMnfg8flG3jcr59eR7Lt1Ms2yQWG6ygirnm/uP8QNAxiQmJZtiFnblwcj4/P8etr1MFRNM7vbde59G+bRbv7uk4qIyYAQGIqyivVMjnH03ccdRnZkyTy4LFMNF3dDWXkdk1TSTmqlpnlTaMGQ4Z6Rj9Z08Mt02M9+b0TmYcbRmmtSmBE1m74l8YAAAAAAAAAAAAAAAAAAEkMD40BAAAAAAAAAAAAAAAAAJIYHhoDAAAAAAAAAAAAAAAAACQxPDQGAAAAAAAAAAAAAAAAAEhiieSjbjbz6ksP1XHAQbGVRaHqrfsot2+hxiq1bM22Uh5/trDN24VDKyYyn9vx5yBwhESbokTRKBERxW1O9bnD4WBxWpqn1W3G4hEW250OVcYhlvnT0lk84awz1DqdQpUsrqY0Frttej95BcP5scTjqky4Psjihiq+H183t1onLTuHxRlZ/fk2Iobv7HGxOBoPqzI1VRUs/uSjxSwONdSrdbKys1gcrOPfp3zTRrVOcOOrahkAABxd7AdpO7Lnk7HTRa36bqiwz2WONs1Sj22nDB1E7tRUIiLaUhNRnwd6+/mCukpVJhbmfXXQxn/tgH+MWie4qY4vyNU/3MlnDOXrBKtZ7PGJYyOiHEeGWtZWfp9PLWsI8XFlrL5UlbF7+fEEQ5tUmW3BbXwdD99ufSik1lnyJp///vX/XlJlTjxH7ov/Bm6HnqAV5KSrZdAO9Q2tl3GJeYmcQBOR+jcV8iczzbGjos7GYmIdQ8vbWcwF7HrOpLYrG82wHsNTTKwTEeuEDPuRHIbjdcibDaJMzLBdOXWxi+9sqOPkFvNCwzyLlTGdt2QVKmHhS18MVUX67OT1JD9czOLsgG67x/fmc98PNuk+akkxbzN72vl2P/u4SK3zWCXvx2bRJFXG6eLtY56PH1/GoF1qHaLOLKq2d1Eltoprc2ikVm+mQLbNol+I9tPruHl9i5br/scR5+1DdYz/JtG4nvNnB3i/ll9Uo8pEvvv9HY26/wIASEROTo5alkcrWWwabcmhUZOITaOt1uaS+o6pJvdDRJQq4kRG2cFWYj3T0UMcw7SWYq2UMU2X5TqmcyfJ8697kbZvc1/bBgAAAAAAAAAAAAAAAACAJIKHxgAAAAAAAAAAAAAAAAAASSyhF39ZlnWojwM6oI583ez5bk2hHXuXGd4BYJN/liFqXNzw9ie1jqFMXL45S5SJp+h11H7kW6UMf0JiepvW4RCX72Qg/XYqU5nWxAyXpNyOelOZYT/y7WCW4f0OLbez5zpJhjoRjex9JdIuS1+IKTFeCSKdVRFFvZ5aVRJ9ze8Sv1E8vFOvE25kcZT4K852GV5PbUX4K59Mr6fetUtsV7xebtcuffwRsd1wI3/xS2SXPpaYnb+QpcnweupdYb7dWBM/lniTfq1XU5RvR65jxU0vX2mfZKgTAG3Rka+b9n63HTt2tF4IDqvWegHTEC0iuqimVl5P3VCfPGOnxsa944ZwWL/6MyRelxyO6DKWGIzGLf4rRAzr7Kzn2zXVtcYQH9PsdPB1UmL6h4yI1zC3pw6H6vWrdxsa+Fhuxw79kjy7xQeWO0K6TIP43iGbeMFdRF9zjeJ3iclXEBNRNCLHVHwitcsw2Qo36t+lNclQJ9osksDrqVPEtZrI66lVET2foKhYtkusFDVMsneJa85wPVFTK6+nNv37j5i4BuU6TQm8lDDFUEaeOzVfTuDfosh1dhluRcoqZHo9dctqtWv37446QeqGRMzQNjfJOWmEt++RiH7dd6OYk+5S7RxRdBdvUxvFK5J3yVetE1GTuFZ3Gl6r3BQXqQMc/Fhcpr5lF+8DDC9xJ5mdaUfEsJ0d8voUK9UbXo3dIF5PvVPvXb6euj7G2w/T66l3OPi+6kO6HYp8d/4avvt/1AkAriNfNwfru9WHDf2G3Jdp/22MiVp/RXIir1BOZLuJPDZo7eyZtiFHPaYyrb1quj2v7U5Ea4+D9mwzkesmxUqg1NatW6lPnz4JHBrAXlu2bKHevXsf6cM4JFAnoD1QJwA41AkADnUCgEOdAOBQJwA41AkADnUCgEOdAOASqRMJPTSOx+P09ddfk9frpZSUBP4JIyQ1y7Kovr6eevXqRTbDvwjsCFAnoC1QJwA41AkADnUCgEOdAOBQJwA41AkADnUCgEOdAODaUicSemgMAAAAAAAAAAAAAAAAAAAdU8f8MwsAAAAAAAAAAAAAAAAAAEgIHhoDAAAAAAAAAAAAAAAAACQxPDQGAAAAAAAAAAAAAAAAAEhieGh8mM2ePZtSUlKopqbmSB8KQLuNHz+eBg4c2Gq5srIySklJoWefffbQHxRABxcIBOj8889vtdyHH35IKSkp9OGHHzYvmzFjBgUCgUN3cABHAOoEHG0SHecHAgGaMWPGAe1r/PjxNH78+APaBsDhtKd+AHRUpvHG0bxdgMMB90AB9kI/AdB2mGMfGUn30PjTTz+l2bNn07fffnukDwUAAI6wJ554An/UANAC6gQAAAAAAAAAAEBy6nSkD+Bw+/TTT+mee+6hGTNmULdu3Y704QB0aH379qXGxkZyOBxH+lAAjJ544gny+/0H/NdoR5OxY8dSY2Mjde7c+UgfChyDUCcADr0vv/ySbLak+9tdAIAODeMNAADYH/QTAIcO5tgHF87kPsTjcQqHw0f6MACOaSkpKeRyuchutx/pQwFIGjabjVwuFwZLAN9BnYCjjdPpbPUP6nbu3HmYjgag40I9gsMp0fFGKBQ6TEcE0PFZlkWNjY1H+jAAEoJ+AuDQwRz74Eqqu2ezZ8+mn/70p0RElJubSykpKZSSktKcd/Xmm2+mF198kQYMGEBOp5Pmz5+/z7wA+8rVWlxcTNOmTaMePXpQamoq9e/fn+666679HtfmzZvp+OOPp4EDB1JVVdXB/MoA7VJfX0+33XYbBQIBcjqdlJGRQRMnTqSVK1eyckVFRTRhwgRyu92UnZ1NDz30EPvcVE9mzJhBHo+HNm7cSGeddRZ16dKFevXqRXPmzCHLsg7H14Nj1ObNm+nGG2+k/v37U2pqKnXv3p2mTp1KZWVlrNy+cuY9++yzzW0+0e58F+vXr6ePPvqouT9ombti48aNNHXqVEpPTye3200nn3wyvfXWW2ybe/qIV155he655x7Kzs4mr9dLU6ZMoWAwSJFIhG677TbKyMggj8dDV155JUUiEbaNpqYmuvfeeykvL4+cTicFAgH65S9/qcrt8d5779GQIUPI5XJRYWEhvfbaa8Zjai2fTTwep0cffZQGDBhALpeLMjMz6brrrqO6urr9rgdHD9SJ3VAn4GhTU1ND06ZNo65du1L37t3pRz/6EftjVJlvaU9d/Oijj+jGG2+kjIwM6t27d/PnTz/9NOXl5VFqaiqNGDGCPv7448P5dQDabMmSJXTSSSeRy+WivLw8+tOf/mQs98ILL9CwYcMoNTWV0tPT6ZJLLqEtW7aocsuWLaOzzz6bfD4fud1uGjduHH3yySeszJ6+rqioiC677DJKS0ujMWPGHJLvB8kl0fGWabwxfvx4GjhwIH3++ec0duxYcrvd9Mtf/pKIdvcF559/fqvjGJOPP/6Ypk6dSjk5OeR0OqlPnz704x//WD082zP3rqiooMmTJ5PH46EePXrQHXfcQbFYjJXFOAgOpW+//bb5jY8+n4+uvPJK9mAs0fH/nnrz7rvv0vDhwyk1NbW5j1mwYAGNGTOGunXrRh6Ph/r3799c3/aIRCI0a9YsOv7445vrzs9+9rN9zjMAEoF+Av0EHHqYYx9eSfV66gsvvJA2bNhAL730Ej3yyCPk9/uJiKhHjx5ERPTBBx/QK6+8QjfffDP5/X4KBAJtyn383//+l0499VRyOBw0c+ZMCgQCVFpaSm+++Sbdf//9xnVKS0vptNNOo/T0dFqwYEHzMQEcSddffz3NmzePbr75ZiosLKTt27fTkiVL6IsvvqChQ4cSEVFdXR2dffbZdOGFF9K0adNo3rx59POf/5wGDRpE55xzzn63H4vF6Oyzz6aTTz6ZHnroIZo/fz7NmjWLmpqaaM6cOYfjK8IxaPny5fTpp5/SJZdcQr1796aysjJ68sknafz48VRUVERut7tN23v00UfplltuIY/H0/zHPZmZmUREVFVVRaNHj6ZQKES33norde/enZ577jm64IILaN68efSDH/yAbevBBx+k1NRUuvPOO+mrr76ixx57jBwOB9lsNqqrq6PZs2fTf/7zH3r22WcpNzeX7r777uZ1r7nmGnruuedoypQpdPvtt9OyZcvowQcfpC+++IL++c9/sv2UlJTQxRdfTNdffz1Nnz6dnnnmGZo6dSrNnz+fJk6c2Kbvf91119Gzzz5LV155Jd166620adMmevzxx2nVqlX0ySef4LXyxwDUCdQJODpNmzaNAoEAPfjgg/Sf//yH/vjHP1JdXR39/e9/3+96N954I/Xo0YPuvvvu5r+C/utf/0rXXXcdjR49mm677TbauHEjXXDBBZSenk59+vQ5HF8HoE3Wrl1LZ555JvXo0YNmz55NTU1NNGvWrOb+ZI/777+ffvWrX9G0adPommuuoW+++YYee+wxGjt2LK1atao5ldQHH3xA55xzDg0bNoxmzZpFNpuNnnnmGTrttNPo448/phEjRrDtTp06lfLz8+mBBx7AH6TCQXGg463t27fTOeecQ5dccgldfvnlrC60dxwzd+5cCoVCdMMNN1D37t3ps88+o8cee4y2bt1Kc+fOZWVjsRidddZZNHLkSPrtb39LCxcupN/97neUl5dHN9xwQ3M5jIPgUJo2bRrl5ubSgw8+SCtXrqS//OUvlJGRQb/5zW+IqG3j/y+//JIuvfRSuu666+jaa6+l/v370/r16+n888+n733vezRnzhxyOp301VdfsT8wisfjdMEFF9CSJUto5syZdMIJJ9DatWvpkUceoQ0bNtDrr79+OE8JdCDoJ9BPwKGHOfZhZiWZhx9+2CIia9OmTWw5EVk2m81av349W75o0SKLiKxFixax5Zs2bbKIyHrmmWeal40dO9byer3W5s2bWdl4PN7837NmzbKIyPrmm2+sL774wurVq5d10kknWbW1tQfl+wEcDD6fz7rpppv2+fm4ceMsIrL+/ve/Ny+LRCJWz549rYsuuqh5mameTJ8+3SIi65ZbbmleFo/HrfPOO8/q3Lmz9c033xzcLwMdRigUUsuWLl2qrsU97az0zDPPqPZ/wIAB1rhx41TZ2267zSIi6+OPP25eVl9fb+Xm5lqBQMCKxWKWZe3tIwYOHGjt2rWrueyll15qpaSkWOeccw7b7qhRo6y+ffs2x6tXr7aIyLrmmmtYuTvuuMMiIuuDDz5oXta3b1+LiKxXX321eVkwGLSysrKsE088sXmZqd+aPn062+/HH39sEZH14osvsv3Onz/fuByOTqgTqBNwdNlT1y644AK2/MYbb7SIyFqzZo1lWbuv3enTpzd/vqcujhkzxmpqampevmvXLisjI8MaMmSIFYlEmpc//fTTFhEZ6yrAkTZ58mTL5XKxOXFRUZFlt9ub+6KysjLLbrdb999/P1t37dq1VqdOnZqXx+NxKz8/3zrrrLPYnDoUClm5ubnWxIkTm5ftqX+XXnrpofx6kIQSHW+Zxht75s1PPfWU2saBjGNMx/Tggw9aKSkprO7tmXvPmTOHlT3xxBOtYcOGNccYB8Ghsqdtvuqqq9jyH/zgB1b37t0ty2rf+H/+/Pms7COPPNJ8r3Vfnn/+ectms7H5jGVZ1lNPPWURkfXJJ5+06zsCoJ9APwGHDubYR0ZSvZ66NePGjaPCwsJ2rfvNN9/Q4sWL6aqrrqKcnBz2memVkOvWraNx48ZRIBCghQsXUlpaWrv2C3AodOvWjZYtW0Zff/31Pst4PB66/PLLm+POnTvTiBEjaOPGjQnt4+abb27+7z2vh9+1axctXLiw/QcOHVpqamrzf0ejUdq+fTsdf/zx1K1bN/Xq9AP19ttv04gRI9hrDT0eD82cOZPKysqoqKiIlb/iiivYX1WOHDmSLMuiq666ipUbOXIkbdmyhZqampr3Q0T0k5/8hJW7/fbbiYjUq3979erF/kVn165d6YorrqBVq1bRtm3bEv5+c+fOJZ/PRxMnTqSamprm/w0bNow8Hg8tWrQo4W3BkYM6gToBR6ebbrqJxbfccgsR7b2+9+Xaa68lu93eHK9YsYKqq6vp+uuvp86dOzcvnzFjBvl8voN4xAAHRywWo3fffZcmT57M5sQnnHACnXXWWc3xa6+9RvF4nKZNm8ba3J49e1J+fn5zm7t69WoqKSmhyy67jLZv395cbufOnXT66afT4sWLKR6Ps2O4/vrrD8+XhaRxoOMtp9NJV155pfGz9o5jWh7Tzp07qaamhkaPHk2WZdGqVatUeVkvTj31VDZvxzgIDjXTNbh9+3basWNHm8f/ubm5rE8houa3U7zxxhuqX9hj7ty5dMIJJ1BBQQG7zk877TQiIlzn0G7oJ9BPwKGHOfbhlVSvp25Nbm5uu9fd05AOHDgwofKTJk2izMxMevfdd8nj8bR7vwCHwkMPPUTTp0+nPn360LBhw+jcc8+lK664go477v+3d+/xUdTn/sAfdhh2s+yyblgTYyAEYzAN0iCCFKUgRamX46XetVVqL7bV0/b0V+3NU3u0WttT257W1lprtVpba71WvOCFolSlXIpQMI2kSBqIMXHJutlls8tkZn9/hEue5/lCQkxAs5/368XrxTP7/c7Mzs58LzObfY7YXWbcuHHqCxHRaJT+8Y9/9Ll+n8/H1kVENGnSJCIilfMDYJeuri66+eab6e6776aWlhb2k4PJZHJQt/Xvf/+bZs6cqZZ/4AMf2P167/Zefllo10BD/qxJJBIhz/MomUzS2LFj6d///jf5fD468sgjWbnDDjuMDjnkEPr3v//Nlh955JHquut97Rx22GH9en+NjY2UTCappKTE+Hp7e3u/1gMHF64JXBPw3lRdXc3iqqoq8vl8fY5x5Fxk1/ku12fbthpHAbwXvP3229TV1aXOWSKio446avdNncbGRsrn88ZyRLT7S0eNjY1ERLRw4cK9bjOZTLIvYL+bOT2Aybsdb5WXl7Obkr0NdBzT3NxM1113HT3++OMql6Tcp0AgsDsl2y7RaJTVwzgIhpqcG+xqtxOJxH6P/03t/IUXXkh33nknfeYzn6FvfOMbNH/+fDrnnHPovPPOI5+v5++lGhsb6Z///Ke6HnbBeQ4DhX4C1w8MPcyxDyw8NO6l97dwdjH9lTARqWTw++vcc8+le+65h37/+9/T5z73uXe1LoDBdsEFF9CHP/xhevTRR+nZZ5+lH/7wh/SDH/yAHnnkkd35int/S6e33oMjgMH0xS9+ke6++276r//6L5o1axZFIhEaMWIEXXTRRezbxEPVbu/L3q6H/l4ne9vnoeJ5HpWUlNDvf/974+t7m0jDewuuicGDawKGUn/PZ9NcBGA48jyPRowYQU8//bSxX9j1pepdfdkPf/hDmjp1qnFd8gvYuI5gsPV3vLU3g31Ouq5LJ598MnV0dNDXv/51qqmpodGjR1NLSwt98pOfVPu0t7FXbxgHwVDrzxzg3YyXioqKaNmyZbR06VJ68sknafHixfTAAw/QRz7yEXr22WfJsizyPI+mTJlCP/7xj43rRR5LGCj0E+gn4MDDHHtoFdxD4/29Cbnr22/vvPMOWy6/6bbrmwgbNmzo13p/+MMf0siRI+nKK6+kcDhMl1xyyX7tF8BQKysroyuvvJKuvPJKam9vp2nTptFNN920+6Hxu+F5Hr3xxhu7vx1HRLRx40YiIqqsrHzX64fh6aGHHqKFCxfSj370o93Lstmsap97t9u7fqaKSLfbRHvvEyZMmECvv/66Wt7Q0LD79cEwYcIE8jyPGhsbd//FJhFRW1sbvfPOO2o7//rXvyifz7P9Hsi1U1VVRc8//zydcMIJGEC9j+GawDUB702NjY3sG83/+te/yPO8/R7j7DrfGxsbd/90IlHPz95t3ryZ6urqBmV/AQbLoYceSkVFRbv/Qri33n1IVVUV5fN5mjhxIpsPSFVVVUTU81OMJ5100uDvMEA/9He8NRADGcesX7+eNm7cSPfccw9ddtllu5c/99xzA94PjIPgYNrf8f/e+Hw+mj9/Ps2fP59+/OMf0/e+9z269tpraenSpXTSSSdRVVUVrVu3jubPn3/Av6AKwxv6CYChhzn2gVVwOY1Hjx5NRPoh8N5MmDCBLMuiZcuWseW33XYbiw899FCaM2cO3XXXXdTc3MxeM/3l5YgRI+iOO+6g8847jxYuXEiPP/74frwLgKHjuq76qZKSkhI6/PDDKZfLDdp2fv7zn+/+fz6fp5///Odk2zbNnz9/0LYBw4tlWao9vfXWW9VfS+66wdi73d6+fTvdc889ap2jR4829gennXYarVy5kpYvX87Wcccdd1BlZSXV1ta+m7fCtkNE9H//939s+a5vP59++uls+ZtvvkmPPvro7rizs5Puvfdemjp1ar9/hpeo59cEXNel7373u+q17u7uQZncwNDDNYFrAt6bfvGLX7D41ltvJSLa7y/eTZ8+nQ499FC6/fbbaceOHbuX//a3v8U5Ce9JlmXRRz/6UXrsscfYnPif//wnPfPMM7vjc845hyzLouuvv171Y/l8nrZt20ZERMceeyxVVVXRLbfcQul0Wm3v7bffHqJ3ArBHf8dbAzGQccyuvwjrvU/5fJ5++tOfDng/MA6Cg2l/x/8mHR0datmuX6jYdR/rggsuoJaWFvr1r3+tynZ1ddH27dv3Z7cBdkM/gX4Chh7m2AdWwf2l8bHHHktERNdeey1ddNFFZNs2nXHGGXstH4lE6Pzzz6dbb72VRowYQVVVVfTEE08Yf6v/Zz/7Gc2ePZumTZtGV1xxBU2cOJGamproySefpLVr16ryPp+P7rvvPjr77LPpggsuoKeeeop9wwHgYEilUjRu3Dg677zzqK6ujkKhED3//PO0atUq9q25dyMQCNDixYtp4cKFNHPmTHr66afpySefpG9961v4SRPYq//4j/+g3/3udxSJRKi2tpaWL19Ozz//PI0dO5aVW7BgAVVUVNCnP/1puuaaa8iyLLrrrrvo0EMPVV/qOfbYY+mXv/wl3XjjjXTkkUdSSUkJfeQjH6FvfOMbdP/999Opp55KX/rSl6i4uJjuuece2rx5Mz388MO78yK9W3V1dbRw4UK644476J133qG5c+fSypUr6Z577qGzzz6b5s2bx8pPmjSJPv3pT9OqVauotLSU7rrrLmpra6O77757v7Y7d+5c+tznPkc333wzrV27lhYsWEC2bVNjYyM9+OCD9NOf/pTOO++8QXmPMHRwTeCagPemzZs305lnnkmnnHIKLV++nO677z665JJL9vtby7Zt04033kif+9zn6CMf+QhdeOGFtHnzZrr77ruRbwnes66//npavHgxffjDH6Yrr7ySuru76dZbb6XJkyfTP/7xDyLq+TLTjTfeSN/85jepqamJzj77bAqHw7R582Z69NFH6YorrqCrr76afD4f3XnnnXTqqafS5MmT6fLLL6fy8nJqaWmhpUuX0pgxY2jRokUH+R3DcNff8dZADGQcU1NTQ1VVVXT11VdTS0sLjRkzhh5++GGVs3J/YBwEB9P+jv9NbrjhBlq2bBmdfvrpNGHCBGpvb6fbbruNxo0bR7NnzyYioksvvZT+9Kc/0ec//3launQpnXDCCeS6LjU0NNCf/vQneuaZZ2j69OlD/XZhGEI/gX4Chh7m2AdYvgB997vfzZeXl+d9Pl+eiPKbN2/OE1H+qquuMpZ/++238+eee24+GAzmo9Fo/nOf+1x+w4YNeSLK33333azshg0b8h/72MfyhxxySD4QCOSPOuqo/Le//e3dr3/nO9/JE1H+7bff3r0sk8nk586dmw+FQvm//e1vQ/KeAforl8vlr7nmmnxdXV0+HA7nR48ena+rq8vfdtttu8vMnTs3P3nyZFV34cKF+QkTJuyOd11bva+ThQsX5kePHp3ftGlTfsGCBflgMJgvLS3Nf+c738m7rjuUbw3e5xKJRP7yyy/Px2KxfCgUyn/0ox/NNzQ05CdMmJBfuHAhK/v3v/89P3PmzPyoUaPyFRUV+R//+Mf5u+++e3ebv8tbb72VP/300/PhcDhPRPm5c+fufm3Tpk358847b3d7ftxxx+WfeOIJtp2lS5fmiSj/4IMPsuW7trVq1Sq23NQHOI6Tv/766/MTJ07M27adHz9+fP6b3/xmPpvNsroTJkzIn3766flnnnkm/8EPfjDv9/vzNTU1atu79mnp0qW7l8lrc5c77rgjf+yxx+aLiory4XA4P2XKlPzXvva1/JtvvqnKwnsPrglcE/Desut8rq+vz5933nn5cDicj0aj+f/8z//Md3V17S4nr9G9XR+73HbbbfmJEyfm/X5/fvr06flly5bl586dy65PgPeSF198MX/sscfmR40alT/iiCPyt99+++7ro7eHH344P3v27Pzo0aPzo0ePztfU1OSvuuqq/Ouvv87Kvfrqq/lzzjknP3bs2Lzf789PmDAhf8EFF+SXLFmyu4ypPwEYDP0db5nGG3ubN+fz724cU19fnz/ppJPyoVAoH4vF8p/97Gfz69at2+vcWzJdj/k8xkEw+PbWNst5yP6O/6UlS5bkzzrrrPzhhx+eHzVqVP7www/PX3zxxfmNGzeycjt27Mj/4Ac/yE+ePDnv9/vz0Wg0f+yxx+avv/76fDKZHNw3DwUD/QT6CRg6mGMfHCPyecNvJwMADJFPfvKT9NBDDxl/Yg4AAAAAAABguKusrKSjjz6annjiiYO9KwAA8B6EfgIADpaCy2kMAAAAAAAAAAAAAAAAAAB74KExAAAAAAAAAAAAAAAAAEABw0NjAAAAAAAAAAAAAAAAAIAChpzGAAAAAAAAAAAAAAAAAAAFDH9pDAAAAAAAAAAAAAAAAABQwPDQGAAAAAAAAAAAAAAAAACggI3sTyHP8+jNN9+kcDhMI0aMGOp9gve5fD5PqVSKDj/8cPL5huf3EnBNwP7ANQHA4ZoA4HBNAHC4JgA4XBMAHK4JAA7XBACHawKA259rol8Pjd98800aP378oOwcFI4tW7bQuHHjDvZuDAlcEzAQuCYAOFwTAByuCQAO1wQAh2sCgMM1AcDhmgDgcE0AcP25Jvr10DgcDvdrg8lksl/lBtubhmVpEedE/HZK1ynaweN31sZVmTIqYnHwkNEsnnSseR/fN7J5tWjNqy+zeNqs2ftcRWdnJ40fP77f58370XB+bzB0hvN5M5zfGwyd4XzeDOf3BkNnOJ83u97bd7/5KAUCO8fPnqfKWa7LF+giiivW43muKtOZ7WbxO10Oi7dneExE5HgWi/3i9bGhMarO6CL+jV3L0usd5Q+yWH7JNxzUU7TRRTZfr4/HPkt/U9gSh2GkpYqQ38f3zxbH0hXbISKy7VEs9jw+f3Ac/Z5Hj+Z1xo4dq3eml3QmRR+65OiCuCYA9sdwPm92vbef/Go1FRWFiIjINfQBnhtisdOty0h+P2/BTe1ULsfvGgWLeFvtGvqW7Z3bWTxxfAmL0xm9L7LNtw1ts0/0hbKIaV8cRxwssaHObnlXjMgbKdr4gO5LXPEheA7fts8yvAHBsmUPqv8ayhZvqUiVIPrMZRG1rBCuCYD9MZzPG9N7e8PwLEJ2HbKVChjWLVupvls2eK/D8wkAs/6cN/16aNzfP28fM0bfODkQDM9/FXm7I2N4S0ViDL1jtB5Uh4hPHIIh/tD4IB2CwTNKPzQOjZbvsX9vcjj/LMJwfm8wdIbzeTOc3xsMneF83gzn9wZDZzifN7veWyAwmooG+6GxvKFtuJm+I88fEPhdHjvd+gHCCPHQeJR43T9qNEkBf98Pjf3yobG4K1UU0A9qg4Pw0Nj0YGJgD435bTVP1HFG6vccCvI64dGYTwzn9wZDZzifN7veW1FRiIqCPTezzA+N+Y0uQ5Oj+AO8DRppeGjs8/H7P0XBvh8auw5ve4NB3rbpGkSyuT5QD40dx/DQ2H7vPjQOqhJmhXBNAOyP4XzemN5b2HCPWnYd8uEHHhoXlkK7JgD60p/zZnj+oDsAAAAAAAAAAAAAAAAAAPRLv/7S+GBbL38lWvzyRNMrr6k64Rx/Hn5YtIzFoUS7qhNyW1kcbGlVZcb5i1m86bWNLF6c0b8/VHnmKSwuvvSDLHb1F/hJ7p3pG6qS/K5sraFMmORfEotvFhh+PymSld/R0n+NbPqmKAAAAABAbzu6MuTzdo0b9Xf4LfnXU+IvnEx/UWurP9PSf5om/5gqGOYLbMNf9/rEnwAHLV6mLKJ/YjkSEn+74GVVGflXW/LPzgK23v+g+Atm1xN/Ke0Y/upMbtrwJxO2r48/5Tb81VmX+AlXV/zFniV/e5WI0mmePCgQ0H/j0fvnY53ufvzeLAAMS06uiEZa+/g7U0Mb0+c6RTvlGf6E2dS/9GbLjoSIHPGrFfrHGXbIBeTKxtjQfvvkX/fK7Rr+Ulr2Jbbos1TONiLK5XhHofbNsHFbHP9uw77InwMP+vn9IvlH0T3r5XEQf+a3WzKZ3P2Lf0ue/Kd6/aT/qBNLSlWZi6+6nsWZ9StYfNW8aarOMTP5smO/dr0q0y7uIZ42bz6Llz77nKqTaHmRxWed8VVV5q7f3sLilau3sviWb39X1Vmy8g6xJKbKzKo+icUZdSeVqKTmCBYHY/zXDXLZDlWnWeRIDEw9XZU5ag7fdu1E/vq3j8R91cEiW3PZnOAv6AAA9g3tJAAAAAAAAAAAAAAAAABAAcNDYwAAAAAAAAAAAAAAAACAAoaHxgAAAAAAAAAAAAAAAAAABQwPjQEAAAAAAAAAAAAAAAAACtjIg70D0pqH31TLNqx/lcX+HH89t3GrquNkHBZ3ZHmcyKVVnbQvxWLTwYlZQRYHMvz11o1Nqs6ri+tZ7HtsPovrHr5Ub8fmcbNhXxIijovYMtSZTiMMS3tveLRaVD37KLGkj3UAAAAAABjkMttphJsnIiJ/MKReL/IH9lnfcxy1zLb7/h6sGFpTSTDC4lAkSJKsY3k8DtqyBJEtvpObTOr9TaX4PMS2+ajdDur347giFsfBccTOEZHryfUYZgdiWz75nnz6PVoWX4/r4+vwPL0vOYdP4Frb5MyFyO617e1deq4GAIUh27XnjoPn6vbEDoh21cfbpFxO3KQhIhJtKFmmdpavV7Zbfr9f1QkE+LKw6Nba4+LmFRG5Lt8Z3coS2WKpPAqptH6Ptmi/szavlXOyqo4jDoNr6EvIEwdP9AumNl++R9lHeap/IioS6/VbuO+029o4UajnXHro17cbChSzKFhUokpUibPo3mWLWPzQGn7flYjouJ9PYvHXLj1blfnPr3+WxY/cvdSwf/u2YW2LWvbCsg4Wn/uxT4gSL/ZjzXqssbwxKZZ0qDLUKNctx08p0sQ1uWixKrEmNIPF1RedbFgP7K+B/DWc6X65aRkAQKHCXxoDAAAAAAAAAAAAAAAAABQwPDQGAAAAAAAAAAAAAAAAAChgeGgMAAAAAAAAAAAAAAAAAFDABjWn8a2vbGfxnON5flzTE+rGP/6bxc/98j5VZlNrE4vtEM91UhqOqjq2zfOhJUVem4xP700yzHOZRaIRVWbW5Kks9iV4PoyH1q5Udeq7G1gcXMTzagTP1LnOPvv0p1isM5IQvSVimdWmyVBnumFZnwKHiwXbDYV650cx5fcAAACA/XXGV3+rliVTvJ/NOToXa0jkHD2mto7FCUO60JYOnn/OX1rD4rfirapOtITnUIsUF6syMgNgUKTGsw3p80QqPGqP8/ccMeSfrZwk8sIakgSGxM5EwjxOG4Yw9fVvsrh5Kx/7NTa3qzolpXzkVhbVeXpjvd7CjmyKfvWtOlVmOAoGXAr4ez5gn61zPcr8vpaYQTikx83k8g/bp846opDNR8rhIF9PKCATXhJZIqek64i8mRm9Lx0pvp32+Da9v8TzSoYifDt2Rp/fjiUzrckLR++/PFY5Q5lMVlw34lj6DYc7IC4umQc50ZFQdVoTPGegZ8ji6Tp7lmVzpvkGQG9T1ZLTfr+CxZXTRqkyZRU8DuvLTcmIVLBZw3Uhr0h1t8GQLFGuRqaO7V0m19lJvxin708MR67bRd3u3m9VuaL9sMXR9xnaOnn8HZ+hjPjQPPGBdBv6HzleOayUx1tbxECDiNLOO2K7pkya/D3KPPZ6dEVk2/xkdty0eF1vRaUWtvRx8VnieBty3Utyfz1P7L+jtyPP/5zV93YKxqFRovAYIiK6/tqvq5dXrlnD4rGxUlXm0+efxuLkimUsfmj171Sdjk9+msUPvv2GKhNp5w3kU6+sZvH9y/l2emxm0aYtf1Qlzv2YXsbpa4tomogN+YpP4ceBli7SZXI6F/L+a9CL0nxZ/AlD/nXYb6YWVLYwsqnDX9ABAOwb2kkAAAAAAAAAAAAAAAAAgAKGh8YAAAAAAAAAAAAAAAAAAAUMD40BAAAAAAAAAAAAAAAAAArYoOY0/s0Js1h81l+fYnHZ7HGqzqMtjSyuD+k8Mb65U1jc5efZCdIRndsnJ3IatyZFkjpP55bzdfCcKUFbJzgqK65kcXkxf+5uVx2j6mQ28ZzLmRn8/VCV3v+XReqNI3SKQJUJTOZR2qSr0J9FfLKI+5HSiYhG9/F6vl9rGQ6eW/cbGr2PRFi2xfOsyBx7prxVPpWzTnMcnjcml+U5AT1DfkifyuPd93Ykf8CQOynIryXPz5sVv05XSBFx9qbFydxBupJMNRhVCZiIcj7+xl3LcCAE/RnwBam0zjPz4SNPU8sAAIbC+LJytSwS5Dl1OzK6PQ+IfHPBMB9rbMvpDqg9yfN3NWxez+JM/at6B6N8O+HimCoSskMs9sn+x9Vjv0BQjnJ4GVP6v9dbxrO4KKTzuZWJnMsxMQQr4amIiYjIHzmcxe0NPF9r3JDX1s6K/qc9qcpYsT196o6sobMcpkaPHkNFgZ6xpD+ox1AB8Z3WbI4fG7+lxyKWON9dY/fPt6XOsEyXquGIcQV5vFbWkNg0neHjhpyjxxHB8L7zNG5L6/OlyCfGW2K8Yho/5kSyzUxG74vMOxkQcyjX0Xn95JaixWKuYzif42meW9Py6c/RdfZsO7djh3od3i/k3Fafz4NjrVry1MfFeVV9lSoz/9ofsHjWAj63FdMsIiKKiO7ElBs2J1O2iubDNvRbYbG7xtS2O2ULZ4pNowI2+QN7byd94kBZFu83QiGd61S2dbYxDzz/0Pw2/4B0XmGi8VV83FNRxV+Pi9tQRESN/BaYGrMREflFfxMp5fm5TfcROhKi3ZQnqjEXsViR4Rz0iR7TL/oJS91nIHLFDQlblDHmIE3zvsNnyHtcqFLvtNCI7k4iInp20VPq9ROn1rL4gksvUWUqZ/Dx7Ilnzmfx4tWPqzqxWpEjWA/z6RO3/CeP20SVr3xf1bn1/m/qFfVJ3BSNnqKLJNJigeGuaEc7j2OG3MgtFWJBc187NyA2bR2S9cJA7nYCAEBv+EtjAAAAAAAAAAAAAAAAAIAChofGAAAAAAAAAAAAAAAAAAAFDA+NAQAAAAAAAAAAAAAAAAAKGB4aAwAAAAAAAAAAAAAAAAAUsJGDubLzxlawuCJW0medo2dVs/i4s+epMrEYT2Hvy/LXb77jMVUnF4ywuKPFYXH8HVfVmfSxy/h2SkaoMskyHst3aK2Zq+qUX3o+iyPnTmXxNFWDqHn1DhbXzx6lyswM8tgRr2cM620S8RIRxwx1jhZx2FCmUJWWjKfwmNFERKTPKCIn67E4k+Ynr+vx14mIbNveZ0xEFPD7WRwU54Ln6vVmc9l9lvFZ+jskctum9XpZvl7b4WdirrVd1cnZIRa7UX5WWWH+/oiIfGJfnFy3KkMeP+st8amYjje58pMTZRzTlQQAcGCUxnTPXFZezuI3mrapMpkUb7sc4u1qa0dc1WloTvB1UJQXaJSjBiKi9SxKUVaVSFGzoV5vup/To5q+NaoRSsRQqkLE5SKu1FWOqOJxhh+nyPGzVZWM2H0nmVZl7F7Hqju3XW93mHIci0ZaPWP7iKX7e7/o72XfLcdAREQ+i88VksmEKhMOibGHx/v/ZKpT1fEsfj6nU/yDTSZzqo7Nd4VChoGz38/fYzqdYnHCcH1Gwvx8Tol9sSig6mQ9sZ2U3l+9HT4edLIpVcYnhoyuJa9h0/eS+efmGi5xy96z0Jff/zYADoZxetFpXxcLDG38utU8bnlAFNDn3YA0/kYtWnITP7eWPD6TxeFi3e9e+a0zWRybqDfliClESjT7lmGyGBdlcobTPrrzhkNukA7J+8GObI58I3bd/zC1J7wtk+25SVBMmJ2s7pePmcE/2NniRs2yVXq902eI7YjXZ0/VdWZPPYTFi57V+3/yHH4vKiCa+J/+5F+qzmGl4maVmOcaZsLk9ufPSMQ9AEeM0TzZKRjkxL4YR37ivkLOdEEUqDOu+hpZO/va1asf77N8Y4ce01wb5eP6076wkMVzLrxQ1YnE9NiiT6U8/NkfvqGKZNbx+UNj/SZVpoX4WO4nt/B+YuENt6o6CfqDWGLooxqaWFhXqucLb4zk11KqW7YXHXq9A5BItgzKegpdf5oxq48YAN7r5FXb99jv4OL9pyXuQ40PyftSRBUV4rlq+XgWV0+qUXW+84uLB7qDfcJfGgMAAAAAAAAAAAAAAAAAFDA8NAYAAAAAAAAAAAAAAAAAKGB4aAwAAAAAAAAAAAAAAAAAUMAGNadxe1DkDW5ex+JYjUj4QkThMP9N8mMrdGaBYpHwZJ1IZRY35BydP/9sFjdsfpjFiYzOdeav5HljMqWqCHXwdGi0WqTeyJboPM61Ffw3x7e9uJXFhxly8Kx7/CkWNz0/XZWZ9r/8eCY289cTOk0P5Sp5LPOsNeoq9LKIo4Yyx/X6v2Gzw5btG022r+ek8Fv63M2Ic9PyibxCPp2nx3Fkvrxkn/shczQZUg+rfMQy95MpF5TcF1NOY5m7z5dpZXF6U4OqE6sQ+SFFGhlTFuGczEdsSIZni5qWyJ3k5HQuP/mO5HvuShZQAjEAeM8pLytTy2SO93Ra9z/tDs+J6ojsccmcIZtciI9h7ADP6+hQm2EPZS6WrYYyfSg5Vy9rl3mQX+nHimR+YtOIRK5HHjvDKOcNmQ+NDwYd33xdR+TqdVzdx1q+PWXy/cgHOFxs29ZJAX83ERGFAkXqdVuMta1+HBpPjAksWx9vn/io21p5vr8tcT5+ISIKxfjOJBN8nNH+pp5PxIr5mKyyQs8N/AF+frS3ixzGcnBFRP4QX29rG9+26xjOMZE72ZQd0hLnXmYHL2XKFiV3L5ERucyNA1FxTezQo73QIXtyP3keclm+P+j829TSzsLw3FNUkdDUWSxu3zSPxe4DvzRsa52I+zNGz+pFjXeImM+76fLvqSri8qOxhi3ZoozMQbzk8Y2qzqsvvcRit0PP+8qPn0NERJ4hB+9w5Xpur7lp33NU2zZlyOV8ohMoien+PiLuM51+5v+y+KlFj6g6+fzf+tx2X85Y0Hd2zVMX/IjFi5+7WpX54lVPsPi4nefOLjJnMBFRl+gZXMOuWD6el88WzXO3oY5P9C2emMubxkXyUwwFZYbowvXqq0/vV/lFL/9RLVt3JW+v/+eG61l8+UXHqzqtoh3Ts5KBufPh21mciusxwbqtPG+wV/MBFtc1nKbqvHDnXWKJYV7SuYZvp9OvioR9fA5UfRY/VhdfpO/PvrF8MYvjG9tVmY8tOIfFM2fWsnjqCXIuA/1hakFlC1M4My2A9wOZS54/gPv0GTpP72ULL2Hxb+68W5VJivv3FeLZQ+XESapOTU0di4+o4Pd+qgzNsi07w7Auc6AgpzEAAAAAAAAAAAAAAAAAAAwJPDQGAAAAAAAAAAAAAAAAAChgeGgMAAAAAAAAAAAAAAAAAFDA8NAYAAAAAAAAAAAAAAAAAKCAjRzMlTVsaWbx7Z+5hsXRz16o6jSV1bA4HverMhfPO5Rvp4Untt6yca2qk/FOY3G71coLJJtUnWV3PMniWGy8KlNSx5Nov7p8Ma/TllB1KmL82fyWF5/j++qoKlTe0MDi5tZNqkzD1BiLq+wWFod8rqrjbA6x+IVcgMUfOn2yqhMJ8jipd5de7/X/jOH14aqzczt5+Z7/+6y+v4Nh23afZVzP2+86Psvqs4zlE2XEeZfJ6E8ul8vKDakyIbF7/mQHi8f69Ale4ueVMkEeJ0mfu+Tjx9ey9HGxHN5+yE8k4AuQJN+3z+O1imx8twYADh7bp9s61+FtZEk0qsokk7xta2lLs7i5Tbf5VriMxZmkHNOIsRQREenxyX6r1eMtqjmMx8vk6MPQNo+dyeNtyw0b2yriEhGnSQvts0xmxQpVI3DuGSy2R+vxbUXZnnHcjqx+fbja3pWlbrfn88s5nno9IJZlc3wckUrnVJ1IhB8/29b9fTrF621paWNxawePiYhilth2iseeYfgVKeb74g/o/SWXn0N1U6pZHO/Q8wnL5uehHeT7Ynv6PdtiDE8ZQ3sihlw5h++vk9WfUTrDK1kp/n5swxitSIxnTePbgL/3sTOMBeE9SJ5kRNGZvC2ed8rxqkztVB7b9EEWL549S9VZ/jyfq9OmRr07iTiPW/6gy/Shduo0tay8lMfF/ViPKy7J5KaXdJlVf+YLmnU71FK/sy/zDDcNhinP9chze9oe19Ntwa7XdklneRvkD+g+tSvNxz1hQxv058eeZ/FTi77e576uWMXjmTP6rDIgzz33ZJ9lHEeO7VwR6fZcLjL1a2T4DHrzGcZkPt8gzKEN9x4K1eK/Jml0aAwREU2cqF//1jd/wuJ7f/ljVWb68bxdtQJ8/pAyfMzyNpPpXp/uBfqhJszCMIVVkdlUqpb1ds2N56pldVN4m/r7X/1GlYnXPy6WRFSZlMfnNynRVGfmTld1Tph+EoujM4tUmdIQv4cbDA/o6EE/yNYDrQkMd58851s0auccuKRM3x8qifH2p2JctXh9nK5TxtvhqnLDhmUzpqekWt+POfo059yb3/1KDiTxeMXYocqGSpYx3Y4bQngaAgAAAAAAAAAAAAAAAABQwPDQGAAAAAAAAAAAAAAAAACggOGhMQAAAAAAAAAAAAAAAABAARvUnMYJ4nl4b9rC49/Tp1Wd2XPmsfjG79+lykRin2Dxq2s2sjiY1PkvGp98lcV+kR4oHNM/xO5zea7ksmyLKlPs8R9rD9o851jj2mf1ept5rruoy/PyBQ3ZFWodvsPpzTq3Wdkq/nv05fN4Xg3HkPeosqKSxXb9G3w7z+s6FcdPZXGpIe0Ge0cFlNTYHjma7J055kz583wiF57jyB+x19wczyVnyvkml8m8TRm3W9WxLJ6kJiq+M+IU8/OJiKhdfJaeoxPdhGy+v53t/D0fFtfvOTiJ542Jl/BcCj5DXr5Yhq/HF9R5JzPOGL7A4deWP2hInBDg681m+PvJpUz5LQEADgxTXr5kko8jnJzuuxNxXmb1P5tYnAscpeoUxQ5ncar5dVFiEPIXGxlyvkbEYGP+aTxea8hluU0ua9BliOdtpnKRfLBF5jkjEqMcUkl4tqxUNaLhC1kci+i8bLVVe/Ylmxlt2O7w1Ol006gRPeesacgYkuMTl/fl6Yzul01jJSmdFWMYP58/+IP6PPS5fL3hgE/EelDc1tbE4nt/fY8q07CNX0tXnnYFiy++4AuqTv1m/r4jxXwsFbL1fCLn8jGO4+rvC+eSvP3IZsXYNaPHfpnMvgf7xcU642tUTNciEZ1DMBrZk3+rK7d9n9uA94oOtSSxdimLG4r1eVdZeiqL54g0wtnTddLO1gTPx9lk6DooKOYzziW6THUVC2edyXPQf/zyybqKiE2zg23ifsOyxa+xOLPV0G/J45cz9LNNK/SyYc7XnSOf09P+up7Ow9tXxnPLkAvXFbXiaT12ipZWsLi6+lMsbm/dquo8tWgti6uqprI41p8E2P3Qnf8Liz91kb5vVjPpGBanU/zYeYa/GZHH0pQ5269yIfPYsg1/i+LIZMniZVdvyefx9eQM49tCtfTFVeQP9IwVZ88sU69Pn8bbzJXHyFaLKP4mb1+CPv6h/GWxzqmeTPF7pCefeqQqE9Td+QFxmiHl8cmXLGBx8fqlqsyizOks3tCm30C2S+YQ5/enZk/TycuPq+HtTplh/5p5t0DlVboM7D/TX8MZMrgDDGs/vfvrNGbMmL4LHmimaWNKxPKRl6n7z4oxc1jnbSZ7BI9l46AfrWmy+5S3gojIXc3Hg1YkZNgXEXfExQJDpvUgv7fgJPh7ziYNOzOE8JfGAAAAAAAAAAAAAAAAAAAFDA+NAQAAAAAAAAAAAAAAAAAKGB4aAwAAAAAAAAAAAAAAAAAUsEHNaVwvYpnJNDy9TtWpFalL45t0Hp8r/udbLM5kec6JcbbOJ9b+d57HLpLjGQ2Ky3QekGNEXrKYpX9E3Z9qZ3F1lv8Qe0lSJ1cKtTex2A6J9Rry3MZEfr9jPZ0vatvzj7DYm8ETYlhB/ZvqTpr/gHtlCf8NeMvS3yOIr/07i+uqdM4Pv7Vnf7cXUA7YzI428uV63q+fdN5JW+TA9kRuPNvS564rcgJlnC5VJityMIn0c9RlTAAgctSJn8+3DddRNMpzf9eEJ6gyKotghbjOF5i+m8I3HhTZlAKk85SH/DwRQsaQ30rmaXJELqVkRl9ruSy/1uRqHUt/rlDAxsi+w5Atp1PngwIYqLBft805kUs+k9K5TeJxnjNlUwMfX5XPPUHVCYrcW+2tm/u7m/tJ5JwMmHLhifY6KZLhbHusH9sxJBKMimW+/lyvMv+MXK8+To2vbWDxieefqMrUTtqTc2d7ekc/9mN4SOdyNGpn3sK4IS9P1M/HprbNpyueIZllRvTvphzHaXEOydVEIjovUnGxvv74dvWYd/HTL7FY5i82kemIu1LynCPqEnmEQ0F+HgYNaZ3lVKbEDqgyToZfA8k4347PdHk6Yr4j5g+hkB47hUL8WPpMOUd7DcJMeUzhfWLlj1hYL2IiovqHea7hzK9/z+JakeOYiKiu7lgWJ+L6+kv+i+cNrrvgYlXmnAt5Ts4Tx/HXK/WmVdYxU88hr+OG19awuKWBx0REtEncQTHM+QtRJt1JXndPK+043fp1kZ/dEm2+4xju4wR4u+QZ8sDbYh7+5a99k9fJ6j4rIfLjvbqexyfPVVUGxV1//JRadt/DfCyxpbGZxXZQ9wFpl/efxizC4ljZogd1HUMH5PI23JGdt6Fv0c2+Yb0F6re/+MrufrN93onq9Vgpv38SyrWrMskmfj48dOd3WBz0j1V1/AE+1pgz4zuqDEX0eXVA6Fs7dN8v72ZxddUkVebhz1zD4ude0vnmb/gJH7NUT5vCYjfD7zcTEa1bzRuD1R16wGrbJSyOln1AlYH9Z2opZHMyVK2J7LUHKY09wP5rJyL9+GAPeXLK6abpIpF9te5aSD0KaX2Hx0HDfDowSiwQ7aXpNrx8ZuEaRiziGYC6h6TuABCRT5SxxXMPTyZgJiJHjN8zhmc7KV5vS3MTiyun8H6FiIjSfF/aW3m/3RY/sPME/KUxAAAAAAAAAAAAAAAAAEABw0NjAAAAAAAAAAAAAAAAAIAChofGAAAAAAAAAAAAAAAAAAAFDA+NAQAAAAAAAAAAAAAAAAAK2MjBXJnMUy3jjv99SNVpzK1j8X87EVVmXTzB4g2bm1h8XCSq6szyeBLqpo4ki/+8ao2qc0JNDYuPipWoMhMry1lcb8dY3Ozo5NfBmmNY3D55HIuLduik2hEvzWLfaytUmS5RJuvw7wDksiIBOBE1i/cdtC0WV5bx90dElE7zMsW2Xu9hFXsypnebkpEPU+90tpHjFRERUSajP0e/XcbiULCCxUG/Pl88j3+Oli+gy1CWxV3pHIszPv0ZZDy+rDXLP9dghl9nRERHFfM4aadVmViJOGeKS0UJQ6L5F//A1yHqtEb0ebjJb4sl+j1aHt+Wk+Vl0mm9/6kUbxvI4sfFcwz7D4Wrs/UgbdgyLMO5WQiSSd0227bo7w3tYTwlljm8704n21WdcOpQvqBxfT/3cn/x8Ra5hvM7HudxMqfL9MlwjSReE3F/1iOvezlW1fvW+vzTLC7/yjxVxt7L/4c727bJHtXzjtPJpHp9mzgYY6N6nC85Dv8MwkE9vvKJ0yyTyYgS+nwJBPjO2GIljqO/f2v79FymL888u5jFNZNqVBl/cCLftp9v28npdmCk2L1YqEiVyYT5ODOX4u/ZZ+uzM5Pjn5slthM5RB//gPhMUobPvvdnktshPx8YVhr5XOChXy1g8cN/WKiq1JzJ4wvPnGtYMV9WbSghZ/jyijV9q172UvI+BxHRh/ltAfJ9+1IW31um24bVTz/CF6zT9yiofdPO/+SJqMuw5eFne7qTunfOwXyWoYf08U/AFXNAw7BIlfEMH3TW4RUdMQ0vieq2Let2snjRs/9i8RGTjlR1qsrUokHxiXNHsfju3/E3sKGhQVcK8jIZw8FLeeKMl0UMx9IT405P9LFjgiFVJxDm10isJKbKFKpbbvwsBYM9ffi9P3tEvf7Ifc+zeNxh+lwtKQ3zBcmNLBxfM0XVmTePj18rq/S9qYNl8bKNatmnbvo+i2un6fFU8OlFPFb3mYhOOKmSxbbdxuKwre8JRMWhyZI+Vo0b32Dx35bre6uw/0x3SqR+NFv9Wo9U3HcRgANj5TtEwV1tiuEmR5nsd0UbFRV9BBGRXGQaAAfEGCwi5nDJZlUl1crn7oEwn++7Qd02ej5+1bbH9f39UDF/j+3ZJhYHg4YxZYK/qdBLvI7P1mOnt2x+fJc8r4pQKMwHe1XVfAbSYenj4ohjl67kx7Zkzgy9oev0osGCvzQGAAAAAAAAAAAAAAAAAChgeGgMAAAAAAAAAAAAAAAAAFDA8NAYAAAAAAAAAAAAAAAAAKCADWpO42Liv639cR/PozEnwfOwEhGVVYkfSH/kAVXGEr/FfvFVX2RxZsUSVSe8+kEWH0VVPD6L500iIgqH+W+fF62qV2WKxc+qj+loYXGqc7Oqk2jjx2HkSdNY3OnTSW06PX6s3mjWv3Xu2jwrQ9zlxzIYHqvqNNfz9VSKPDGerXOftGV5zjGnWX+ODc17fnc921U4OcjCo6op5B9NRERFvm71ei7Hf3++K5XZZ0xENFLkjrNkojgicl1+Io4UOfaChmQcjsXPD9fh53u5p/Nb2k/yfGM//vYvVJlwjJ+/F5/Lr63KsM4zQP8W+Wc+OJOFkVk69+Mbpfz3/13HkNM4y4+n6/HjlMvqc9eR6xF1ZG4rGMYO4znH6S3d7h48pvzFMjeezg8J739ZlXdV52Rqihty1hTzcU/dx6azuKxc5p8nWnzXH8SSDf3ZxQF4iYfPmdrZFhGn+l5tSORiq9L5I+0IH/fYYtyT2SS3S0RNL4oFcn8N12fn6yx8bukqVWTaJb3GzQWUorz00EPJ7+857gFjunbeD1sin2U0pse38da3+CpCOg9SMMTHQV4rH/ckOnQbKsdpoTBfhynfbzDQdw5mqSHH+5uco/dlvMhD1dIh3qMhN7gt5gpuTo/JAj5+8h0xQcxLgjp5lT8s8ilnxXYMOTFlW+bJHKTEx7dqfAbDWsko/nm/uiKvyvhCI1hcPVmv55RB3as95AxCZ2MlkjP6CnGb47jZIikzESUT/Jps9Ezn/c6+rdshevFBw+vDz5tvvk2jbMMccic3wPtuy+Ltn21om9UyQxmfWJRyeDvlZPR4KxjkH/SWlkYWL3la5zSOnc/jiCGN4ICINKt/fZbnbt2a0WOp0gox/zHk+/PbfDwVDPJ+LhrV463pM/j7rhWpZcv1bSdDBljtoov7UWgYyr3jkJXruX1bWlmpXr+wiueBr1K5K4myaX7+emK8Nb7qKFXHL4YW9Q8+rnfOz7O6NsdlO6b3Zc5sfu84OEmvti+nLNCVHv7tr1m8Yb3OEx8M83O8vLJclamcOI7FjWvXsjib0POFbIRfFyvXrlZl7v8jz0d94kdPU2VgcMhRcZOIf/zHt1WdbevXsbgykFNlIi4fs4cifJxsB3VL1trCnxPkRKLY8dV1qk7Q5tdNOqfHB8EYn9f6Inx+7xk6F79oe2VXGDDkrJXtgOEWtTrgbj/mtvLxgzx0s/peBfTiUJycnSPWlqRuo9wOPgaoGlfJ66f0/PmpBtGOxXR77s/weWvlJD6uaGxuUnUyGX7inbzgPBbnXD0Xvva677B4U7Mek/lCfL3hUn7dnDjvOFWnvT7O4tPmHc/iOXPPUHWKxQlfe7q+t6a9w6JNmcWqRFXwRLFkIouycqA3xPCXxgAAAAAAAAAAAAAAAAAABQwPjQEAAAAAAAAAAAAAAAAAChgeGgMAAAAAAAAAAAAAAAAAFLBBzWn8/X/y3MLl//MzFqcfeE5XEslNWkj/prcjEr2VzbiDxatf4blaiIjqiecZqCb+G+XVkz6j6lil/Af1V993uyqz7XW+LZ+f5wxMk8690xRfz+JQ/AgWN9o6z8CmNr6/7a06t2ZdXS2LYxU8LhN5OIiInAz/jfr2TZtY/OTiZarOho08r0P7xtdUGXdL7/3Vv4M/XKU6O8jblX/akNhB5mOT+dyM+dpEfmJTur+swzNreXI9ts6BmQnw88Hn8N/6DzU3qDplf/kNiy/TaZupxOJ5BMJrVoh9MeSHTIl8Sg38GvFsnTukbdoxvExW5wrx5fj77hQ5jrMZndNYHjuZnxA59d4vePs9/aprWLz6lVdVjWnHVrK4soxfw4989/8GZc+GjrzOdQ4ynXsV3m8Sad1utXbwz76lRX/2FRU8V9iaTf9kcTorcssTEW2Qeev7kUd4UKw3LNP9WJ8c0S+k9HEJVFSyeGxM5PIbx3M/ExE1NXWIJXFVRuPbfuh3d6oSD357z2fU2dmPVQ4T/mARBXYm1AraepQz0uPnvG3zcWVJjOfOIyJKxXlO42Ra50EKBHnOxUiI52SKv63rZMWQtkjEjmFckcl0qWX7a7wh53h1Dc+5t2UFH1/5LD0ucrL8+Hal5blM5BPj9kiIHyfH1uPbiDiWGXm9GvKY5bJ9X9O905JapgEwDFv1z/Bcvf9lmN9ffdOPWHzaZJ0rdjDoq1rnQtQZXPUoTMamOpUlPBNysqpWlWlveKPnP92F833/7918EYXDY4iIKGVoOmRzJ5ucpG7OaZvoujdt1e1hTswl/WIc7aQMddL8/s90kR/VaX1H1fnB19tZ3PhPQ97VHJ8PV1eJrNmOfpM5h4/bPno2z5c6+/QJqo48vNW6ix0SrYYLramex4kDm7rvPe0jUz5E4dE945YTpp2oXg+K5sFvGEen03ywadk8eWnWpxNNyzTrJQFd5rY7/8Dia+68XpXR+H3UL55zhSrRuJ7n0jzlpJks/vLNX1V1zplxMotn2VFVJlIqxjA+PV+u38zvjfnE9fjy8/reQlasJp3Rn0FVVSXfv5lTVRkYGlUiPuf0Q1WZ/7yT38t8YMn3DGsawBz1PUVewzKOkSavEcN9p5F9dB7GMQxfb+DoBSxeuf4GVWPKvrdS0P77iZ+Sf9SoniCoP6Oud/i4Id7C55IZnz63fUfy88GL6qTXsQjf1lER/pzPM3z0x8yaz+LiikNECRkTfeQLn2fx0e362ZMn5q3HzJzE4vmG+whPtT7P4liMj7da1CyAaKy4TgLUn5zGh7CoKjjbUEb2WXzwGiCdU3ooFc7MAwAAAAAAAAAAAAAAAAAAFDw0BgAAAAAAAAAAAAAAAAAoYHhoDAAAAAAAAAAAAAAAAABQwPDQGAAAAAAAAAAAAAAAAACggI0czJX9+I9/YPEpD9zP4qspq+o03/8dFh9BriqTE/ELn7ycxR20VtWpELElktUnf/hdVSdAPIl2gjaqMs0i2fUZ376RryOgk40f1cDXUx8JsPjX/9is6jgTeeLw/7jtblVmwamH8zridffFlKrj29zE4p/f8j1eZ9t6VUeLGZb13nq+H+sYHrZ1/ou63J7P02dZuoBPJIn3h1lo2YY6Dv8ks1n5yRLlHH4tZXIiYX08oXclxxOo+1NrWZxoWK7q1AT5dmqmqyJEIXFdx9JiZ8W+EZG6tMQFG6SkqmJl+XrTSb3eXIa3FqkcvwZyWV0nk+Hr9TxRwDV8RjCIiln02OsNqsRZkw5lcbNhLeKsUz1J0FCnSsRNIr74mh+oOnFxjq1c9qwqs3TNOhZv2cjPO3czf52IiF593rCHvZnOQ9kW6+uGojNY+OnbfsHikooSVeWJP/yGxet/cV0f+wZDad3GLWpZc6vo3+1pqkxdHY83xcexuP7pOw1bE2OYMWfwuJOPTYiIyBflccQwRrB530dRsR3TBWqJ7zWGxDps0b8SkWwJAgFdwucPsTiR4vvi5GQnQGSd+CUWu/EOUcKwIbGoIqT78lvv+fvu/3d1yVZs+Eqnt5Pj7BwrBvXn6BMj/2QHH79E/Po7r5EgX9ae1GPgYJB/9iVhfk51RCKqjhfgdQIB3hanOtpUnebmTWrZ/jK1+EFqZ3F1GT+nmjfpc9fJjWGxK69xIgqISyse58cunZMzMSJnB+/XfBZfr23r7Vjic/UF9OeY6z0XK5zpBBARvSXGU5GzVZFckrcFDh05JLuirySil9fyODZOlzladH9Hi9enzyAlM2Mei7fSPFWmfmeX09XZSf818T7D3g0/jrNnSuzoJogsMW6oKhUFyvrexsliDtLDtKy3iWrJ/b/8B4v//NunWOy39LgoFOFvoDig56gf/NBxLD7l7A+yOCpveBHR7/7I7ysFIhNYXN7X29sL2aP+Tdy+qq/frups2sj7wuZNTXzf9LCI/OLii/hNA8TClNzSQl6w53h4coxMhvtKhj8PssTCZ5e/xOJf/f7Pqk7tVH4eHjVRzqCJrrnzRrWsb/x+1a2P6Hk3ER9vL258kMXLn1+marRtFteAHo7QxZ+8hMXV06eoMtGYuFhKxL3iNj3vzuT4HYiEra9rz+Hv2zHcKoMDY35YL9vw/LUs/smDn1Flnvzd93mdl/h91HiiXtWxxLOQoOhrUsTHN0RE1UX8Pk11je5/wmE+j/LZ/DxNp/R5Gm/nLXrLVn5OZrtbVR3ZDSdNI6XuRrFAthWGewRRfsPijw/fwGJ9ZcK+LNnyMlkje/qCj3/28+r119fzczNl8fOn7vhjVJ2Vza+y+OWNL6oyY8fxz/bKa7/O4uPoWFVnIE3fV667hcXx1/R1E63h72HyVH7d+H9ylapzWtlJ+9yuS3qMY9HofdbpH8NkQhmM7Qwc/tIYAAAAAAAAAAAAAAAAAKCA4aExAAAAAAAAAAAAAAAAAEABw0NjAAAAAAAAAAAAAAAAAIACNqg5jZdfz38zPUo8ScAiqlZ13hD5uQ4z5IkZJ37tPEU856VNOhlBQjwPl7/k30E652uxyAp8FOncfdb4mSxe/dIqFi9e/Jyqs0Vsve3EU3iB43XuosdvOp/FhpQvNOPLv+QLfnYPCyOGWsccx5Pf9C+HscglU32JLuL2+hw9h6jp4X6s9/1v27ZtlOnqyQPgM+Rvs4M8P14gLHK+mbLWufxz8wyfo18kZ3FIxiLHJBGNC/NtfTjK1xtuNOSlCE7i8Vad55tkCkB7K48NqR5psoz5eTmm6ihVpSrB93dkUOfEaAzwc7VjK8+DYwe6VZ1YhOcrjIh8nJmkKbMZDNQpl3+LxU/fdVOfdWR+YkMKL5IZJF8Vsci8TUREsrWWZ9Rh4VGqzlli2afPP1eVeUEsWy1SbTQ3/FPVaV61ksUbnuU5mppfWqHqUFocmajus6ady3PSRsrKWRwYx/NFExFddu23WbxIXFfLfnS13hcYMhs26izeKZFYqLRKfwfQJ9re0xbwsVLtlK+oOg/dzHMP2VN5ruTKcbpv8ZJ8jFNScrgqExRpwUTXSEFDyjpDqlu+b6Z8xeKS8Bm6WE+UUXnsDWQK3ZxIP+wa8qWViUNVbUjjdGKvvjDV2Ulf/0Lf+zIcvJNI0KhRPa2y5+gPPxjh56orxvSpjE5wGQjzkyrk6Q/fcfg4QubdDQYMJ12Qryckxl+2q+cgOe/dJ6h75LFFatnYHJ//lGR5z+aGZ6s6b4jOL+A3nKzimkgm+RxpW4fOhyaHr0XiIrZtU75Fvu10VvfM7b3akx1Ol94u7AeR675WZ4Yrm8LLHHP8fFXmqbvu4AvWPSlKDDCHd4jPf6+47XYWX3upmIMQUYu4tJp02j3KiVy2utcyZtXj6zVMvN1mPgfa2hJSZbK1vP+rFLc1DOkTVRbDTYYBa9fO/cnqacywdf3//IlGjdqZv9WVMwEi+xB+NMeW8E81YGrPxXpChr7k9Vd4bkrH4R9IY4PM2Ui0YtsSFldE+bn9za99T9U562x+cjQaLqOH/sjzBnY9+yaLN7e1qDrdItftl7+i760NxLXf5/e8GkW+YlMee5/IsWuJ8ZZt68/IdNsAdtqRJdr5+Yb8eoxj+/hn7yP9mSxbtZbFl37/66qM9NKWV/ZjJ/dHR99F+vBA4+N9FzK0m4t/+U0Wn3Gins9fffUXWRwRk5cTZugxl+fj57RjmGO4YgA1rrJSF4KDRl5ZV59fqsp8+PyfsPhX1/2Oxese/o2q43Tw+by8nxuP63FzTpxAW5r1PYFoMR9/x2L8PC2RE1IiisV4fxkV9z/Xrdcd0jY1YU6rMvquHR83xeZcoWo8/eJXWTzdsFbov9VPrqExY8YMuH6ctqplr6bWsNgX1j11q2jPb/njzSz+6Gx+T5KI6IpxC/e5L6YnVfElT7PYqrxIlSkWeYIbXuLv6a6f3KfqzP7KvvdlcPIXvz/hL40BAAAAAAAAAAAAAAAAAAoYHhoDAAAAAAAAAAAAAAAAABQwPDQGAAAAAAAAAAAAAAAAAChgeGgMAAAAAAAAAAAAAAAAAFDARg7lyhdc9d8s/ktzXJX5TcOrLD7MF1NlxqUdFicdnsg6267XW0w8UftY8XqYbJL8xJPI11BWlTlmS4Kvp5Vvu4TWqjobxHrf8Pws/vi3z1d16kQ88aaHVRn62ZV6WS9Jw7IXnAhfUD2Dx42rDLWKRZ1yXcSq3vN/p4uoybC/w5DP8shn9ZxrjpNRrztpvqwz3dr3OkXsdy1VJmjz89cvtlMWb1d1PuoPsdjyxJZW8QT3RERUzuvQNF2EEiKWl1ZVqaqSyfDrZsmbfP+r1m9TdY4p5+ddpMqvynSL+I0O3g7Y5Ko6Y8uiLK4YV8br+MQ1A/026/Ib1LI/3fVtFjeK1w1nIZ0l4rcMZW74I29z7n34WV7A1uchkTi/R+dYOGdOlapx/aWXsDiqShBtFnHjK6+wuHn9alUn2dzMYsdN8QLVum+kTW0sLCsLqiKt/1rH4nt/xrcTHDdO1amo5O97TKxMlYEDJ5nR7ZYjuoXNrctVmfa2c1ksug2KidOfiGj2RWeyeD0/XShiaA4DwdEszuqhE/nFtqPiVA3q5lzVsfSwTQnKS0AfOvLJLlWUMW1H1nHk5ai7adUV2oZj13tb/Xl/w4Vl+ciyeg5aOqXHTh3igNs2P1mTGT4vICIqjvAPvziqD2hbOx+weA4vY9v6g/Qcvq1g2N5nTEQUDYn9Tasifbp3+dNq2WmT+Mm6ZdVLLP7QZyapOoeN4yfrW1v1sXM9cRwCfHzokq6T7OB9VFgcp4BfX9Tys24xjFU7MnvKdLuGxgT2YQqLvvjMShafskCf34eJ2NCU0bjan7I43v4dFtuGr6A31PNxTsDW58PRU6az+KOn8L6kwrAvFaKNj+thD927Is/ilU8vVmX8OTF5yXawsKVZjk6J6mbOZPGJ516iysycyGM5RmxRNYiaxGn+8ktvqzJ2sKcD2bF9h2ENw1Mw6KdRo3rOG1sOYAxaGjayeIsYVxMRrX+tnsW5Tv2JuMTPhYi4D1JdXk0aH483J/7I4m/cpPf/hb+fxuJgQK+1ftUmFq9r4GN6X1Cvt27mcSz+zvf/xeIzzj9S1YlV8nj1er0v7R28vfaLz2RXn95bRtyfcHbwfsIN6HYh6/C5e8AtoMFRH7rcHFk77wvl0in1uiv64Rzpk+rWe+7tYyumXkA28npM8H636AXDvUtxPD86g/dZU6bwPpeIyPH4Oe/59PmbdeTkxDAJgve0mTK+4VIW3z57gapz701XsbjE5c80Tp5Vq+psi/MJRH1DgyrT2srX07iZ94VlMT1QOqashsVeK39y0OLp5yuk2hNT28zP7cgx/F7g2he/qmoYnizAQRQjfW/w5LBeJi3OPMjiaI6fH7d87XpVZ/YfTmRxLU1g8e9//riqc+J5t7D4nl99Qe+M6KI+duaPWLzo50+qKr+Ywtvhq066SK+3QOEvjQEAAAAAAAAAAAAAAAAAChgeGgMAAAAAAAAAAAAAAAAAFDA8NAYAAAAAAAAAAAAAAAAAKGCDm9N4LM8+dOXPv8bi3zzLf2+fiOjENp7jtXVipSrTIHJeJV8Tv+Wf1Nl7k2Gef6a5g+eXiKd5vhoiIurg+3JWg0zWSnTcqXNYPPJFnk+sU6eqpFbi225az/PnTDPksJFZSpp+8gtdaCCmijw8rTxvjDmnscxpoPPPTVmwJ2+D25Wmep0+aljKORny7cxL4rk6eaIr8u55Iutu1jMkXBRskduHiEimT/ba+GdU/6TI50pEGZEz64Rx/HpNxnVunOpyngvP+v6P9Q5OkXlfRfK+x/6gqvxa5BGoK+H7suzhF1Wdc37Cc2C01Okcr2MSPA/PV2Z+ntfp0DnKton2JSM+Myerz3fYl0Np1/eRXhH5i4l02ybTZpkyhd0rTt7/ueQ0VSYp88+9qvsbTeY55tdRPHSGqrFpBk9M2tKsr+Gb7+L5orIPDEKOd9NXvETT0Fqv89xQvV7Ul+ZjeC7cYDHyeh9MdsAwSPDxa8IO6txDTz3Gz7uKcTxX0rGzPqDq1PIUR7SRD4vo9X/o872yWOQ/83TOxaYkL+OU8biYD9mIiCgici4XhXlsSIWnctjIvMhEOlubfEeWIVWbLT4Cqx9fuZSbVrmUiejJXkOurgHkvX2/8vl85PP1HETP0Lgl0vz8dnI8NqRxJMvi7XkkpJN2yyFXOiV6JE9/+Fk5Bgjz/bVcXScSEvnDBumz/fnDK1hc5OMnZvJpnk+KiOi8L9/I4va2nCqTTfOT0xKdSyisj6UlknDv+jx3SST03Kx3vmIiomRWH7vuXuvtzvc9Ri4copE86jxVYvqpZ7P4PJHDmM9g+++mk3jcJPbF9ClVEM/n12oo85aI5elgGr4cJWJDSmOqnTmCxSsf1+f8o7/m8+rUtlcMa+Ls4DUsPvlSndO4pI91jDEskznP461Nqsw5l8wgIqKuzlF0Vx/bGC5G+QPk9/e0caacxp1v8jzCix/8M4stn+5b6mr4ICdSNl2ViZby8zsUlGeZXu/yO+5Ry3qbMmW8WnbY4Xy9nqvnmyf/xwwWN27ezOL1DfoqKS6dxeJNW/l9p5/+Sudxbmjm64mU6Tb/vFPPYfELT77At/Mvnn+ZSH9uJaV87l5Rpq8Yv5jb5DoKaHDUh4atDRT09wx+nYw+Xxwx8vzrWn3f44WW1/rYiqlFL8y+eNHLPJ9mXYxfx1Mm6ZzGlOXjVdfLqiJxcV86HERW16Gi5ngHaLufXyDvMREdN/0hFt//o5+weMOLvA8jIsomeD83MqBHPdPqKlnsWmJukNX9p1d6BIuPmsHv5x5Vyu/NEhGFS/h5Wj5psipTUs0n62dN5K8fqOMPB15NkN/FvXPhL1m8aWGzqhMlfT+/tx9/Xz8De/rxZ1j8g+/9S5WZXnski5Pt4h5vU5Oq8/NrH2DxnHllLJ5izd3nvu7dZhHLa1i3Fe81+EtjAAAAAAAAAAAAAAAAAIAChofGAAAAAAAAAAAAAAAAAAAFDA+NAQAAAAAAAAAAAAAAAAAK2ODmNA7yPDEPvcRzODQ9tk5VOS4SZfG69iZVJhzhievGlk1jsVejf9s/EeHPw5sdng/FMuTlPSHAf+//Op/+Lf9pc3mOgOVfE7lkDTmNZYYXf5LnVjqCtOfkgm2mbFAD8OLLPH6jr7wmREQiF8fqB1SJ8g/tyVDQnc0OJI3m+5Lt7/nXQ2dpsD1+vvhcfl5mDPnzco7IqZvSuXwiPn7OJ3P8fH4mrpM9vlzO8078oI1vZ3WTqkKfmMIzoP1uyld0IYVnD1m+9peqxJereFvhZPi194t1Oi/SVy+6ksXTF56qytxz0w9ZXEz8PRcXf1DVaQ4sZXFLM7/W2hM61zPs3Z1/f52CIVPWth5/FYdzyWqeR25bVp/vD/zoOlFphSozMCK3fSXP1XLKFJ1jaP3T97H4pz95Xq92i84v+67p1OZD51WeCzcTlXnL4UAKhfUYJxjiY6e0p8+5rZt53rrlG9eweM3fdQ6V+Wd+icXjRBrNeEZcM0QUEuOp6mqdnyYaG8XipubtLJ41bbSqUy4uv6jYF5VmkIhCIm2TIVUyRUVsyonZF9lzG1LsKoaM49TSq+L2AupqPM8jz+tp1EYa8k56Dh8np0WyU8c25EHu4AfQb8jBJ3MhOzZvWLM5XccT2/ZckZ/bkAM7bIf1wkHwUlrnCe7tued033j7c2ew+PMLf2CoyfPWp1yRQzqi309EJCJPp3g70Lx5q6rjiWFyJKjHqnavuZjTjexnux1xOgtP/KgeA886aR6LZdumMysSBQzLJNmiy1iP2HU7a5geU5NIb+YXl7UzTtdJiFhnKyaKiMvk06efoMqUtPCx55JH+dh/S6fu61Y8uYjFv4novs75xtdYfIZ4D6Y+qVwcLNvwnf6WnR2IYYg8bNn2SGMu413GHMLbrXPO4Tl3I+J1IqKgGDi4slMgopw4/Nkcv3I81zQgl7kd+f0V15AStkvkpO3o0LmGc0FZRsxR23SOQL9oVu/5SX/y8O1/rj6/cwqLO2fpayYe51ds5UQ+sDuqSo/9LNGnptt2qDK36jSHBWFH8m2y/D3j6cRW3RhsTfCDd9+Sxw7EbhWM1jQ/5vGUPue9HC/jM7RhaYdf10dMxTx7qMjW+mCOKqeJAcCUm/h91Vv/wO+7EhHde/N3WLyh8SW94saNLBwrxvT/75YnVJXLvzqbxXLUse9MswBaJU3d5+tVdEif6/jNSzyPvNOyXJURt+rp/j/pXOB/iYRYvKmpkcVBP8+/TER09ER+TdRaMmf9QGdRE/su8h6HvzQGAAAAAAAAAAAAAAAAAChgeGgMAAAAAAAAAAAAAAAAAFDA8NAYAAAAAAAAAAAAAAAAAKCA4aExAAAAAAAAAAAAAAAAAEABGzmoayurYWFkNk8MfdTM01WVwGs8Ll/9D1VmW2sLi1PZNIvT6YyqU2nbLK5z+etuJKjq1ISjfN86WlWZTRV1LF787FN831QNLVjuZ7HeeyKVrv7ic3ShRY/wON3Q98bfeK3vMn1p19tp3fyj3f93d+Tf/TaGCZ+ff9a2x6+JgCtOTCLq9uVYXFJRrMpUxvlZs/L3D7P4vLf0vnzU5Z99S4Dvy726Cn38+KhhaR/aNrJwVulMXWbjEhY+t3gFi9X5T0TZdh4v++HTqsy5azpYfOV/XsLiE2bz65eIKN7RxuKulCNK6LYC9u78Iy0aM8YiIqJHGt5Ur//n2bwty0yKsDhJ/PwnIqLmfrRtA3HY8Ty2s3yzm/SFlHltKV+wpUOVkUpE3G4s9R6W2HSw96CgBW39/T4nw6+TxoYWVSbk422X27SSxRk3ruo8FeDXQG0Vb0OPnqjbw7qqMIv/e+FoVaZeDI4eeCTJ4pGurnPEOB4vf4XHrze8reok43LcptuTQIAfzzFR/p4qJ5aqOpNreD98Ypkq0qca07Je6+nUh2DYcp1uckf09LXuDtnnEpHDxziWxT8zz+LjFyKiZJKfU75cQpUJhUIsjoT5XMHz9Jgsl+Xnh88SY/ic3v9oZCxfsEUVOWBy8vvBWX3dyyL2aH6cAkH+nomIshn+vtNZ3nYYPlUSh5tsy9O74u5ZtsPFfGKXWR85kcfzTlBlKmr5dSHnl4bRFekrqW+ylW3crMssb9rO4rSjt+5z+R6GLX7W2Ek9B0lmxHWdSaoyEdEWNC5/SZWJ/2M1i1s7+VwgZZidT69ZwOK6GcerMpXj1KI+lVsiLo6pMutW/JOIiJyutHptuIoecggFAj1tUSqt37ddwtupippqFvuDerzSleLr6Ujqz7k4ws+7w8p4h19WqjvraJi3ka+u4ufcf5z6GVVnojhZbFu3d464x9WW5Of/gqSeg4yvmMTi+rX8dd3LEQXFsKfVMLXZ0PgOi0tKDmHxxKpDVZ3Vq3gbn07xa/OvL+n5XdDlF0R5bAADrmHq4rPOpDGje87rRQ/qdu3bD32PxaZ+GAZu5SZ+/VW16yMcCvIBlW3rK65uAe+/K6Yc8u53Doxt23uZGBLT/7vkWFVmzgJ+V/SrC69VZZY9dSuL24m3s6tfWa/qfP2rs/u5lwAHzlMPivushqdrnWIa+19f/qoqs/SZJ1ncFufjxdNP0s8lP3nJ+Sy2VInChb80BgAAAAAAAAAAAAAAAAAoYHhoDAAAAAAAAAAAAAAAAABQwPDQGAAAAAAAAAAAAAAAAACggA1uTuMWnpd0hXg5IH+4n4jqpvK4Y+oHVZnmX27gdap4rp+cW67qHDGZ5xkqr+Cvv9qs96VlI0/gcu9q+Q6I1t/5GxYHpx3D4kBovKrTVb+OxSmb5xno1LtCbSKe/rWbVBlH5N1b970vG9Y0CMThPe0SXWTe2VW7/5/d7tKG36/ThYahbZ3bKeD05DPJOfo7GFmH506yxPc0Qp6+KCrKeC6lCkfn65q0ZhmLZ/fnR/dF3qZpIl/XGZWGOvHbebxR51empHjfcZFvbBPPG0ZEtO7+tSyW2Quy1B/6Ta9r5tt++TWei9UJ6WwnPo8f36DIA+pleL5O2LcOIure+f9b77ldvd76Om9XLXsaL5AwNM6t+hoYFG+JNv4tfuY99BPdB5QNIAFgQPa03cZiAEaeo3PuNf2ricWpZctUGfdont+PElt5XKS35S7+PxZvPZGPtyo/NEXVuewTM1hsar9t0VzX1fLxS9jQzP76l2tZfPuXj9GFBqDkmIUsbn/1WVFCZuzs2//99H617Mtfumi/11MoLHskWXbP+Ceb1ee3l+vfKKC3pMhmlknqHKqRLC8zJsJPPNeQAdAv8tNZtszDa8hpXMrHSoENOrdm1pAzdSjI3Kzr1jypylRWzmFxrHgei5MpnU80HudzpkyGHxfP0fmKkzmem8qWDQMRhUKR3f/v7kZGxl3mn8Lzb511ts4fWitifdZpMiXqq0v/rsq81sDndG+18HGabbgGjqvheVWrbJ0XW+6fX0yJylL6HSTiLSzOOnrbviDPXRZv0+PK1Q08l2qiH9fjJ752I4u/fPakvZTcO51ZkGiD6HJyGX29tWxqIiKi7tyBaTfeC+Z/5FgKhcYQEVF9g06y6wX4Z19dw8cVMZ0Sm0S6a1NKbDVekd2RzzDnnjOHt5l1U+pY/NZW/bn5xekdDI5QZX7zwENiX/jOfPxTOleyJd5AcyPv917fqPMIOxZfrz+qrz07wseDGfGWbMN9vqPEZyKPbaLN1Erxvru8tNRQpkDZfiK7ZyL6pZ/9QL2ssz/CYFrfxJNpBh/W97jGl0dYbAf1vae0w6+lOSd9aBD2Doaj6fxUoSee1M8EbvgZHx/+6ubrWfzQI3eoOvdv/gKLL544wB0E2Kt3RHxInzWWPrGyzzIdbbynq52kbyK1TB7L4tiEGhaPiYrEyEQUT/IncI3NfOxRLZ4nFhL8pTEAAAAAAAAAAAAAAAAAQAHDQ2MAAAAAAAAAAAAAAAAAgAKGh8YAAAAAAAAAAAAAAAAAAAVsSHMa3/zJE1g85YrLVZX1k2ex+M9X/0yVcf7C80XUVJSxuMHTSSbnX8sTAFxXwfMOVRh+k7y8gucgi510qirzceLLZNYh/evoRB0i58tfn/4ni01P7ttF6p5Q/B1VpltlPn73qq7Qy778JZ4furaqRJVxeuUu297pEFFh5DQOFhVTINiTp6u96S31evtWnizJyfH8S8dU8nOZiKis7Q0WV774oipTEhaJgxbw3+mnZ3W+ImoRsbwGdPo5okdFIqc3r9dlPnIKj5t57rCO3+sMXjL75qOGTffJkG/psDJ+blZP5Dk9g4aES7bF2w8/8bxrjiHvNOzdatqTp6525kz1+gvjJ7PY3VAvSux/LsuB62NbXXqRPYCcxs3IYQzvQna7TrqXTMi8u7rNtz2e15F8opHvMo0++Hpz8Sb+6mbdHi5fwXMaV1XptdaJlHSbRGK7akOd0PlTWXz7tTxnE6V1btb+sFUKzGIR9yenMe9/zjv7/AHtS6Ea6fPRSN+u80+fh65rGpDs4RjymDo+3jin0zqPnCPyltpiLBWK6rxIXoLXcV3Zb+h9sURy1g8ffboq89yGB9Wyd6tWnctE9cQnFIsa16gy08VY9ZxJs1mclIlADcss4sffZ+vP1ROHynX1Z0QsN7Xp9cJ03PG8EZ0+gHWsWfFvteyJX/2CxcueX6LKJBN8djulhie/O20en+8TEc2p4gll25v1DNkV50NU5EyNFItkfkTU1N7O4nVNW1WZ+q2vsfi5l3SOtMauPtr5Sn3NRqfzewmms1Omu5VTr0dfI2XFS8+yeNumjapMdmc75O44kGPkg2tMhCi8cxgzfZpu2xKim8hk8yxevkIefaJQmI+LgjKxMBEFA6NYLNupjExyTETNrXxbso9KOroNbeKnMgV12m9at5GfG/IaCUX0vnSl+LZDZTzH6nHlfB5GRCSab8oYhocyTb1MU15iSD0su+omMVR1MnpSde/DPP/mmqXP6RUXqPPOvYzsET25r5vQPw6q2Wfp86xITHm8dfeyuLZcX7RBm48ZE+mEKtP8t5f4guxF/dxLKHR6lkL0Q3Gv/ozLn2Hxymf12O+IAs7PCgfKIX2WWOHw52KJN1b1Y718PlFaqa+KY2w+tpt1yoUsrrP6TuK9opXvSwvpOUk5DSQZ+DsiPmQA6ziw8JfGAAAAAAAAAAAAAAAAAAAFDA+NAQAAAAAAAAAAAAAAAAAKGB4aAwAAAAAAAAAAAAAAAAAUMDw0BgAAAAAAAAAAAAAAAAAoYCMHc2XX/XYGi8Mxl8WHxZapOi1rVrD4/jvv7HM769/oe18WL3uYx3MW8gLTputKU2eysLK6WhWpqzqExR7PsU2Nab1aR8THLfgAixcu3ajqtNz8MxZXfXimKrPplef0xvpSxMP5VwdYfMEl+riUxvibdBxv/7c7TI0OjqWiYM8xbG9rUq83NSdZfMyMOhZHLH3ChFbxa6KiqVVvWH7dw+Lh+pSuMkVe7YbVKlUi/lBElwny90h/X8/ClQld5Tci3tSPXdEyasm4KD8QIVEmm7T1amxx8Cx+cF15AcM+/e8P7qSRgZ6GpuGvS3WBLa8d4D0auGClPt998aShJMDQsSxLLbNt2Qno87KsJMziqvJZLF6zokFvrJOHXo63oUGbjxmIiP629B8sXrlMjxFerqhgcXlZMYubDJ1ASRmPt6aeYLEvq+vI4UnGVEZ0HTG+KxQR4zoiIsMieBe6nW7yjejpXNNJfe5mkh0sDoX4uRwOh1SdnMPnHCnDZ09+foIkM/xkGFsSU1U8sd50mo/bMhm+r0REbXE+CIuV6vnE7Dgf17/01gpVZn/Vk94XOeoxDWlWd/ELsHzFUywuLudjVyKigJ+3Ba7HtxRw9feSg2F5fHVbEQzuudp2OIM6TX1fCxqGr32Ji3j1ipWqTGNDPV/g6f4mFuON8XEzTuDxzNmqzvqNTSx+3TCXaU/y68/n97O4okpOQojqG/mc+c/P6Lnwmi1r1bK+TeFhib5mH3qWbzt16iRVZlwJjxsbebxuDe8viYia1vF5U3Ljar3eSE+b1+3sUK8NV/9seIeCwZ42wrb1BeAL8WWWmM9FInoMnxFtfiaTU/SltFsAACMXSURBVGWKideT2/YZ9kWWkXHN7BpVx3F4a9za2qzKtLfy/iZo8xMsFh2t6ojdp8aNvF9obTfcAAjya8/x6XagK8fb9EyWt996XErks/hxSLfxmwJBsV0iolyObzvx1it6fwtU6p3Eu755GxUj2oThfkoh+nCpvk/9vVtu4AvCJw1gzaY2e9QA1gN9MY1v+7prPYCh1XvenLCIz51wcHYEChyfC9+x9j5V4oVF8l5U3zfebZvPy8v1VIFqp35QrFXef95qWPM4Fs0s4882XfpXn/tmspqeZ3Gr287iWdbRqk6MKsSSFhHL14cW/tIYAAAAAAAAAAAAAAAAAKCA4aExAAAAAAAAAAAAAAAAAEABw0NjAAAAAAAAAAAAAAAAAIACNqjJomrn8JxHEZFbK5jRuSwDrkhaV6KKELUblu2vZffsOzZo6uey/TWQ/K2bMobfd1++ap915v9A58856/SpLC4v40nfPEf+XjqR7RN5ygI6R5C/V+6ekfnCSQK7bMnfyPb3vPeWrTqRsO3neffKRY6jT5aUqzrhk0RusA+UqTLUJn+Hn+ejqZ0yWdeZJPJVvyZyJwUMyYdrxfdK0oa8N4/zXFxZkSrppzpdFK2TC0SuberSddRXXFKuLpPheRGSbfz4Rkkfy7Q4XUX6wn5kVoDeVn3vqwd7FwZNpknn2mw68LsBBW5sTOdZrazkuUzWrY6qMofFeK64SJg3ops36Uy9CZHT2O/j/Vrja3rcsey+X4glOkcdjeFtb8UknoCmpEznZomW8vb7v6/hucTm6HSSmikZcbFh2RBI9qPzcHoNwVJ6CDFsjbZ85N+Zq7vZkNM40Ufu+EhE5zR2HD7YSGcMY7IAT/TVIVIAR0I8fySRzneWyfIPtuVNPXZKJ/l6YsX6pDvm+NNZPPIlnnnt1XZ9re37qJj1ZwwjR/V/Xn0Xiy+zP6PrRHkOWNfi170Xkhl1icZ4/DOKjatUZZKpPfkscztGmHa3IK175d8srj1T56iTWUgbxEfg+HU/EYzytjm3SedVdcU8sOUdfn6vbNR11q3lc4PFr+g8vatV7mF+ttaWH6/qtLfySUbc26zKDAzfX1q5XpVYtJLnT1509BRVpnxiJYuDQX51JdvaVB0vsYXF/ozOOdvh65mcuG63em3Y8u0g8vW0GfEOfTPI7/KxUSTCz2/HMP9sbeEXheXTfz9h+UR+X4f3Je2GnMCppMx1z+fLjQ1NemcEJ5dVy1yH93UNr73F4oceWKvq2GL/PY/3HBlHT7JdmY/YlGvY4S2MI/LWyz64Z9u8Xwup460HaSccP5/FR0/UNwdv/9FlalkhuOW/P0uhQE8+3BUrdJvqiF5g2vEnqjJ1Igf9l77yLRb/Zt1aVecT1XzM7gvqz+3edbrN7E1niSdqNCwbCnWGZZ84i88pPjTVUCgs2+vSAWwd+YsPFFP+Ynm3ULZAwzGnMcB7Ax+/VBTrZ1PLntF5jjnd14jhFUXDqgjpO2eGZyPKdhHzsV6OtpAUVK2OHsc1ufz5xJ8XvcDiltqNqs7nJ32CxQ7Vs9hGTmMAAAAAAAAAAAAAAAAAADhQ8NAYAAAAAAAAAAAAAAAAAKCA4aExAAAAAAAAAAAAAAAAAEABw0NjAAAAAAAAAAAAAAAAAIACNnIwVyZyUpNFERbXN7SoOj/40Rq+oH0w92gYefl+vUx8euWf4fFZZ09SVcrGOSz23DSLfTb/zHoK2byMz1JFco5j/P9w95e/rqYRVs93LyrKa9Xrs47lCd/Tzy9jcdPmTarOlHH8eNNEW5WhsPi+R9lMFlqzT9F1jj+bx2fLFPF+XYfE9fmrG1WJ1CtJFn+7mb++2PTVFHma6ZzxmswzbzgsbqaNxW80rmDxEdZUXckXFOvlK86R24+dAwAYGqWlJWpZMMDjdSXFqoxt5VicaIuzOJdJ9LntTO4NFpeUHaHKhKv5/vlkm0pEti2WuR0sTPNuhIiIPJcvfGFpBYvnHnWZrkQrDMsG36/u/QuL582bo8rE4/w9xmKyzyUKBUfs/n8BDZ3osEOiFPD3nBPxqD4urXE+Nk1m+MHxJfUJE7T52NSlnCqTzsiY19GjW6KSYj5gaW3jn2t9vR7H2X4+jrAjUVXGFw6zeOb881jsPasHRsu2rTfs4b7NHDOZxcmMnK0RNXZvZnFYvB7x67aiehK/7ueffQaLHWpSddYve4nFp51zoSqzrX3P/qW3p+jWP6kiBemrn72cxbf8bLoq44h2NtPWyuIKr1OvuLWBhQ3tfZ9jL9zP5wbW/fKMIXIp1ed6+lLf8sq7XsfeyUkEv84rQvo9RaLi+KY3qzLJVbw96HT4HCK1jffDPfgEJzBST3BaulPGssPZa+sbKBAYTUTmewtZ4ue3LGIZ5p/ZHO8XugztoV3/hljCVzTS1p+PLfqfRAfvw3I5w3ZsPu+ORvTYyfLxbQVFmUSHbpv9VhGL2+NbWJzZofuWyg9UsziZ1P1nIsXfk2WJfQvq+wg+8R5dl683mdT7n3b4ejNxwwCxQE255ks0ZkxP21SnbqYQER0i4ryhDP8MIiF+TonpBRER/e7h+8SOzFBlWqtOYPFzb/Dx+HVnzVV1Lv3zi4atcZ+YtYDF9y1/ts860lnzF6hlVz+2SCwZtd/rNZPH3DS4l+246agDALyfjWDRiRXzVImWRtOYuLeJasmWJl4n4x6pq5km9L249I7eFzFvraCQqKPHcbp91887P2bx/mdNRSOLaybpZ3ZEpSyySd5nMjwIGUL4S2MAAAAAAAAAAAAAAAAAgAKGh8YAAAAAAAAAAAAAAAAAAAUMD40BAAAAAAAAAAAAAAAAAArYoOY0DuR4jpRMlv+YuBOuVHUmTuP5gNpNaR/kT5037f++ve+N1Lmh5l/H408uPJUv8Ok6MndMMMRzyfmDZaqOK5IEpdN6valeeW4yqW71+nCVO6SCRozsuYySh5aq1199jecKG7viNRb/b0L/Nv61r/O4Zrxhw/JjskQi4bVP6jp/+LpYB88PSdOm6joiD9/dz+v1PiQ2vUHmENApmjRH5nLR+ZZmHsN/7/+8j+sckmMr+P5WVPE8CDI3FBFRVuQba2/lidUTSZ6/EADgQPL59Pf7duWD3a1d51Vt2crbw+Iwb2dzub6TyTvOVr6OmB4j1E2dwuKSWLkqEwzwnMvFIZ6LraRMd3SxUr6tVFIOBg9M/mKTl196nsVXXKrz9FSPO3S/1jl6UEfk723prgQ5O3Mblhyux052Cx+rtsZ57sq0qycL5aX8HPMb8k46Yjwr8ytmMrpOguQYge9LmxgzEBHFyvk1kHH1etPv8OsvJFPc2TrXsxxOXXjcuSx+cqXOAXvauZ9nccqQE7V+Lb+WMi3rWPzUC0+rOh+fwHNgPvQHPt69+tqrVJ2gyMlkBfTnGA3uuc5H5tPq9YLVvpSFrUuW7qXg3jUYl777vFiDkb/YzJTnUZ4zrqFM38I+3r+MLRP5w2zd7yZF+5E0zIeTCdlP9ZWvTcsWzjR6nzoTLuX8PZ+vZ/g7h6w4F2TLFgzqCWhA5Jv3XL1eW/QdMjbllXbFolCYj3EiEZ1/NpPhfYBrOJUdh+fz9atLQt9H2JbkdRJv89g15PpTKaMNhaLinoAljovpL1HkkfJE3+14+lja1lgW+22dX7xwjSeiMTv/b8pvvkPEpvadn0RzPnQci5972ZBL3i8+g+aNqojMYTxdvP6Jj1+m6nxD5DSeJ8Y0RES/e+UhFl9wE78BumF9k6qz5HE+Zrl/ic6DfP2LL/MFc/U4fmDkxWT4nFxx3VrIaTwYTKMB1QYdiB0BACVtuL9P7fI+u+yz9Dxx9d+XszgR/5AqUyFuLbyQepzFixYtUXVOnj9brONkFofVAxgiPcaPqhKWqHf1ND4/LqZKw3pHiVgeJ72doYS/NAYAAAAAAAAAAAAAAAAAKGB4aAwAAAAAAAAAAAAAAAAAUMDw0BgAAAAAAAAAAAAAAAAAoIANaga1oE/kYwjzXDLR0kpVp7Ka5xxtb9msyrS28XxhbyX476E3N7eoOskOXibexl9v0un/iBpFfDBTaZXw8NM3TFRFzjqX56S1xFcAsp7ORxMJ8hWnHZ7ZwfP078ZbYsWZTE6VIavX789bI/Trw9TYoyaTb1TPb85nXH1cmjt4zpIVhhzGkvwErjbk+T5KnJuNIrXMlMm6jp0S294qspu16rxbj+T4xp9p0DuTkGn3RLIQy5C+wE3IJbzQKeedreo8/eCjekVK2z5fbYivU8tamnlSZn+A51IIecilBAAHTzqtByNOVjasOvd6UzNvaCOTq1jsOobEdiLvSiTEx3HJdpHEnoiyIu/78rhq4Km25mgWF4d5xxGP6/eYk7kGPcPY4yB5o4H3JWte+7cqM27cOBaXREzHuzC1tW+lUXbPnCF6iM4RVBbl552X4edCKKCPpZPieRt9ts5nKeXEGMfv96syoQgfA5SV8f0tjuk5SCwmcj8aPvqMGJNlEzxXcutb+jo65eizWVxUfBiLzzjhE6pOMMgTO7XE9VgvkePjnrTDj3eLIW/mDfd8n8V1RXw7rRtXqzpVR9ax+GOfOEeXmbhnPaOGKlVuwTK1QYZJxn6K+ivUMlskX23v1H0UkWzT+XlYXV5FkiWyFna06fO5vZtfS0HS+cGjMZ4D3RbXfjqjz3mZfzUU1PODaFRsS9RpatL3OUx5abVdn1OeiLr6Uf79z7ZDZNujiYjIMjSiQTFEcDyZL1evM+3wY+1T+YqJcmI98TjvW0y5kmVSYL+41lx5k4aIbLF/tuH6TCR4P+ATZVpb+blORJQTw8PDjuTXUTA6hqScuBa77X7kCrf6LiPzNAdEv+wzJhjl73Gkv+++vHBkaM/tW1O7Idukvu/JnXXJ+Sxu+Ke8IUq668j0fZN0Wu1UvmD2bFWmbCRvL2sn9p2n8Yxrb+Cxocw3V6xl8YwPHaPK3H/fwyy+eNByGsuDZTjJrdAgbQt6MzUnspWSLbGpFcNsDWDwxciQuz3E75VQWs4n9Fy4ofEFFvusr/S57Q+Fp7G4/Hw9b1n2yiIWJ0urWRw0/L1tgvhzyo1Ur8pMES1Kp3jI2OisUXVm2rJnk/t7YB9U4i+NAQAAAAAAAAAAAAAAAAAKGB4aAwAAAAAAAAAAAAAAAAAUMDw0BgAAAAAAAAAAAAAAAAAoYHhoDAAAAAAAAAAAAAAAAABQwEYO5spcy7/P132OTthsOVkWxyIRVSYUsllc4eVYfNw0kUCbiBzHYbHn8jT3mYzel9bWVhbnHP1MPZn0WLx6XQdfR5uqQsR3hWy+K3TiSfo9n3L6DBaXjLNVmWBELuNxNpky7Asv43P4sXQ69BtIebyORXpfAvaeZZ7tqNeHq3S2i0Z43T2BOC+JiNpJLCsRBTJ6nf7zT2LxstKQKvOlux5j8YZu/vqxW/V668R5V1fDE6qXlk1WdZY0bmRxR7H+7Fub4yxu7tTb3l8nzq1Vyxq3Psvi5cuXqjKJDL8ei0JhFncmk6pOVrQV4WCQryOojz8AwIHS0ZFQyzxP9rNBVSazZRWL623RlqV1e0gU4OtIWCxuTvI2lojIH+QdWeKNVapMQIwPW+gtvm+rHzfsy3vXS8ueZPGxRz+5l5J7XP/tG9Sy6qo9/XCmq+vd79j7hOvlyN05nPYZBkJHxPj5XB7h4xV/gJ+XRESdKb6epB6SUSbDyzhiDpJO67lB8SF8X6KHFrO4tlaPV6IRPvbwDPMfIr7trW1NLI6V6LnB+EmVLG5saGRxTc0sVScZ52M01zA3aKhfz+IE8fjCE05XddYsX8Ziy+ODzOpSvq9ERKfMPJ7F5cUxVaZ50565WGq76bgVKn7OB0aWqxJ+MX71B3h73tmh2+9sN78mYkX6vPMH+Ng/KLZj+/l2iIicnBhbh/Vnbdu8X4iExHUf49caEVFGjOP9ripCzlt8rh4ZE1VlIhG+zA7y92Dber4TCfNjo1shrUgcq3ExOREkymT5Z5By9Dw6vfO+hee51Nayph9bfv9znBz5fD23qpJJ3U8ExGfkE+e7Q/rkcMQiw8dMZPGFAXldhfV4S976CKrtmDbEeYb7COL2FaXTvN8oL+N9IxGRI+7b5MR1lsjpsUYwKu7hWfoc9IvbYj7i15kalu4s1Vsmw+v4svpWpGXxz1q3LoXMoT03FnWbSjRi/1dZM4WF7a3tukwbH0fQtOo+V1s+RYyNfPp6jHfz9a5Zv9ywpo0iLhNxmJSZU1l42glnqiJNG5sM2xoMYm7lGO7H2qVDtO3CZhgOiFaq75iof307ALx7QR+/N5UhOQcRfQ8RBSP8Sp9i6gqFAPHnhdW2fn5YPXeqWMIfqLhi3k5EVEKzRTzdsPVDWBShd1gctF9VNbLUwOIA1YgSfY8pBxP+0hgAAAAAAAAAAAAAAAAAoIDhoTEAAAAAAAAAAAAAAAAAQAHDQ2MAAAAAAAAAAAAAAAAAgAI2qDmNLZtnAMiI/GIW6ZzH2Rwv47k6s4AlEgv4xG94y5w2RKTS+8m8SdGozkdztMd/Q90zZEbw2TxvxsdE3mPH1ZlXvJxYj8hVFInqOhGRrzjr6NyDjiuSx1giX6GtMzJksiJfjjjcfsOxDNh8/7IZQ9Ka3vuSLZycxn7PRz5v53Heoc+XcpGju6VM/H6+ra+Jlx2+7NVWfTybynkulFSQ56LeEDLkd6moYmHVjBNZHJ40UVX58+NrWdyyUucVGAyXfYHnA0gk9G/73/uHehZ3GHIP5kResFiMJznwBw1518U10OHwdRQjsQkAHEQy7yoRkWXJvlrnfiSRKzb5hswDb2rceE5Up/0lHve5FSIinWNv02qdg37/yfds2n/5XUhDYltZZiRfj8zNSkRki2R9citBQ05POZ6679e/UGWqe+Wodbq7Dfs6PHWl09Q9sueYZvx6PC7H/bHwvnO1Eukxe9bSn72cC/h8/HPNZHXuuY4O+WnzzzVSrMfNMj1k8Wg99mjcysdtr76xgu+L4Wpb98gmFudEfqW/1vMcSERER4/n484Lzz1HlamuuZDFy5bysetHPzJP1fn8xy9h8Ugx9k+363lLaxvPD7X+Qb2/vfOHZrKFk+e7b/w8dA2ZPm0xRw2G+LzQknnticgR80JTlqxwROQwFoUcmSiWiHIuLxQK6lzJYZErtkiM0VMZvd4uOae29XynOCoaEMPcNpMS11dWTAYs/b16Rxxyy3T/wcfX0yXyqMv8xUREGXEvJGfIaezb9ZYGkLL0/cq27d25gGUebSJDTmP1eZju44i22JDQUo4sfKKP8kxjBLHp/kwd5Xp8tr4tVz6O5y6XubgjxTpfdzrJr+lOcT+utUPnrC2ZwNeTdnX7/cgf7+YL3uJ9FoVmqjpXfOkbLHbENW0b2jH5kRzYzH3vJ02GZfJ8OKTv1XTwcc/L62QOYaJNrfwcqgocrspcPP14Fh8zmZ+7FNRXxZyj+HzhwnNP0/vnims/zcdBFKnss07t5PGqSDggzqzM23o9PtFeB2Q+ZRPZR5mycst5HTJ3Hyz9uWtt+is73B4E2D8//tHDalmms5HF046+gsVrNui5vGd6FjUk+Py5f9d8f9ryQ1hUQnqOLfuIlrZnWVxeKnMcDy38pTEAAAAAAAAAAAAAAAAAQAHDQ2MAAAAAAAAAAAAAAAAAgAKGh8YAAAAAAAAAAAAAAAAAAAVsUHManz3hj4O5OoD3vLb7H9gT+PUv3Udn1fG4guduSWzm+d2IiDa9JnK8hQ2/jZ8UuYVFypV4h87L90LDWhb/7VkeZ4cqnaKhlZl/Os8J42TFe3Z1TszKiTzX5nhDHjPH4e876Of5anKOzouQSPC8TZkMz5Xc3Cxy5wAAHEDhoE6651NZmFoHsGadl8+cA3h/NQ/COkzkex5oThvxvrudfb2612W9eTlDzswcz53Ykm5TZRrf0ssKwbZt22ikNYqIiMJhnWd1pMgcZHsid6XhA3FEHtBEW4sq4w/wz0TmVA0ZxluxWIzvi0he2d6mr712MRZZ+Y/1qsySlfeoZX2JidzlCeLjlxOOmKLqXPbJ81mcNeRtrozw9dozeW7Kxtf0/heL8dVZ5/O8yJmkzom5cvlyFoct3bYdUVW1+//pzHb1euHi57fTrecP8W0pEfM8wpYhfzi5/DNwu3XO3XbxWbqy7e02tZB8W4GRer0hmatcnFPptD5XPdeQhFaQ34j3ZQ05l8V8QOaXLTLk0E2LvN2RiJ6H+AN868mUmK8Z+i3P4+/J9I3+0M79cd0RA+rt34+KgkUU2H3OGs4xcSp44sDZprzULj/+AZ+eu/vkekXOblNeO1st5XUcw/zTssQE2XBu50TOcS/Ey1g+/R59Ylk0zNsBv8zlSkTk5+uNtxvOsree1ct6S+vX/da3+b6IHMyu8WMV70k3HQXMoz0nvs5nTaRzx/floe99j8UrPN13h03524UTaipZ3B4X6wmLHMdEdOLxM1j8kSmT9IotnleSIuN0GVUnz/eldYsqUjPzGL4gaDh2KZH/OyDnN6Z9iYlY9gFERAkRH2IoA/vLNDqQy/qaz5n056/skOMYgIvHeTv81asvN5TiY7KjampZ3NCo5/KZxEa+HcNFHXvfX5D8fkR56ZkHaT964C+NAQAAAAAAAAAAAAAAAAAKGB4aAwAAAAAAAAAAAAAAAAAUMDw0BgAAAAAAAAAAAAAAAAAoYHhoDAAAAAAAAAAAAAAAAABQwEYe7B0AGDZyOgt74oU1B2FH+ic7ROstq42x+ITZVapMichOP7GihMXVVbWqTt20eSxubU2qMhknwfeljK+3I8lfJyJqatrE4nSKrzeXGaojBQDQt2hQL7M9TyzR/c/ApAZpPYUlazxuOJYD1ZXJsDiX3f9+2EnrOraY9oQi4uJy+l5vIBBgcTKZVmXufer7fa+oD6dUzlPLvn/zN1jc2trK63xuoV6ReIsPXPczVaSh/jUWZ7bneGx4j8tXrWZxc7qDxfGkHqOV+Pixq6qoVGWqq6t3/78zjWto73KGZXER8xPazYX7sd42tcTtjgxgX/g1nO3WF1c2YYslARGbPn9Zxq9K2OIr8Y6nz1/dZ/L1Fhm+Vp9I8DmE6+njEomGWGxZ/H1ns/y4EBEFLD4niob0eqPFUSIi6u526HW9a8NSsChIgUBPA2ZZ+gNxHX5s1ajI1edcwObH2s7qsZM8K302X+IZxls+4uu1LH5eOo7eF5+P1wkE9XssKzuMxWOLS1k8JqLPFVucc67L998hfo4SEeVs0ebndPs9ECGbH4egn3dIppZD/k2LL6z3t2AtfYVo9M5jWFOtXy8TV4Gl20fp5JmTWfxY9nxVpmRaiVomxcS1ddNdi1gcrdVjmtUbeZ912TzdPpKzkcd2VBQwTJKWvsDCTFz3a5XjYmqZEhCtQfwtHkdka0FEtrgmXX3vSf3dlmUoAvtN9gFEuqeXrazp0PdnRi23hY8QgLv9J3eLJaYx/TgWnfzRU8Truo31aDqLY7j4hhz+0hgAAAAAAAAAAAAAAAAAoIDhoTEAAAAAAAAAAAAAAAAAQAHDQ2MAAAAAAAAAAAAAAAAAgAKGnMYA70N1c85k8Ya161nsdm4eku3OOesktez737+exbNqpuz3el3ieYW3OTIvG1FHnOckSyZbVRlHZBhpbOTrzWR0XrN0MiPK8DyIuaw54xIAwIEQNOSAkfnzBkbmhiQiGpw8dgD7MnbsWLJH9uTak3kpiYhSKX7OB4M8Z13SkC/XEnlBydXZzSJiPZbLM5dlDfvSuLGRxYEAzxF471P3qDr98cX5X2Xxz267hReY1Pc6Wh5+nC8wpPaTLrzhS32WaX6F5zhe8tgjqswbm5tYvG7VqyyunVyr6owrKWdxTU2NKuM42V7/x/hr70zf+5bnr8yl2J/jacgpSTJvqiGPoyK3ZUoYLsvIbesczOEifpKPCeqTPhLgx8ZxdJmAn7+HYJBvq8iQR7Vd5PYORnSZaEwcKx9vhxKG3Jr9yWls7/y8nQLK3Xbtd4492LvwnvfckusO9i7s083XHXewd2FYufHm75J/ZE8jUHek7j/LS3ju4cqKclWm7KS5LI5M47mRzyoxDCQaeP9OLY2qyPzjeZ7J37/ExxGP/lGPIyjB77kYczCnRJm02HYDv9dDRPSNL/w/Fp91rs7THLnkHL4g06y3Le81OaKPctt1HUv0u4b7VSRz20/SbT7sv9IRIw72LgDATv/9jU+x+IqFn1JlSvqY617+mXMHc5fet+QMKmlKDz2E8JfGAAAAAAAAAAAAAAAAAAAFDA+NAQAAAAAAAAAAAAAAAAAKGB4aAwAAAAAAAAAAAAAAAAAUMDw0BgAAAAAAAAAAAAAAAAAoYCMP9g4AwP5bt+zxQV/nhVdcqpb9z/98k8U1ZR8YlG0tWfVzFje3rmdxLtep6jiuy+JopESVCYdjLHbF67at9yUU8YslvJbneroSAMABUhZxhmjNIcOy1ne91kh0hlp23Q3Xs3hTSzOLw9FiVeeIqmoWN21qYXF9w0ZVJxbgfcCUqhpVpqSM9x23/fz/WPzSch73x9VfuFkte+LpxSxuaHpxv9c7XD209Od9FxoKrx+czZrcuuRHPD7qR3sp+T61ZPDHqdBbf/oFOegN9qOMqV+Q9Xgda6QeXPsDFl+DLUfkRH6bl7HFIN3y6e+2+yxepzio31NJNLqPvTVvS6435+jjawcjfP+CAb1/Nt/nTCbNYs/TdVyL18kavtPfnkwSEVG3261eA4DCcPWXPkVjgkVERBSoqNIFysp4LNofIiKKjeXxxjd4bGhTqbWNhfHl61WR2LSZLH78T7ezuH7FGlXngd/dz+KmtpwqU5UULXhLk9gwb5eJiK679hoWB2Olqgx1xHmc3qbLZEUZ2Y/ZYV3HEe8hFddlkuJzqdLvGwDgfU00jyWG5vK9LNkhYsMtMifJ43i7LmOHtrM47fA+IZnLqDotcX7PyyH+PKIspp+DDCX8pTEAAAAAAAAAAAAAAAAAQAHDQ2MAAAAAAAAAAAAAAAAAgALWr5+nzufzQ70fMAwN5/NmOL43Z8cOtSyd4j+f0zla/2z0QGxPd7G4K8O3ncvpn4WTP08dsPX+Wj7+8z7yx/A8w6/5yZ+g68rwdWS79HYGajieN7sM5/cGQ2c4nzeD9d6y2axh6WD8ROXQ/PR+Pq9/hjTbxX96Z0eO9wG5rP5pni7xs3qyjLNDH5cdPr7ebFb/NF8mU8Ti7u53/7NwOcO+uN7APiNcEwDccD5vhva9yXWb2ny5rD9leBufz+vvoMu35Rn6Bc+TMV/PCMO+5EfwFZt+rtnp7sdPd4/goU/sn9Ot19st33e3ft8jxPfxu8W+uIb97R6x7zpEe97nrvq4JgC44Xze7HpvqcyeseaOtB43k7hvQ5ntuswokZZLrqfLsN4MH1unsnrcPCoj6qX5vqS7+DqIiHKinU0Z5jud8j1tF9vxdDuc6eLr6c7obatjlTYcK/mzoSrNgqGOLJI2bHu7WNaZ0mUGQSFcEwD7YzifN7veW2fn4NyvL3TyMMoug4jIEV2A7J6IiEaO4IW2d/P2PpPTfURXF68jf55apr55N/pzTYzI96PU1q1bafz48YOyU1A4tmzZQuPGjTvYuzEkcE3AQOCaAOBwTQBwuCYAOFwTAByuCQAO1wQAh2sCgMM1AcD155ro10Njz/PozTffpHA4TCNGjOirOBS4fD5PqVSKDj/8cPL5hucvoOOagP2BawKAwzUBwOGaAOBwTQBwuCYAOFwTAByuCQAO1wQAtz/XRL8eGgMAAAAAAAAAAAAAAAAAwPA0PL9mAQAAAAAAAAAAAAAAAAAA/YKHxgAAAAAAAAAAAAAAAAAABQwPjQEAAAAAAAAAAAAAAAAAChgeGgMAAAAAAAAAAAAAAAAAFDA8NAYAAAAAAAAAAAAAAAAAKGB4aAwAAAAAAAAAAAAAAAAAUMDw0BgAAAAAAAAAAAAAAAAAoID9f2eXARESOQe5AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQBRSenSvD7P"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNM71q8bwyoP"
      },
      "source": [
        "#**View an Image in More Detail**\n",
        "Here, we look at the normalized red, green, and blue (RGB) color channels as three separate, grayscale intensity images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mP_wDHrAQl4",
        "outputId": "0e290f04-0f56-41cc-d796-b83f510d2039",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        }
      },
      "source": [
        "rgb_img = np.squeeze(images[3])\n",
        "channels = ['red channel', 'green channel', 'blue channel']\n",
        "\n",
        "fig = plt.figure(figsize = (36, 36))\n",
        "for idx in np.arange(rgb_img.shape[0]):\n",
        "  ax = fig.add_subplot(1, 3, idx + 1)\n",
        "  img = rgb_img[idx]\n",
        "  ax.imshow(img, cmap='gray')\n",
        "  ax.set_title(channels[idx])\n",
        "  width, height = img.shape\n",
        "  thresh = img.max()/2.5\n",
        "  for x in range(width):\n",
        "    for y in range(height):\n",
        "      val = round(img[x][y],2) if img[x][y] !=0 else 0\n",
        "      ax.annotate(str(val), xy=(y,x),\n",
        "      horizontalalignment='center',\n",
        "      verticalalignment='center', size=8,\n",
        "      color='white' if img[x][y]<thresh else 'black')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'images' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-945ecfa1cd34>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrgb_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mchannels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'red channel'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'green channel'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'blue channel'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m36\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m36\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Global setting: adjust as desired (allowed range for bits_to_keep is 4 to 12)\n",
        "bits_to_keep = 4\n",
        "\n",
        "progress = {}\n",
        "\n",
        "def extract_components(fp8):\n",
        "    \"\"\"\n",
        "    Given an FP8 number in SEEEMMMM format:\n",
        "      - Bit 0 is the sign ('0' for positive, '1' for negative)\n",
        "      - Bits 1–3 are the 3-bit exponent (E3)\n",
        "      - Bits 4–7 are the 4-bit mantissa (M4)\n",
        "    Returns a tuple: (sign, exponent, mantissa)\n",
        "    \"\"\"\n",
        "    sign = fp8[0]\n",
        "    exponent = int(fp8[1:4], 2)\n",
        "    mantissa = fp8[4:]\n",
        "    return sign, exponent, mantissa\n",
        "\n",
        "def extend_mantissa(mantissa):\n",
        "    \"\"\"\n",
        "    Extends the 4-bit FP8 mantissa into a 13-bit fixed-point value.\n",
        "    Form the 12-bit integer portion as: \"01\" + mantissa (4 bits) + \"000000\".\n",
        "    Then append a fractional bit of 0 (the binary point is implicit).\n",
        "    Returns the 13-bit value as an integer.\n",
        "    \"\"\"\n",
        "    int_str = \"01\" + mantissa + \"000000\"  # 2 + 4 + 6 = 12 bits\n",
        "    int_part = int(int_str, 2)\n",
        "    return int_part << 1  # Append a fractional 0 to get 13 bits\n",
        "\n",
        "def exponent_shift(val, bits_to_shift):\n",
        "    \"\"\"\n",
        "    Shifts the given 13-bit value right by the exponent-based shift amount.\n",
        "    (No sticky bit is captured during this shift.)\n",
        "    \"\"\"\n",
        "    return val >> bits_to_shift\n",
        "\n",
        "def second_shift(val):\n",
        "    \"\"\"\n",
        "    Shifts the 13-bit fixed-point value right by (12 - bits_to_keep) bits.\n",
        "    While shifting, the dropped bits are OR’ed together to yield a sticky bit.\n",
        "    Returns a tuple: (pre_round, sticky)\n",
        "      - pre_round: the fixed-point number now has one fractional bit (rightmost)\n",
        "      - sticky: 1 if any dropped bit is 1, else 0.\n",
        "    \"\"\"\n",
        "    shift_amt = 12 - bits_to_keep\n",
        "    mask = (1 << shift_amt) - 1\n",
        "    sticky = 1 if (val & mask) != 0 else 0\n",
        "    pre_round = val >> shift_amt\n",
        "    return pre_round, sticky\n",
        "\n",
        "def ieee_round_value(pre_round, sticky):\n",
        "    \"\"\"\n",
        "    Rounds a fixed-point number (with one fractional bit) using IEEE 754\n",
        "    round-to-nearest, ties-to-even.\n",
        "    The value is assumed to have:\n",
        "       integer part = pre_round >> 1 and fractional part = pre_round & 1.\n",
        "    The sticky bit is used to break ties.\n",
        "    Returns the rounded integer.\n",
        "    \"\"\"\n",
        "    frac = pre_round & 1\n",
        "    int_part = pre_round >> 1\n",
        "    if frac == 1:\n",
        "        if sticky == 1:\n",
        "            return int_part + 1\n",
        "        else:\n",
        "            return int_part + 1 if (int_part & 1) else int_part\n",
        "    else:\n",
        "        return int_part\n",
        "\n",
        "# ----------------------------\n",
        "# FP_TO_FX is now the same as the original process_fp8_numbers.\n",
        "def FP_TO_FX(fp8_numbers):\n",
        "    \"\"\"\n",
        "    Processes a list of 64 FP8 numbers (one row) according to the following steps:\n",
        "      1. Determine MAX_E (the highest exponent among the 64 numbers).\n",
        "      2. For each FP8 number:\n",
        "           - Extend the 4-bit mantissa into a 13-bit fixed-point value (format: 01MMMM000000.0).\n",
        "           - Shift right by (MAX_E - exponent) for exponent-based alignment.\n",
        "           - Apply a second right shift by (12 - bits_to_keep) and capture a sticky bit.\n",
        "           - Save the value from immediately after the exponent-based shift (stored_before_second_shift).\n",
        "           - Round using IEEE 754 round-to-nearest, ties-to-even.\n",
        "           - Check the bit at index \"bits_to_keep\" (with index 0 being the rightmost/fractional bit):\n",
        "                • If MAX_E equals 7 and that bit is 1, undo the rounding (using the stored value after exponent shift).\n",
        "                • If MAX_E is less than 7 and that bit is 1, mark a flag for the post-processing step.\n",
        "      3. If the flag is set, then for every FP8 number in the row:\n",
        "           - Use its stored_before_second_shift value, shift it right by (13 - bits_to_keep) bits,\n",
        "             compute the sticky bit of the dropped bits, and round (IEEE 754 round-to-nearest, ties-to-even).\n",
        "           - Increment MAX_E by 1.\n",
        "      4. Shift every final converted mantissa left by (12 - bits_to_keep) bits to align the output.\n",
        "      5. Truncate the result to exactly 12 bits.\n",
        "      6. If the original FP8 sign bit is '1', convert the final value to 12-bit two's complement.\n",
        "    Returns a tuple: (list of 12-bit binary string results, final MAX_E)\n",
        "    \"\"\"\n",
        "    progress = { 'extend': [], 'shift1': [], 'sticky': [], 'preround': [], 'rounded': [], 'bit_at_index': [], 'value': [], 'final_fx': [] }\n",
        "    # 1. Determine MAX_E among the 64 FP8 numbers.\n",
        "    exponents = [extract_components(fp8)[1] for fp8 in fp8_numbers]\n",
        "    MAX_E = max(exponents)\n",
        "    shift_flag = False\n",
        "    results = []\n",
        "\n",
        "    # First loop: process each FP8 number and store the value immediately after exponent-based shift.\n",
        "    for fp8 in fp8_numbers:\n",
        "        sign, exponent, mantissa = extract_components(fp8)\n",
        "        conv = extend_mantissa(mantissa)\n",
        "        progress['extend'].append(conv)\n",
        "        shift_amt = MAX_E - exponent\n",
        "        shifted_val = exponent_shift(conv, shift_amt)\n",
        "        progress['shift1'].append(shifted_val)\n",
        "        # Save the value BEFORE the second shift.\n",
        "        stored_before_second_shift = shifted_val\n",
        "        # Now apply the second shift as before.\n",
        "        pre_round, sticky = second_shift(shifted_val)\n",
        "        progress['sticky'].append(sticky)\n",
        "        progress['preround'].append(pre_round)\n",
        "        rounded = ieee_round_value(pre_round, sticky)\n",
        "        progress['rounded'].append(rounded)\n",
        "        # Check the bit at index \"bits_to_keep\" (index 0 is the rightmost/fractional bit)\n",
        "        bit_at_index = (rounded >> (bits_to_keep - 1)) & 1\n",
        "        progress['bit_at_index'].append(bit_at_index)\n",
        "        if MAX_E == 7:\n",
        "            if bit_at_index == 1:\n",
        "                # Undo rounding by using the stored value from before the second shift.\n",
        "                rounded = pre_round >> 1\n",
        "        elif MAX_E < 7:\n",
        "            if bit_at_index == 1:\n",
        "                shift_flag = True\n",
        "        progress['value'].append(rounded)\n",
        "        results.append({'sign': sign, 'value': rounded,\n",
        "                        'stored_before_second_shift': stored_before_second_shift})\n",
        "\n",
        "    # 3. Post-processing: only apply if the flag is set.\n",
        "    if shift_flag:\n",
        "        MAX_E += 1\n",
        "        for item in results:\n",
        "            sp = item['stored_before_second_shift']\n",
        "            new_shift_amt = 13 - bits_to_keep\n",
        "            mask = (1 << new_shift_amt) - 1\n",
        "            sticky = 1 if (sp & mask) != 0 else 0\n",
        "            new_pre_round = sp >> new_shift_amt\n",
        "            item['value'] = ieee_round_value(new_pre_round, sticky)\n",
        "\n",
        "    # 4 & 5. Align the final result: shift left by (12 - bits_to_keep) bits and truncate to 12 bits.\n",
        "    final_results = []\n",
        "    for item in results:\n",
        "        final_val = item['value'] << (12 - bits_to_keep)\n",
        "        final_val &= (1 << 12) - 1  # ensure exactly 12 bits\n",
        "        progress['final_fx'].append(final_val)\n",
        "        # 6. Apply two's complement if original sign is '1'\n",
        "        if item['sign'] == '1':\n",
        "            final_val = ((~final_val) + 1) & ((1 << 12) - 1)\n",
        "        final_results.append(format(final_val, '012b'))\n",
        "\n",
        "    for k,v in progress.items():\n",
        "      print(f'{k}: {v}')\n",
        "\n",
        "    return final_results, MAX_E\n",
        "# ----------------------------\n",
        "\n",
        "def FX_TO_FP(fx_output, max_e):\n",
        "    \"\"\"\n",
        "    Converts a list of 12-bit fixed-point (FX) numbers (given as binary strings)\n",
        "    into FP8 format with an 8-bit SEEEMMMM representation:\n",
        "      - S: Sign bit (1 if the FX number is negative, 0 if positive)\n",
        "      - EEE: 3-bit exponent computed as (max_e + 1 - number of leading zeros)\n",
        "      - MMMM: 4-bit mantissa extracted starting two bits to the right of the first '1'\n",
        "    If the FX magnitude is zero then returns \"00000000\" for that number.\n",
        "    \"\"\"\n",
        "    progress = { 'exp_val': [], 'lz': []}\n",
        "\n",
        "    fp8_results = []\n",
        "    for fx in fx_output:\n",
        "        # Convert binary string to integer.\n",
        "        fx_val = int(fx, 2)\n",
        "        # Determine if negative by checking the 12th bit (bit index 11).\n",
        "        if fx_val & (1 << 11):\n",
        "            # Negative: convert to magnitude using two's complement.\n",
        "            magnitude = ((~fx_val) + 1) & ((1 << 12) - 1)\n",
        "            sign_bit = '1'\n",
        "        else:\n",
        "            magnitude = fx_val\n",
        "            sign_bit = '0'\n",
        "\n",
        "        if magnitude == 0:\n",
        "            fp8_str = \"00000000\"\n",
        "            progress['exp_val'].append(0)\n",
        "            progress['lz'].append(12)\n",
        "        else:\n",
        "            # Convert magnitude to a 12-bit binary string.\n",
        "            bin_str = f\"{magnitude:012b}\"\n",
        "            try:\n",
        "                lz = bin_str.index(\"1\")\n",
        "            except ValueError:\n",
        "                lz = 12\n",
        "            progress['lz'].append(lz)\n",
        "            # Compute exponent as (max_e + 1 - number of leading zeros).\n",
        "            exponent_val = max_e + 1 - lz\n",
        "            progress['exp_val'].append(exponent_val)\n",
        "            exponent = f\"{exponent_val:03b}\"\n",
        "            # Extract 4 bits of mantissa starting two bits to the right of the first '1'.\n",
        "            mantissa = bin_str[lz+1: lz+5]\n",
        "            mantissa = mantissa.ljust(4, \"0\")\n",
        "            fp8_str = f\"{sign_bit}{exponent}{mantissa}\"\n",
        "        fp8_results.append(fp8_str)\n",
        "\n",
        "    for k,v in progress.items():\n",
        "      print(f'{k}: {v}')\n",
        "\n",
        "    return fp8_results\n",
        "\n",
        "def FP_TO_FX_TO_FP(fp8_numbers):\n",
        "    \"\"\"\n",
        "    End-to-end conversion:\n",
        "      - Takes a list of 64 FP8 numbers (each in SEEEMMMM format)\n",
        "      - Converts them to 12-bit FX using FP_TO_FX\n",
        "      - Then converts those FX numbers back to FP8 using FX_TO_FP\n",
        "    Returns a list of 8-bit FP8 outputs (SEEEMMMM) for the 64 numbers.\n",
        "    \"\"\"\n",
        "    fx_output, max_e = FP_TO_FX(fp8_numbers)\n",
        "    fp8_converted = FX_TO_FP(fx_output, max_e)\n",
        "    return fp8_converted\n",
        "'''\n",
        "x = [-0.75, 0.296875, 1.875, 3.625, -3.75]\n",
        "x = [bin_to_str(encode_e3m4_bin(i)) for i in x]\n",
        "y = FP_TO_FX_TO_FP(x)\n",
        "print(y)\n",
        "print([decode_bin_e3m4(str_to_bin(i)) for i in y])\n",
        "'''"
      ],
      "metadata": {
        "id": "FJYHkLNpyfJd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2a5ef134-6c9d-4eb0-d059-9b038443b7cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nx = [-0.75, 0.296875, 1.875, 3.625, -3.75]\\nx = [bin_to_str(encode_e3m4_bin(i)) for i in x]\\ny = FP_TO_FX_TO_FP(x)\\nprint(y)\\nprint([decode_bin_e3m4(str_to_bin(i)) for i in y])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FP8 to FX Quantization Error"
      ],
      "metadata": {
        "id": "kLgNXXSXCGtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "from math import log2, floor\n",
        "\n",
        "e_bits = 3\n",
        "m_bits = 4\n",
        "e_bias = 2 ** (e_bits-1) - 1\n",
        "\n",
        "def generate_bin_e3m4():\n",
        "  grid = []\n",
        "\n",
        "  # all possible e3m4 binary values\n",
        "  for S in product('01'):\n",
        "    for E in product('01', repeat=e_bits):\n",
        "      for M in product('01', repeat=m_bits):\n",
        "        if not (E == ('1',) * e_bits and M == ('1',) * m_bits): # ignore NaN\n",
        "          if E != ('0',) * e_bits or M == ('0',) * m_bits: # ignore subnormal numbers\n",
        "            bin = S + E + M\n",
        "            grid.append(bin)\n",
        "  return grid\n",
        "\n",
        "def bin_to_str(x: tuple[str, ...]) -> str:\n",
        "  return ''.join(x)\n",
        "\n",
        "def str_to_bin(x: str) -> tuple[str, ...]:\n",
        "  return tuple(x)\n",
        "\n",
        "def decode_bin_e3m4(x: tuple[str, ...]) -> float:\n",
        "  S = x[0]\n",
        "  E = x[1:e_bits+1]\n",
        "  M = x[e_bits+1:]\n",
        "\n",
        "  E_val = sum([2 ** (e_bits-i-1) * int(a) for i,a in enumerate(E)])\n",
        "\n",
        "  M_val = sum([2 ** -(i+1) * int(a) for i,a in enumerate(M)])\n",
        "  if E_val != 0:\n",
        "    M_val = M_val + 1\n",
        "\n",
        "  value = ((-1) ** int(S)) * 2 ** (E_val - e_bias) * M_val\n",
        "  return value\n",
        "\n",
        "def encode_e3m4_bin(x: float) -> tuple[str, ...]:\n",
        "  if x == +0.0:\n",
        "    return tuple('0' * 8)\n",
        "\n",
        "  S_str = '0' if x > 0 else '1'\n",
        "\n",
        "  exp = floor(log2(abs(x)))\n",
        "  E_str = f\"{(exp + e_bias):03b}\"\n",
        "\n",
        "  mantissa = abs(x) / 2**exp - 1\n",
        "  m_bin = []\n",
        "\n",
        "  while len(m_bin) < m_bits:\n",
        "    mantissa *= 2\n",
        "    bit = int(mantissa)\n",
        "    m_bin.append(str(bit))\n",
        "    mantissa -= bit\n",
        "  M_str = ''.join(m_bin)\n",
        "  return tuple(S_str + E_str + M_str)\n",
        "\n",
        "fp8_bin = generate_bin_e3m4()\n",
        "for x in fp8_bin:\n",
        "  x_enc = encode_e3m4_bin(decode_bin_e3m4(x))\n",
        "  if x != x_enc:\n",
        "    print(f\"{decode_bin_e3m4(x)} != {x_enc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UY09LGz9CM6L",
        "outputId": "7e5d6a19-e2da-4fc7-983b-bc4f28c579e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.0 != ('0', '0', '0', '0', '0', '0', '0', '0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "def symm_quant_fx(x: list[float], n_bits):\n",
        "  alpha = 2 ** math.ceil(math.log2(max(np.abs(x))))\n",
        "  scale = 2 ** (n_bits - 1) / alpha\n",
        "  return [round_fp8(round(scale * i) / scale) for i in x]\n",
        "\n",
        "def round_fp8(x: float) -> float:\n",
        "  '''\n",
        "  fp8_bin = generate_bin_e3m4()\n",
        "  fp8_values = [decode_bin_e3m4(x) for x in fp8_bin]\n",
        "\n",
        "  i = np.argmin(np.abs(np.subtract(fp8_values, x)))\n",
        "  return fp8_values[i]\n",
        "  '''\n",
        "  if abs(x) < 0.25:\n",
        "    return round(x * 4) / 4\n",
        "  else:\n",
        "    scale = 2 ** (floor(log2(abs(x))) - 4)\n",
        "    return round(x / scale) * scale\n",
        "\n",
        "def q_mse(x: list[float], q: list[float]) -> float:\n",
        "  return sum([(a - b)**2 for a,b in zip(x, q)]) / len(x)"
      ],
      "metadata": {
        "id": "cDkFbc245H3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "n = len(fp8_bin)\n",
        "x_fp8_bin = random.sample(fp8_bin,n)\n",
        "\n",
        "batch = 16\n",
        "q_x_fp8_bin = []\n",
        "i = 0\n",
        "while n > batch:\n",
        "  chunk = x_fp8_bin[batch*i : batch*(i+1)]\n",
        "  q_chunk = [str_to_bin(y) for y in FP_TO_FX_TO_FP([bin_to_str(x) for x in chunk])]\n",
        "  q_x_fp8_bin.extend(q_chunk)\n",
        "  i += 1\n",
        "  n -= batch\n",
        "if n != 0:\n",
        "  chunk = x_fp8_bin[batch*i:]\n",
        "  q_chunk = [str_to_bin(y) for y in FP_TO_FX_TO_FP([bin_to_str(x) for x in chunk])]\n",
        "  q_x_fp8_bin.extend(q_chunk)\n",
        "\n",
        "x_fp8 = [decode_bin_e3m4(a) for a in x_fp8_bin]\n",
        "q_fx = [decode_bin_e3m4(a) for a in q_x_fp8_bin]\n",
        "mse_fx = q_mse(x_fp8, q_fx)\n",
        "\n",
        "q_int12 = symm_quant_fx(x_fp8, 12)\n",
        "mse_int12 = q_mse(x_fp8, q_int12)\n",
        "\n",
        "q_int8 = symm_quant_fx(x_fp8, 8)\n",
        "mse_int8 = q_mse(x_fp8, q_int8)\n",
        "\n",
        "q_int4 = symm_quant_fx(x_fp8, 4)\n",
        "mse_int4 = q_mse(x_fp8, q_int4)\n",
        "\n",
        "df_FX = pd.DataFrame({\n",
        "    \"FP8\": x_fp8,\n",
        "    \"FP->FX->FP\": q_fx,\n",
        "    \"int12\": q_int12,\n",
        "    \"int8\": q_int8,\n",
        "    \"int4\": q_int4\n",
        "})\n",
        "\n",
        "print(df_FX)\n",
        "print(\"quantized MSE\")\n",
        "print(df_FX.apply(lambda x: q_mse(x_fp8, x), axis=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dh1AQrqrfeu-",
        "outputId": "5ed6dc9c-e699-474a-c190-606fa7158467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "extend: [3712, 3840, 3456, 2048, 3840, 2432, 3072, 2304, 2560, 2944, 2048, 2560, 3584, 2560, 2688, 3584]\n",
            "shift1: [3712, 480, 864, 128, 240, 608, 192, 144, 2560, 184, 256, 1280, 448, 40, 168, 3584]\n",
            "sticky: [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0]\n",
            "preround: [14, 1, 3, 0, 0, 2, 0, 0, 10, 0, 1, 5, 1, 0, 0, 14]\n",
            "rounded: [7, 1, 2, 0, 0, 1, 0, 0, 5, 0, 0, 2, 1, 0, 0, 7]\n",
            "bit_at_index: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "value: [7, 1, 2, 0, 0, 1, 0, 0, 5, 0, 0, 2, 1, 0, 0, 7]\n",
            "final_fx: [1792, 256, 512, 0, 0, 256, 0, 0, 1280, 0, 0, 512, 256, 0, 0, 1792]\n",
            "exp_val: [7, 5, 6, 0, 0, 5, 0, 0, 7, 0, 0, 6, 5, 0, 0, 7]\n",
            "lz: [1, 3, 2, 12, 12, 3, 12, 12, 1, 12, 12, 2, 3, 12, 12, 1]\n",
            "extend: [2432, 3840, 2176, 2304, 3456, 2304, 2432, 2688, 2432, 3328, 2816, 2304, 3072, 2816, 3584, 2560]\n",
            "shift1: [304, 3840, 544, 576, 432, 144, 608, 1344, 152, 1664, 1408, 1152, 384, 352, 1792, 2560]\n",
            "sticky: [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
            "preround: [1, 15, 2, 2, 1, 0, 2, 5, 0, 6, 5, 4, 1, 1, 7, 10]\n",
            "rounded: [1, 8, 1, 1, 1, 0, 1, 3, 0, 3, 3, 2, 1, 1, 4, 5]\n",
            "bit_at_index: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "value: [1, 8, 1, 1, 1, 0, 1, 3, 0, 3, 3, 2, 1, 1, 4, 5]\n",
            "final_fx: [0, 1024, 256, 256, 0, 0, 256, 256, 0, 512, 256, 256, 0, 0, 512, 512]\n",
            "exp_val: [0, 6, 4, 4, 0, 0, 4, 4, 0, 5, 4, 4, 0, 0, 5, 5]\n",
            "lz: [12, 1, 3, 3, 12, 12, 3, 3, 12, 2, 3, 3, 12, 12, 2, 2]\n",
            "extend: [2688, 2176, 3968, 3584, 3968, 2176, 2688, 3328, 3840, 2944, 2944, 3328, 2176, 3584, 3456, 3200]\n",
            "shift1: [2688, 544, 62, 224, 496, 68, 42, 832, 1920, 2944, 1472, 3328, 34, 896, 3456, 800]\n",
            "sticky: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
            "preround: [10, 2, 0, 0, 1, 0, 0, 3, 7, 11, 5, 13, 0, 3, 13, 3]\n",
            "rounded: [5, 1, 0, 0, 1, 0, 0, 2, 4, 6, 3, 6, 0, 2, 7, 2]\n",
            "bit_at_index: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "value: [5, 1, 0, 0, 1, 0, 0, 2, 4, 6, 3, 6, 0, 2, 7, 2]\n",
            "final_fx: [1280, 256, 0, 0, 256, 0, 0, 512, 1024, 1536, 768, 1536, 0, 512, 1792, 512]\n",
            "exp_val: [7, 5, 0, 0, 5, 0, 0, 6, 7, 7, 6, 7, 0, 6, 7, 6]\n",
            "lz: [1, 3, 12, 12, 3, 12, 12, 2, 1, 1, 2, 1, 12, 2, 1, 2]\n",
            "extend: [2304, 2048, 3456, 3328, 3584, 2688, 2176, 3968, 3072, 3840, 2432, 2816, 2304, 2432, 3968, 3712]\n",
            "shift1: [2304, 1024, 432, 104, 112, 2688, 136, 3968, 3072, 120, 304, 704, 2304, 152, 496, 928]\n",
            "sticky: [0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1]\n",
            "preround: [9, 4, 1, 0, 0, 10, 0, 15, 12, 0, 1, 2, 9, 0, 1, 3]\n",
            "rounded: [4, 2, 1, 0, 0, 5, 0, 8, 6, 0, 1, 1, 4, 0, 1, 2]\n",
            "bit_at_index: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "value: [4, 2, 1, 0, 0, 5, 0, 8, 6, 0, 1, 1, 4, 0, 1, 2]\n",
            "final_fx: [512, 256, 0, 0, 0, 768, 0, 1024, 768, 0, 0, 256, 512, 0, 0, 256]\n",
            "exp_val: [6, 5, 0, 0, 0, 6, 0, 7, 6, 0, 0, 5, 6, 0, 0, 5]\n",
            "lz: [2, 3, 12, 12, 12, 2, 12, 1, 2, 12, 12, 3, 2, 12, 12, 3]\n",
            "extend: [2688, 2816, 3328, 3584, 2560, 3456, 3584, 3328, 2944, 3456, 3968, 3200, 3712, 3840, 2304, 3712]\n",
            "shift1: [42, 1408, 3328, 112, 320, 108, 224, 1664, 368, 432, 1984, 1600, 3712, 1920, 288, 58]\n",
            "sticky: [1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "preround: [0, 5, 13, 0, 1, 0, 0, 6, 1, 1, 7, 6, 14, 7, 1, 0]\n",
            "rounded: [0, 3, 6, 0, 1, 0, 0, 3, 1, 1, 4, 3, 7, 4, 1, 0]\n",
            "bit_at_index: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "value: [0, 3, 6, 0, 1, 0, 0, 3, 1, 1, 4, 3, 7, 4, 1, 0]\n",
            "final_fx: [0, 768, 1536, 0, 256, 0, 0, 768, 256, 256, 1024, 768, 1792, 1024, 256, 0]\n",
            "exp_val: [0, 6, 7, 0, 5, 0, 0, 6, 5, 5, 7, 6, 7, 7, 5, 0]\n",
            "lz: [12, 2, 1, 12, 3, 12, 12, 2, 3, 3, 1, 2, 1, 1, 3, 12]\n",
            "extend: [3968, 2944, 2048, 2816, 2432, 3840, 2560, 2048, 2176, 3712, 2432, 3584, 3968, 3072, 2944, 3840]\n",
            "shift1: [248, 46, 64, 176, 1216, 3840, 2560, 16, 272, 928, 304, 3584, 124, 48, 736, 960]\n",
            "sticky: [1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
            "preround: [0, 0, 0, 0, 4, 15, 10, 0, 1, 3, 1, 14, 0, 0, 2, 3]\n",
            "rounded: [0, 0, 0, 0, 2, 8, 5, 0, 1, 2, 1, 7, 0, 0, 1, 2]\n",
            "bit_at_index: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "value: [0, 0, 0, 0, 2, 7, 5, 0, 1, 2, 1, 7, 0, 0, 1, 2]\n",
            "final_fx: [0, 0, 0, 0, 512, 1792, 1280, 0, 256, 512, 256, 1792, 0, 0, 256, 512]\n",
            "exp_val: [0, 0, 0, 0, 6, 7, 7, 0, 5, 6, 5, 7, 0, 0, 5, 6]\n",
            "lz: [12, 12, 12, 12, 2, 1, 1, 12, 3, 2, 3, 1, 12, 12, 3, 2]\n",
            "extend: [3328, 2816, 2048, 2048, 2688, 3712, 3968, 2304, 3200, 2048, 2304, 2176, 3456, 2944, 2048, 3584]\n",
            "shift1: [208, 2816, 256, 2048, 168, 928, 124, 2304, 200, 16, 72, 34, 1728, 2944, 32, 896]\n",
            "sticky: [1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "preround: [0, 11, 1, 8, 0, 3, 0, 9, 0, 0, 0, 0, 6, 11, 0, 3]\n",
            "rounded: [0, 6, 0, 4, 0, 2, 0, 4, 0, 0, 0, 0, 3, 6, 0, 2]\n",
            "bit_at_index: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "value: [0, 6, 0, 4, 0, 2, 0, 4, 0, 0, 0, 0, 3, 6, 0, 2]\n",
            "final_fx: [0, 1536, 0, 1024, 0, 512, 0, 1024, 0, 0, 0, 0, 768, 1536, 0, 512]\n",
            "exp_val: [0, 7, 0, 7, 0, 6, 0, 7, 0, 0, 0, 0, 6, 7, 0, 6]\n",
            "lz: [12, 1, 12, 1, 12, 2, 12, 1, 12, 12, 12, 12, 2, 1, 12, 2]\n",
            "extend: [2048, 2432, 3328, 2048, 2048, 2688, 2304, 2176, 3072, 2176, 2176, 2304, 2048, 3328, 2304, 3072]\n",
            "shift1: [2048, 2432, 1664, 1024, 32, 1344, 576, 272, 3072, 1088, 544, 72, 64, 832, 2304, 384]\n",
            "sticky: [0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1]\n",
            "preround: [8, 9, 6, 4, 0, 5, 2, 1, 12, 4, 2, 0, 0, 3, 9, 1]\n",
            "rounded: [4, 5, 3, 2, 0, 3, 1, 1, 6, 2, 1, 0, 0, 2, 4, 1]\n",
            "bit_at_index: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "value: [4, 5, 3, 2, 0, 3, 1, 1, 6, 2, 1, 0, 0, 2, 4, 1]\n",
            "final_fx: [1024, 1280, 768, 512, 0, 768, 256, 256, 1536, 512, 256, 0, 0, 512, 1024, 256]\n",
            "exp_val: [7, 7, 6, 6, 0, 6, 5, 5, 7, 6, 5, 0, 0, 6, 7, 5]\n",
            "lz: [1, 1, 2, 2, 12, 2, 3, 3, 1, 2, 3, 12, 12, 2, 1, 3]\n",
            "extend: [3072, 2944, 2176, 3456, 3712, 2304, 3840, 3584, 3200, 2048, 2560, 2432, 2560, 2944, 3584, 3968]\n",
            "shift1: [768, 736, 2176, 864, 232, 576, 120, 1792, 800, 1024, 80, 38, 1280, 184, 56, 62]\n",
            "sticky: [0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1]\n",
            "preround: [3, 2, 8, 3, 0, 2, 0, 7, 3, 4, 0, 0, 5, 0, 0, 0]\n",
            "rounded: [2, 1, 4, 2, 0, 1, 0, 4, 2, 2, 0, 0, 2, 0, 0, 0]\n",
            "bit_at_index: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "value: [2, 1, 4, 2, 0, 1, 0, 4, 2, 2, 0, 0, 2, 0, 0, 0]\n",
            "final_fx: [512, 256, 1024, 512, 0, 256, 0, 1024, 512, 512, 0, 0, 512, 0, 0, 0]\n",
            "exp_val: [6, 5, 7, 6, 0, 5, 0, 7, 6, 6, 0, 0, 6, 0, 0, 0]\n",
            "lz: [2, 3, 1, 2, 12, 3, 12, 1, 2, 2, 12, 12, 2, 12, 12, 12]\n",
            "extend: [2816, 2816, 3456, 3840, 2048, 2304, 2560, 3200, 2688, 3072, 2560, 3968, 3840, 3712, 3712, 2176]\n",
            "shift1: [88, 44, 432, 60, 128, 36, 40, 400, 336, 48, 320, 992, 120, 116, 116, 2176]\n",
            "sticky: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "preround: [0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 3, 0, 0, 0, 8]\n",
            "rounded: [0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 2, 0, 0, 0, 4]\n",
            "bit_at_index: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "value: [0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 2, 0, 0, 0, 4]\n",
            "final_fx: [0, 0, 256, 0, 0, 0, 0, 256, 256, 0, 256, 512, 0, 0, 0, 1024]\n",
            "exp_val: [0, 0, 5, 0, 0, 0, 0, 5, 5, 0, 5, 6, 0, 0, 0, 7]\n",
            "lz: [12, 12, 3, 12, 12, 12, 12, 3, 3, 12, 3, 2, 12, 12, 12, 1]\n",
            "extend: [3072, 2560, 3072, 3712, 3840, 2944, 3200, 3840, 3328, 2432, 2688, 3072, 2432, 3328, 2688, 3968]\n",
            "shift1: [384, 640, 192, 1856, 3840, 368, 50, 480, 52, 304, 2688, 1536, 2432, 208, 672, 496]\n",
            "sticky: [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
            "preround: [1, 2, 0, 7, 15, 1, 0, 1, 0, 1, 10, 6, 9, 0, 2, 1]\n",
            "rounded: [1, 1, 0, 4, 8, 1, 0, 1, 0, 1, 5, 3, 5, 0, 1, 1]\n",
            "bit_at_index: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "value: [1, 1, 0, 4, 7, 1, 0, 1, 0, 1, 5, 3, 5, 0, 1, 1]\n",
            "final_fx: [256, 256, 0, 1024, 1792, 256, 0, 256, 0, 256, 1280, 768, 1280, 0, 256, 256]\n",
            "exp_val: [5, 5, 0, 7, 7, 5, 0, 5, 0, 5, 7, 6, 7, 0, 5, 5]\n",
            "lz: [3, 3, 12, 1, 1, 3, 12, 3, 12, 3, 1, 2, 1, 12, 3, 3]\n",
            "extend: [3072, 3456, 3968, 2688, 2944, 3584, 2816, 3712, 3200, 3328, 2816, 2432, 3328, 3456, 3584, 2176]\n",
            "shift1: [1536, 432, 1984, 168, 92, 3584, 2816, 116, 100, 208, 352, 2432, 832, 108, 224, 2176]\n",
            "sticky: [0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "preround: [6, 1, 7, 0, 0, 14, 11, 0, 0, 0, 1, 9, 3, 0, 0, 8]\n",
            "rounded: [3, 1, 4, 0, 0, 7, 6, 0, 0, 0, 1, 5, 2, 0, 0, 4]\n",
            "bit_at_index: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "value: [3, 1, 4, 0, 0, 7, 6, 0, 0, 0, 1, 5, 2, 0, 0, 4]\n",
            "final_fx: [768, 256, 1024, 0, 0, 1792, 1536, 0, 0, 0, 256, 1280, 512, 0, 0, 1024]\n",
            "exp_val: [5, 4, 6, 0, 0, 6, 6, 0, 0, 0, 4, 6, 5, 0, 0, 6]\n",
            "lz: [2, 3, 1, 12, 12, 1, 1, 12, 12, 12, 3, 1, 2, 12, 12, 1]\n",
            "extend: [2944, 3456, 2944, 3200, 3456, 2816, 3072, 3200, 3200, 2048, 2944, 3712, 3456, 3840, 3200, 2688]\n",
            "shift1: [1472, 3456, 92, 1600, 54, 44, 3072, 400, 100, 512, 92, 232, 1728, 240, 200, 84]\n",
            "sticky: [1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
            "preround: [5, 13, 0, 6, 0, 0, 12, 1, 0, 2, 0, 0, 6, 0, 0, 0]\n",
            "rounded: [3, 7, 0, 3, 0, 0, 6, 1, 0, 1, 0, 0, 3, 0, 0, 0]\n",
            "bit_at_index: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "value: [3, 7, 0, 3, 0, 0, 6, 1, 0, 1, 0, 0, 3, 0, 0, 0]\n",
            "final_fx: [768, 1792, 0, 768, 0, 0, 1536, 256, 0, 256, 0, 0, 768, 0, 0, 0]\n",
            "exp_val: [6, 7, 0, 6, 0, 0, 7, 5, 0, 5, 0, 0, 6, 0, 0, 0]\n",
            "lz: [2, 1, 12, 2, 12, 12, 1, 3, 12, 3, 12, 12, 2, 12, 12, 12]\n",
            "extend: [3200, 3712, 3072, 2560, 2560, 3200, 2432, 2560, 2176, 3328, 2816, 3200, 2816, 2688, 2816, 3712]\n",
            "shift1: [100, 464, 96, 160, 80, 3200, 608, 160, 136, 104, 2816, 3200, 704, 672, 704, 1856]\n",
            "sticky: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1]\n",
            "preround: [0, 1, 0, 0, 0, 12, 2, 0, 0, 0, 11, 12, 2, 2, 2, 7]\n",
            "rounded: [0, 1, 0, 0, 0, 6, 1, 0, 0, 0, 6, 6, 1, 1, 1, 4]\n",
            "bit_at_index: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "value: [0, 1, 0, 0, 0, 6, 1, 0, 0, 0, 6, 6, 1, 1, 1, 4]\n",
            "final_fx: [0, 256, 0, 0, 0, 1536, 256, 0, 0, 0, 1536, 1536, 256, 256, 256, 1024]\n",
            "exp_val: [0, 5, 0, 0, 0, 7, 5, 0, 0, 0, 7, 7, 5, 5, 5, 7]\n",
            "lz: [12, 3, 12, 12, 12, 1, 3, 12, 12, 12, 1, 1, 3, 3, 3, 1]\n",
            "        FP8  FP->FX->FP   int12   int8  int4\n",
            "0   -29.000       -28.0 -29.000 -29.00 -28.0\n",
            "1     3.750         4.0   3.750   3.75   4.0\n",
            "2    -6.750        -8.0  -6.750  -6.75  -8.0\n",
            "3     1.000         0.0   1.000   1.00   0.0\n",
            "4    -1.875         0.0  -1.875  -2.00   0.0\n",
            "..      ...         ...     ...    ...   ...\n",
            "219  25.000        24.0  25.000  25.00  24.0\n",
            "220   5.500         4.0   5.500   5.50   4.0\n",
            "221  -5.250        -4.0  -5.250  -5.25  -4.0\n",
            "222  -5.500        -4.0  -5.500  -5.50  -4.0\n",
            "223  14.500        16.0  14.500  14.50  16.0\n",
            "\n",
            "[224 rows x 5 columns]\n",
            "quantized MSE\n",
            "FP8           0.000000\n",
            "FP->FX->FP    1.127633\n",
            "int12         0.000000\n",
            "int8          0.003470\n",
            "int4          1.222499\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = random.sample(fp8_bin, 1)\n",
        "print(f'{bin_to_str(x[0])} = {decode_bin_e3m4(x[0])}')\n",
        "x = [bin_to_str(x[0])]\n",
        "q = FP_TO_FX_TO_FP(x)\n",
        "str_to_bin(q[0])\n",
        "print(q[0], ' = ', decode_bin_e3m4(str_to_bin(q[0])))"
      ],
      "metadata": {
        "id": "WzEmyO3hRtkn",
        "outputId": "6044771d-5084-446e-8388-3781e2419752",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01010110 = 5.5\n",
            "extend: [2816]\n",
            "shift1: [2816]\n",
            "sticky: [0]\n",
            "preround: [11]\n",
            "rounded: [6]\n",
            "bit_at_index: [0]\n",
            "value: [6]\n",
            "final_fx: [1536]\n",
            "exp_val: [5]\n",
            "lz: [1]\n",
            "01011000  =  6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization"
      ],
      "metadata": {
        "id": "lwAAovaNybTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log2\n",
        "\n",
        "def e3m4_quantization(x_float, fx_bits = -1, max_E = False):\n",
        "  E = 3\n",
        "  M = 4\n",
        "  B = 2 ** (E-1) - 1\n",
        "  c = (2 - 2 ** (1 - M)) * 2 ** (2**E - B - 1)\n",
        "\n",
        "  maxval = torch.max(torch.abs(x_float))\n",
        "  # print(maxval)\n",
        "  '''\n",
        "  if torch.isnan(maxval):\n",
        "    maxval = torch.tensor(c)\n",
        "  elif maxval == 0:\n",
        "    return x_float\n",
        "  '''\n",
        "  minval = -maxval\n",
        "\n",
        "  gamma = maxval / c\n",
        "  bias = 2**E - torch.log2(maxval) + torch.log2(2 - 2.0 ** (torch.tensor(1-M))) - 1\n",
        "  xc = torch.min(torch.max(x_float, minval), maxval)\n",
        "\n",
        "  log_scales = torch.clamp((torch.floor(torch.log2(torch.abs(xc)) + bias)).detach(), 1.0)\n",
        "  scales = 2.0 ** (log_scales - M - bias)\n",
        "\n",
        "  quantize_e3m4 = torch.round(xc / scales)\n",
        "  elem_scale = torch.where(quantize_e3m4 == 0, 0, 2.0 ** (torch.floor(torch.log2(torch.abs(xc) / gamma)) - M))\n",
        "  # if torch.isnan(elem_scale).any():\n",
        "  #  print(\"nan elem scale\")\n",
        "  quantize_e3m4 = round_e3m4(quantize_e3m4 * elem_scale) # FP32 -> FP8\n",
        "\n",
        "  if fx_bits != -1:\n",
        "    '''\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "    start.record()\n",
        "    '''\n",
        "    if max_E:\n",
        "      quantize_e3m4 = round_e3m4(fp_maxE_quantization(quantize_e3m4, bits_to_keep=fx_bits))\n",
        "    else:\n",
        "      quantize_e3m4 = round_e3m4(fx_quantization(quantize_e3m4, fx_bits))\n",
        "    '''\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "    print(f'elasped time: {start.elapsed_time(end)}')\n",
        "    '''\n",
        "    # quant_func = lambda x: fp_maxE_quantization(x, bits_to_keep=fx_bits) if max_E else lambda x: fx_quantization(x, fx_bits)\n",
        "    # quantize_e3m4 = round_e3m4(quant_func(quantize_e3m4)) # FP8 -> FX -> FP8\n",
        "  quantize_e3m4 = quantize_e3m4.to(xc.device)\n",
        "  quantize_e3m4 = torch.where(elem_scale == 0, 0, torch.div(quantize_e3m4, elem_scale)) # FP8 -> FP32\n",
        "  result = quantize_e3m4 * scales\n",
        "  # result = torch.round(xc / scales) * scales\n",
        "  return result"
      ],
      "metadata": {
        "id": "QrTmvhmswqnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.masked import masked_tensor, MaskedTensor\n",
        "@torch.compile\n",
        "def fp_maxE_quantization(x: torch.Tensor, bits_to_keep = 4, vector_size = 32) -> torch.Tensor:\n",
        "  x_shape = x.shape\n",
        "  sections = torch.split(x.flatten(), vector_size)\n",
        "  q_sections = []\n",
        "\n",
        "  for x in sections:\n",
        "    # FP8 -> FX\n",
        "    value, preshift, shift_flag, maxE = proc_fp8(x, bits_to_keep)\n",
        "\n",
        "    # Post-processing: only apply if the flag is set.\n",
        "    if shift_flag:\n",
        "      maxE += 1\n",
        "      remainder = torch.fmod(preshift, 2.0 ** (13 - bits_to_keep))\n",
        "      sticky = torch.where(remainder == 0, 0, 1)\n",
        "      preround = torch.floor(preshift / 2.0 ** (13 - bits_to_keep))\n",
        "      value = ieee_round(preround, sticky)\n",
        "\n",
        "    # Align the final result\n",
        "    final = value * 2 ** (12 - bits_to_keep)\n",
        "\n",
        "    # FX -> FP8\n",
        "    sign = torch.where(x < 0, -1, 1)\n",
        "\n",
        "    # mask = final != 0\n",
        "    # mfinal = masked_tensor(final, mask)\n",
        "    l = torch.where(final == 0, -1, torch.floor(torch.log2(final)))\n",
        "    lz = torch.abs(l + -11)\n",
        "\n",
        "    exp_val = torch.where(final == 0, 0, maxE + 1 + -1 * lz)\n",
        "\n",
        "    # Extract 4 bits of mantissa starting two bits to the right of the first '1'\n",
        "    mantissa = torch.where(final == 0, 0, torch.floor(final / (2.0 ** (l-4))) / (2 ** 4))\n",
        "    fp8_value = sign * (2.0 ** (exp_val - 3)) * mantissa\n",
        "    q_sections.append(fp8_value)\n",
        "\n",
        "  '''\n",
        "  for x in sections:\n",
        "    x = x.cpu().detach().numpy()\n",
        "    x_bin = [encode_e3m4_bin(i) for i in x]\n",
        "    q_x_bin = FP_TO_FX_TO_FP([bin_to_str(i) for i in x_bin])\n",
        "\n",
        "    q_x = []\n",
        "    for i,a in enumerate(q_x_bin):\n",
        "      if not a.isdigit():\n",
        "        print(f'{x[i]} is converted to {a} which is not binary')\n",
        "      q_x.append(decode_bin_e3m4(str_to_bin(a)))\n",
        "    #q_x = [decode_bin_e3m4(str_to_bin(i)) for i in q_x_bin]\n",
        "    q_sections.append(torch.Tensor(q_x))\n",
        "  '''\n",
        "  x = torch.cat(tuple(q_sections), 0)\n",
        "  return torch.unflatten(x, 0, x_shape)\n",
        "\n",
        "def find_maxE(x: torch.Tensor):\n",
        "  maxval = torch.max(torch.abs(x))\n",
        "  return 0 if maxval == 0 else torch.floor(torch.log2(maxval)) + 3\n",
        "\n",
        "def ieee_round(pre: torch.Tensor, sticky: torch.Tensor):\n",
        "  frac = pre % 2\n",
        "  i = torch.floor(pre / 2)\n",
        "  i = i + torch.clamp(i % 2 + sticky, max=1) * frac\n",
        "  return i\n",
        "\n",
        "def proc_fp8(x: torch.Tensor, bits_to_keep: int) -> torch.Tensor:\n",
        "  shift_flag = False\n",
        "  maxE = find_maxE(x)\n",
        "  # mask = x != 0\n",
        "  # mx = masked_tensor(x, mask)\n",
        "  nom_exp = torch.where(x == 0, 0,torch.floor(torch.log2(torch.abs(x))))\n",
        "  biased_exp = torch.where(x == 0, 0, nom_exp + 3)\n",
        "  exp_val = 2 ** nom_exp\n",
        "  mantissa = (torch.div(torch.abs(x), exp_val)-1) * 16\n",
        "\n",
        "  extended_mantissa = mantissa * (2 ** 7) + 2 ** 11\n",
        "  shifted_mantissa = extended_mantissa / (2.0 ** (maxE - biased_exp))\n",
        "\n",
        "  remainder = torch.fmod(shifted_mantissa, 2.0 ** (12 - bits_to_keep))\n",
        "  sticky = torch.where(remainder == 0, 0, 1)\n",
        "  preround_mantissa = torch.floor(shifted_mantissa / 2.0 ** (12 - bits_to_keep))\n",
        "  rounded_mantissa = ieee_round(preround_mantissa, sticky)\n",
        "  bit_at_index = torch.floor(rounded_mantissa / 2.0 ** (bits_to_keep-1)).int() % 2\n",
        "\n",
        "  if maxE == torch.tensor(7):\n",
        "    rounded_mantissa = torch.where(bit_at_index == 1, torch.floor(preround_mantissa/2), rounded_mantissa)\n",
        "  else:\n",
        "    shift_flag = torch.any(bit_at_index == 1)\n",
        "\n",
        "  return rounded_mantissa, shifted_mantissa, shift_flag, maxE\n",
        "'''\n",
        "x = random.sample(fp8_bin, 64)\n",
        "x = [decode_bin_e3m4(i) for i in x]\n",
        "print(x)\n",
        "input = torch.tensor(x)\n",
        "print(fp_maxE_quantization(input))\n",
        "\n",
        "print('\\nOriginal')\n",
        "x = [bin_to_str(encode_e3m4_bin(i)) for i in x]\n",
        "y = FP_TO_FX_TO_FP(x)\n",
        "print([decode_bin_e3m4(str_to_bin(i)) for i in y])\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "neyoAaS28Wvc",
        "outputId": "0fa575b1-349a-4467-cd4f-dc4b40b90c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nx = random.sample(fp8_bin, 64)\\nx = [decode_bin_e3m4(i) for i in x]\\nprint(x)\\ninput = torch.tensor(x)\\nprint(fp_maxE_quantization(input))\\n\\nprint('\\nOriginal')\\nx = [bin_to_str(encode_e3m4_bin(i)) for i in x]\\ny = FP_TO_FX_TO_FP(x)\\nprint([decode_bin_e3m4(str_to_bin(i)) for i in y])\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log2\n",
        "\n",
        "def fx_quantization(x: torch.Tensor, fx_bits: int) -> torch.Tensor:\n",
        "  alpha = 2 ** torch.ceil(torch.log2(torch.max(torch.abs(x))))\n",
        "  scale = 2 ** (fx_bits - 1) / alpha\n",
        "  return torch.round(scale * x) / scale\n",
        "\n",
        "def round_e3m4(x: torch.Tensor) -> torch.Tensor:\n",
        "  M = 4\n",
        "  elem_scale = torch.where(x == 0, 0, 2 ** (torch.floor(torch.log2(torch.abs(x))) - M))\n",
        "  quantize_x = torch.where(elem_scale == 0, 0, torch.div(x, elem_scale))\n",
        "  quantize_x = torch.round(quantize_x)\n",
        "  dequantize_x = quantize_x * elem_scale\n",
        "  remove_sub_x = torch.where(torch.abs(dequantize_x) < 0.25, torch.round(dequantize_x * 4)/4, dequantize_x) # any subnormal values are rounded to either 0 or 0.25\n",
        "  return remove_sub_x\n",
        "\n",
        "def e3m4_fx_quantization(x: torch.Tensor, fx_bits: int) -> torch.Tensor:\n",
        "  E = 3\n",
        "  M = 4\n",
        "  B = 2 ** (E-1) - 1\n",
        "  c = (2 - 2 ** (1 - M)) * 2 ** (2**E - B - 1)\n",
        "\n",
        "  maxval = torch.max(torch.abs(x))\n",
        "  if torch.isnan(maxval):\n",
        "    maxval = torch.tensor(c)\n",
        "  elif maxval == 0:\n",
        "    return x\n",
        "\n",
        "  gamma = maxval / c\n",
        "  bias = 2**E - log2(maxval) + log2(2 - 2 ** (1 - M)) - 1\n",
        "  xc = torch.min(torch.max(x, -maxval), maxval)\n",
        "\n",
        "  log_scales = torch.clamp((torch.floor(torch.log2(torch.abs(xc)) + bias)).detach(), 1.0)\n",
        "  scales = 2.0 ** (log_scales - M - bias)\n",
        "\n",
        "  quantize_e3m4 = torch.round(xc / scales) # FP32 -> FP8\n",
        "  elem_scale = 2 ** (torch.floor(torch.log2(torch.abs(xc) / gamma)) - M)\n",
        "  quantize_e3m4 = quantize_e3m4 * elem_scale\n",
        "  # print(quantize_e3m4)\n",
        "  quantize_fx = fx_quantization(quantize_e3m4, fx_bits) # FP8 -> FX -> FP8\n",
        "  # print(quantize_fx)\n",
        "  result = quantize_fx / elem_scale * scales # FP8 -> FP32\n",
        "  return result\n",
        "\n",
        "def e3m4_maxE_quantization(x: torch.Tensor) -> torch.Tensor:\n",
        "  E = 3\n",
        "  M = 4\n",
        "  B = 2 ** (E-1) - 1\n",
        "\n",
        "  maxval = torch.max(torch.abs(x))\n",
        "  minval = -maxval\n",
        "\n",
        "  c = (2 - 2 ** (1 - M)) * 2 ** (2**E - B - 1)\n",
        "  gamma = maxval / c\n",
        "\n",
        "  bias = 2**E - log2(maxval) + log2(2 - 2 ** (1 - M)) - 1\n",
        "  xc = torch.min(torch.max(x, minval), maxval)\n",
        "\n",
        "  log_scales = torch.clamp((torch.floor(torch.log2(torch.abs(xc)) + bias)).detach(), 1.0)\n",
        "  scales = 2.0 ** (log_scales - M - bias)\n",
        "\n",
        "  quantize_e3m4 = torch.round(xc / scales) # FP32 -> FP8\n",
        "  elem_scale = 2 ** (torch.floor(torch.log2(torch.abs(xc) / gamma)) - M)\n",
        "  quantize_e3m4 = round_e3m4(quantize_e3m4 * elem_scale)\n",
        "  quantize_fx = round_e3m4(fp_maxE_quantization(quantize_e3m4))\n",
        "  quantize_fx = quantize_fx.to(xc.device)\n",
        "  result = torch.div(quantize_fx, elem_scale)\n",
        "  result *= scales\n",
        "  return result\n",
        "\n",
        "input = torch.rand(3,3) - 0.5\n",
        "print(input)\n",
        "x = e3m4_fx_quantization(input, 8)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "FDfcYyCpGv0i",
        "outputId": "b94177b1-11ea-454f-9464-e2da32a20cbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1415, -0.1303,  0.2877],\n",
            "        [ 0.2263,  0.0781,  0.1279],\n",
            "        [ 0.1561, -0.3531,  0.2231]])\n",
            "tensor([[-0.1412, -0.1295,  0.2825],\n",
            "        [ 0.2236,  0.0794,  0.1295],\n",
            "        [ 0.1589, -0.3531,  0.2236]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet18 Module"
      ],
      "metadata": {
        "id": "sPxpfhb2x3v-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEc93C7xAXEE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "fbf2e310-de03-4f95-fdcd-5a08a6255166"
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "  expansion = 1\n",
        "  def __init__(self, in_planes, planes, stride=1):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "    self.shortcut = nn.Sequential()\n",
        "    if stride != 1 or in_planes != self.expansion*planes:\n",
        "      self.shortcut = nn.Sequential(\n",
        "          nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "          nn.BatchNorm2d(self.expansion*planes)\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.bn2(self.conv2(out))\n",
        "    out += self.shortcut(x)\n",
        "    out = F.relu(out)\n",
        "    return out\n",
        "\n",
        "class BottleNeck(nn.Module):\n",
        "  expansion = 4\n",
        "\n",
        "  def __init__(self, in_planes, planes, stride=1):\n",
        "    super(BottleNeck, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_planes , planes, kernel_size=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "    self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "    self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "    self.shortcut = nn.Sequential()\n",
        "    if stride != 1 or in_planes != self.expansion*planes :\n",
        "      self.shortcut = nn.Sequential(\n",
        "          nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "          nn.BatchNorm2d(self.expansion*planes)\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = F.relu(self.bn2(self.conv2(out)))\n",
        "    out = self.bn3(self.conv3(out))\n",
        "    out += self.shortcut(x)\n",
        "    out = F.relu(out)\n",
        "    return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "  def __init__(self, block, num_blocks, num_classes=10):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.in_planes = 64\n",
        "\n",
        "    self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "    self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "    self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "    self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "    self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "    self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "  def _make_layer(self, block, planes, num_blocks, stride):\n",
        "    strides = [stride] + [1]*(num_blocks-1)\n",
        "    layers = []\n",
        "    for stride in strides:\n",
        "      layers.append(block(self.in_planes, planes, stride))\n",
        "      self.in_planes = planes * block.expansion\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.layer1(out)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out)\n",
        "    out = self.layer4(out)\n",
        "    out = F.avg_pool2d(out, 4)\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = self.linear(out)\n",
        "    return out\n",
        "\n",
        "ResNet18 = ResNet(BasicBlock, [2,2,2,2])\n",
        "#ResNet34 = ResNet(BasicBlock, [3,4,6,3])\n",
        "#ResNet50 = ResNet(BottleNeck, [3,4,6,3])\n",
        "#ResNet101 = ResNet(BottleNeck, [3,4,23,3])\n",
        "#ResNet152 = ResNet(BottleNeck, [3,8,36,3])\n",
        "\n",
        "import torch\n",
        "in_size = 32\n",
        "input = torch.randn(20,3,32,32)\n",
        "model = ResNet(BasicBlock, [2,2,2,2])\n",
        "print(model(input).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-71548825fccf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBasicBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mexpansion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_planes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplanes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_planes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplanes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if train_on_gpu:\n",
        "  ResNet18 = torch.nn.DataParallel(ResNet18)\n",
        "  cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "HzflAMLT0ic6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet9 in FP32"
      ],
      "metadata": {
        "id": "FEmAwNPnWb_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def identity(x: torch.Tensor) -> torch.Tensor:\n",
        "  return x\n",
        "\n",
        "class ConvBN(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, quant_func = identity):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn = nn.BatchNorm2d(out_channels)\n",
        "    self.quant = quant_func\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.quant(F.relu(self.bn(self.conv(x))))\n",
        "\n",
        "class Residual(nn.Module):\n",
        "  def __init__(self, channels):\n",
        "    super().__init__()\n",
        "    self.res1 = ConvBN(channels, channels)\n",
        "    self.res2 = ConvBN(channels, channels)\n",
        "\n",
        "  # downsampling not required when number of input and output channels are the same\n",
        "  def forward(self, x):\n",
        "    out = self.res2(self.res1(x))\n",
        "    out = out + x # identity shortcut connection\n",
        "    return out\n",
        "\n",
        "class Layer(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "    self.convbn = ConvBN(in_channels, out_channels)\n",
        "    self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.pool(self.convbn(x))\n",
        "\n",
        "class ResLayer(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "    self.layer = Layer(in_channels, out_channels)\n",
        "    self.residual = Residual(out_channels)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.residual(self.layer(x))\n",
        "\n",
        "class ResNet9(nn.Module):\n",
        "  def __init__(self, quant_func = identity, input_size=32, num_classes=10):\n",
        "    super().__init__()\n",
        "    self.prep = ConvBN(3, 2*input_size)\n",
        "    self.layer1 = ResLayer(2*input_size, 4*input_size)\n",
        "    self.layer2 = Layer(4*input_size, 8*input_size)\n",
        "    self.layer3 = ResLayer(8*input_size, 16*input_size)\n",
        "    #self.pool = nn.MaxPool2d(4)\n",
        "    #self.flatten = nn.Flatten()\n",
        "    self.linear = nn.Linear(16*input_size, 10, bias=False)\n",
        "    self.quant = quant_func\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.prep(x)\n",
        "    x = self.layer1(x)\n",
        "    x = self.quant(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.quant(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.quant(x)\n",
        "    x = F.avg_pool2d(x, 4)\n",
        "    x = self.quant(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.linear(x)\n",
        "    x = self.quant(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "soPB_QlBmArU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Total ResNet9 model parameters"
      ],
      "metadata": {
        "id": "sr5hckl0VN4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "in_size = 32\n",
        "input = torch.randn(20,3,in_size,in_size)\n",
        "model = ResNet9(input_size=in_size)\n",
        "q_model_fp8 = ResNet9(input_size=in_size, quant_func = e3m4_quantization)\n",
        "q_model_int12 = ResNet9(input_size=in_size, quant_func = lambda x: e3m4_quantization(x, fx_bits=12))\n",
        "q_model_int8 = ResNet9(input_size=in_size, quant_func = lambda x: e3m4_quantization(x, fx_bits=8))\n",
        "q_model_int4 = ResNet9(input_size=in_size, quant_func = lambda x: e3m4_quantization(x, fx_bits=6))\n",
        "q_model_maxE = ResNet9(input_size=in_size, quant_func = lambda x: e3m4_quantization(x, fx_bits=6, max_E=True))\n",
        "\n",
        "if train_on_gpu:\n",
        "  model = torch.nn.DataParallel(model)\n",
        "  q_model_fp8 = torch.nn.DataParallel(q_model_fp8)\n",
        "  q_model_int12 = torch.nn.DataParallel(q_model_int12)\n",
        "  q_model_int8 = torch.nn.DataParallel(q_model_int8)\n",
        "  q_model_int4 = torch.nn.DataParallel(q_model_int4)\n",
        "  q_model_maxE = torch.nn.DataParallel(q_model_maxE)\n",
        "  cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "dgL0sXMovZbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "model = ResNet9()\n",
        "print(model)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f'total parameters: {total_params}')\n",
        "'''"
      ],
      "metadata": {
        "id": "wlAlE-r1U-55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkqQ7fsUxA0r"
      },
      "source": [
        "#**Defining the Network Architecture**\n",
        "ResNet9 architecture is described in https://github.com/davidcpage/cifar10-fast/tree/master\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDAlhqzyyYaG"
      },
      "source": [
        "### **Specifying the Loss Function and Optimizer**\n",
        "We use CrossEntropyLoss as Loss function and\n",
        "\n",
        "[Stochastic Gradient Descent](https://leon.bottou.org/publications/pdf/compstat-2010.pdf) as Optimizer with momentum and weight decay specified by the research paper of ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlkXUNc-aVV9"
      },
      "source": [
        "import torch.optim as optim\n",
        "# specify loss function (categorical cross-entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# specify optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loading Model Parameters**"
      ],
      "metadata": {
        "id": "IX-F1WEgwQtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "save_dir = '/content/drive/My Drive/ResNet_data/'\n",
        "\n",
        "state_dict = torch.load(save_dir + 'ResNet9.pt') if train_on_gpu else torch.load(save_dir + 'ResNet9.pt', map_location=torch.device('cpu'))\n",
        "\n",
        "if state_dict:\n",
        "  if not train_on_gpu:\n",
        "    new_state_dict = OrderedDict()\n",
        "    for k,v in state_dict.items():\n",
        "      new_state_dict[k[7:]] = v\n",
        "    state_dict = new_state_dict"
      ],
      "metadata": {
        "id": "w6dEXUfyvfUl",
        "outputId": "d4baeabf-1640-4808-a98c-506c76d5127f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract model weight\n",
        "weights = {k: state_dict[k] for k in state_dict.keys() if k[-6:] == 'weight'}\n",
        "\n",
        "weights = { k: w.cpu().numpy() for k,w in weights.items() }\n",
        "\n",
        "np.savez_compressed(save_dir + 'ResNet9_weights', **weights)\n",
        "npzfile = np.load(save_dir + 'ResNet9_weights.npz')\n",
        "print(npzfile.files)\n",
        "npzfile['layer1.residual.res1.conv.weight'].shape"
      ],
      "metadata": {
        "id": "SHDH0rcwDL-4",
        "outputId": "255ac268-0ce3-451d-ace4-6340507ff4a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['prep.conv.weight', 'prep.bn.weight', 'layer1.layer.convbn.conv.weight', 'layer1.layer.convbn.bn.weight', 'layer1.residual.res1.conv.weight', 'layer1.residual.res1.bn.weight', 'layer1.residual.res2.conv.weight', 'layer1.residual.res2.bn.weight', 'layer2.convbn.conv.weight', 'layer2.convbn.bn.weight', 'layer3.layer.convbn.conv.weight', 'layer3.layer.convbn.bn.weight', 'layer3.residual.res1.conv.weight', 'layer3.residual.res1.bn.weight', 'layer3.residual.res2.conv.weight', 'layer3.residual.res2.bn.weight', 'linear.weight']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128, 128, 3, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### quantize weights"
      ],
      "metadata": {
        "id": "mV_O2sjBxEKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract model weight\n",
        "weights = {k: state_dict[k] for k in state_dict.keys() if k[-6:] == 'weight'}\n",
        "\n",
        "# per-tensor quantization\n",
        "weights_fp8 = {k: e3m4_quantization(w) for k,w in weights.items()}\n",
        "print('Quantizing 12-bit')\n",
        "weights_int12 = {k: e3m4_quantization(w,12) for k,w in weights.items()}\n",
        "print('Quantizing 8-bit')\n",
        "weights_int8 = {k: e3m4_quantization(w,8) for k,w in weights.items()}\n",
        "print('Quantizing 4-bit')\n",
        "weights_int4 = {k: e3m4_quantization(w,4) for k,w in weights.items()}\n",
        "print('Quantizing 4-bit maxE')\n",
        "weights_maxE = {k: e3m4_quantization(w,4,max_E=True) for k,w in weights.items()}\n",
        "\n",
        "state_dict_fp8 = {k: weights_fp8[k] if k in weights.keys() else state_dict[k] for k in state_dict.keys()}\n",
        "state_dict_int12 = {k: weights_int12[k] if k in weights.keys() else state_dict[k] for k in state_dict.keys()}\n",
        "state_dict_int8 = {k: weights_int8[k] if k in weights.keys() else state_dict[k] for k in state_dict.keys()}\n",
        "state_dict_int4 = {k: weights_int4[k] if k in weights.keys() else state_dict[k] for k in state_dict.keys()}\n",
        "state_dict_maxE = {k: weights_maxE[k] if k in weights.keys() else state_dict[k] for k in state_dict.keys()}"
      ],
      "metadata": {
        "id": "NOhI2r8OyNgx",
        "outputId": "f7a90f59-6150-4a00-f3c5-943b3e86a084",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantizing 12-bit\n",
            "Quantizing 8-bit\n",
            "Quantizing 4-bit\n",
            "Quantizing 4-bit maxE\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-98aafb45759a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mweights_int4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0me3m4_quantization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Quantizing 4-bit maxE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mweights_maxE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0me3m4_quantization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_E\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mstate_dict_fp8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mweights_fp8\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-98aafb45759a>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mweights_int4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0me3m4_quantization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Quantizing 4-bit maxE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mweights_maxE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0me3m4_quantization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_E\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mstate_dict_fp8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mweights_fp8\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-c42ad093a71a>\u001b[0m in \u001b[0;36me3m4_quantization\u001b[0;34m(x_float, fx_bits, max_E)\u001b[0m\n\u001b[1;32m     37\u001b[0m     '''\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmax_E\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m       \u001b[0mquantize_e3m4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround_e3m4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_maxE_quantization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquantize_e3m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbits_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfx_bits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mquantize_e3m4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround_e3m4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx_quantization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquantize_e3m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfx_bits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m                 \u001b[0;31m# Restore the dynamic layer stack depth if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-b87340db0628>\u001b[0m in \u001b[0;36mfp_maxE_quantization\u001b[0;34m(x, bits_to_keep, vector_size)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Extract 4 bits of mantissa starting two bits to the right of the first '1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mmantissa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mfp8_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msign\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_val\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmantissa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mq_sections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp8_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(*[torch.numel(v) for k,v in weights.items()])"
      ],
      "metadata": {
        "id": "06-mbLvq_CtH",
        "outputId": "413a3358-18e2-4229-a153-9b218f66e613",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1728 64 73728 128 147456 128 147456 128 294912 256 1179648 512 2359296 512 2359296 512 5120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dMWiYJf1Ou-"
      },
      "source": [
        "#**Training Loop**\n",
        "Here we train the architecture on training data and check its validation loss by using the validation set and saving the model only if there is an improvement ie decrease in the validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLPoZuGAcUM8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "5e660d98-5b07-4e51-f82e-adad1b54c219"
      },
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 20\n",
        "\n",
        "valid_loss_min = np.Inf # track change in validation loss\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "  # keep track of training and validation loss\n",
        "  train_loss = 0.0\n",
        "  valid_loss = 0.0\n",
        "\n",
        "  ###################\n",
        "  # train the model #\n",
        "  ###################\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if train_on_gpu:\n",
        "      data, target = data.cuda(), target.cuda()\n",
        "    # clear the gradients of all optimized variables\n",
        "    optimizer.zero_grad()\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = model(data)\n",
        "    # calculate the batch loss\n",
        "    loss = criterion(output, target)\n",
        "    # backward pass: compute gradient of the loss with respect to model parameters\n",
        "    loss.backward()\n",
        "    # perform a single optimization step (parameter update)\n",
        "    optimizer.step()\n",
        "    # update training loss\n",
        "    train_loss += loss.item()*data.size(0)\n",
        "\n",
        "  ######################\n",
        "  # validate the model #\n",
        "  ######################\n",
        "  model.eval()\n",
        "  for batch_idx, (data, target) in enumerate(valid_loader):\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if train_on_gpu:\n",
        "      data, target = data.cuda(), target.cuda()\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = model(data)\n",
        "    # calculate the batch loss\n",
        "    loss = criterion(output, target)\n",
        "    # update average validation loss\n",
        "    valid_loss += loss.item()*data.size(0)\n",
        "\n",
        "  # calculate average losses\n",
        "  train_loss = train_loss/len(train_loader.sampler)\n",
        "  valid_loss = valid_loss/len(valid_loader.sampler)\n",
        "\n",
        "  # print training/validation statistics\n",
        "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "      epoch, train_loss, valid_loss))\n",
        "\n",
        "  # save model if validation loss has decreased\n",
        "  if valid_loss <= valid_loss_min:\n",
        "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "    valid_loss_min,\n",
        "    valid_loss))\n",
        "    torch.save(model.state_dict(), 'ResNet9.pt')\n",
        "    valid_loss_min = valid_loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-79a7fb336026>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# update training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0;31m######################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1M4p3Is2RMg"
      },
      "source": [
        "#**Loading the Best Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "angPoGSTeykj"
      },
      "source": [
        "if not state_dict:\n",
        "  state_dict = torch.load(save_dir + 'ResNet9.pt')\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "q_model_fp8.load_state_dict(state_dict_fp8)\n",
        "q_model_int12.load_state_dict(state_dict_int12)\n",
        "q_model_int8.load_state_dict(state_dict_int8)\n",
        "q_model_int4.load_state_dict(state_dict_int4)\n",
        "q_model_maxE.load_state_dict(state_dict_maxE)\n",
        "\n",
        "torch.save(state_dict_maxE, save_dir + 'ResNet9_maxE.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_model_maxE.load_state_dict(torch.load(save_dir + 'ResNet9_maxE.pt'))"
      ],
      "metadata": {
        "id": "MbdPFqW24aZi",
        "outputId": "d08b3834-3a03-44b5-a401-4c104dacf630",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_lEna0p2Wp5"
      },
      "source": [
        "#**Testing Loop**\n",
        "The real test of the model architecture how well does the model recognizes the image and what is the accuracy on the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsqLF8SVfsnn"
      },
      "source": [
        "import os\n",
        "LINE_CLEAR = '\\x1b[2K'\n",
        "LINE_UP = '\\033[1A'\n",
        "\n",
        "def test(m):\n",
        "  # track test loss\n",
        "  if os.path.exists(save_dir + 'progress.npy'):\n",
        "    progress = np.load(save_dir + 'progress.npy')\n",
        "    starting_idx = progress[0]\n",
        "    test_loss = progress[1]\n",
        "    class_correct = np.load(save_dir + 'correct.npy')\n",
        "    class_total = np.load(save_dir + 'total.npy')\n",
        "  else:\n",
        "    starting_idx = -1\n",
        "    test_loss = 0.0\n",
        "    class_correct = list(0. for i in range(10))\n",
        "    class_total = list(0. for i in range(10))\n",
        "\n",
        "  if train_on_gpu:\n",
        "    m.cuda()\n",
        "\n",
        "  m.eval()\n",
        "  # iterate over test data\n",
        "  for batch_idx, (data, target) in enumerate(test_loader):\n",
        "    if batch_idx <= starting_idx:\n",
        "      continue\n",
        "    print(f'evaluating batch {batch_idx} out of {len(test_loader)}')\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if train_on_gpu:\n",
        "      data, target = data.cuda(), target.cuda()\n",
        "    data = data.clone().detach()\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    with torch.no_grad():\n",
        "      output = m(data).detach()\n",
        "    '''\n",
        "    if torch.isnan(output).any():\n",
        "      print(\"nan found\")\n",
        "    '''\n",
        "    # calculate the batch loss\n",
        "    loss = criterion(output, target)\n",
        "    # update test loss\n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)\n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(batch_size):\n",
        "      label = target.data[i]\n",
        "      class_correct[label] += correct[i].item()\n",
        "      class_total[label] += 1\n",
        "\n",
        "    progress = [batch_idx, test_loss]\n",
        "    np.save(save_dir + 'progress.npy', progress)\n",
        "    np.save(save_dir + 'correct.npy', class_correct)\n",
        "    np.save(save_dir + 'total.npy', class_total)\n",
        "    print(LINE_UP, end=LINE_CLEAR)\n",
        "    print(f'saved batch {batch_idx} out of {len(test_loader)}')\n",
        "  # average test loss\n",
        "  test_loss = test_loss/len(test_loader.dataset)\n",
        "  print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "  for i in range(10):\n",
        "    if class_total[i] > 0:\n",
        "      print('Test Accuracy of %5s: %.1f%% (%3d/%2d)' % (\n",
        "          classes[i], 100 * class_correct[i] / class_total[i],\n",
        "          np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "      print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "  print('\\nTest Accuracy (Overall): %.2f%% (%2d/%2d)' % (\n",
        "      100. * np.sum(class_correct) / np.sum(class_total),\n",
        "      np.sum(class_correct), np.sum(class_total)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(model)"
      ],
      "metadata": {
        "id": "CdSPO_q2dSua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(q_model_fp8)"
      ],
      "metadata": {
        "id": "JdExUuigdZLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(q_model_int12)"
      ],
      "metadata": {
        "id": "o7NmMfHUdvCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(q_model_int8)"
      ],
      "metadata": {
        "id": "yMuZg2_qd2nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(q_model_int4)"
      ],
      "metadata": {
        "id": "LOlXboqNd-Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# opt_model = torch.compile(q_model_maxE)\n",
        "test(q_model_maxE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95gO__0Nm0ur",
        "outputId": "1b7ebd47-b97b-4348-9403-4e87d29c2566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluating batch 0 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 0 out of500\n",
            "evaluating batch 1 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 1 out of500\n",
            "evaluating batch 2 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 2 out of500\n",
            "evaluating batch 3 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 3 out of500\n",
            "evaluating batch 4 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 4 out of500\n",
            "evaluating batch 5 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 5 out of500\n",
            "evaluating batch 6 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 6 out of500\n",
            "evaluating batch 7 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 7 out of500\n",
            "evaluating batch 8 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 8 out of500\n",
            "evaluating batch 9 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 9 out of500\n",
            "evaluating batch 10 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 10 out of500\n",
            "evaluating batch 11 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 11 out of500\n",
            "evaluating batch 12 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 12 out of500\n",
            "evaluating batch 13 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 13 out of500\n",
            "evaluating batch 14 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 14 out of500\n",
            "evaluating batch 15 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 15 out of500\n",
            "evaluating batch 16 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 16 out of500\n",
            "evaluating batch 17 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 17 out of500\n",
            "evaluating batch 18 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 18 out of500\n",
            "evaluating batch 19 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 19 out of500\n",
            "evaluating batch 20 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 20 out of500\n",
            "evaluating batch 21 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 21 out of500\n",
            "evaluating batch 22 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 22 out of500\n",
            "evaluating batch 23 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 23 out of500\n",
            "evaluating batch 24 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 24 out of500\n",
            "evaluating batch 25 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 25 out of500\n",
            "evaluating batch 26 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 26 out of500\n",
            "evaluating batch 27 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 27 out of500\n",
            "evaluating batch 28 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 28 out of500\n",
            "evaluating batch 29 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 29 out of500\n",
            "evaluating batch 30 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 30 out of500\n",
            "evaluating batch 31 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 31 out of500\n",
            "evaluating batch 32 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 32 out of500\n",
            "evaluating batch 33 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 33 out of500\n",
            "evaluating batch 34 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 34 out of500\n",
            "evaluating batch 35 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 35 out of500\n",
            "evaluating batch 36 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 36 out of500\n",
            "evaluating batch 37 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 37 out of500\n",
            "evaluating batch 38 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 38 out of500\n",
            "evaluating batch 39 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 39 out of500\n",
            "evaluating batch 40 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 40 out of500\n",
            "evaluating batch 41 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 41 out of500\n",
            "evaluating batch 42 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 42 out of500\n",
            "evaluating batch 43 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 43 out of500\n",
            "evaluating batch 44 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 44 out of500\n",
            "evaluating batch 45 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 45 out of500\n",
            "evaluating batch 46 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 46 out of500\n",
            "evaluating batch 47 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 47 out of500\n",
            "evaluating batch 48 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 48 out of500\n",
            "evaluating batch 49 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 49 out of500\n",
            "evaluating batch 50 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 50 out of500\n",
            "evaluating batch 51 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 51 out of500\n",
            "evaluating batch 52 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 52 out of500\n",
            "evaluating batch 53 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 53 out of500\n",
            "evaluating batch 54 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 54 out of500\n",
            "evaluating batch 55 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 55 out of500\n",
            "evaluating batch 56 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 56 out of500\n",
            "evaluating batch 57 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 57 out of500\n",
            "evaluating batch 58 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 58 out of500\n",
            "evaluating batch 59 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 59 out of500\n",
            "evaluating batch 60 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 60 out of500\n",
            "evaluating batch 61 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 61 out of500\n",
            "evaluating batch 62 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 62 out of500\n",
            "evaluating batch 63 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 63 out of500\n",
            "evaluating batch 64 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 64 out of500\n",
            "evaluating batch 65 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 65 out of500\n",
            "evaluating batch 66 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 66 out of500\n",
            "evaluating batch 67 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 67 out of500\n",
            "evaluating batch 68 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 68 out of500\n",
            "evaluating batch 69 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 69 out of500\n",
            "evaluating batch 70 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 70 out of500\n",
            "evaluating batch 71 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 71 out of500\n",
            "evaluating batch 72 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 72 out of500\n",
            "evaluating batch 73 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 73 out of500\n",
            "evaluating batch 74 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 74 out of500\n",
            "evaluating batch 75 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 75 out of500\n",
            "evaluating batch 76 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 76 out of500\n",
            "evaluating batch 77 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 77 out of500\n",
            "evaluating batch 78 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 78 out of500\n",
            "evaluating batch 79 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 79 out of500\n",
            "evaluating batch 80 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 80 out of500\n",
            "evaluating batch 81 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 81 out of500\n",
            "evaluating batch 82 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 82 out of500\n",
            "evaluating batch 83 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 83 out of500\n",
            "evaluating batch 84 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 84 out of500\n",
            "evaluating batch 85 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 85 out of500\n",
            "evaluating batch 86 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 86 out of500\n",
            "evaluating batch 87 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 87 out of500\n",
            "evaluating batch 88 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 88 out of500\n",
            "evaluating batch 89 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 89 out of500\n",
            "evaluating batch 90 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 90 out of500\n",
            "evaluating batch 91 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 91 out of500\n",
            "evaluating batch 92 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 92 out of500\n",
            "evaluating batch 93 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 93 out of500\n",
            "evaluating batch 94 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 94 out of500\n",
            "evaluating batch 95 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 95 out of500\n",
            "evaluating batch 96 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 96 out of500\n",
            "evaluating batch 97 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 97 out of500\n",
            "evaluating batch 98 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 98 out of500\n",
            "evaluating batch 99 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 99 out of500\n",
            "evaluating batch 100 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 100 out of500\n",
            "evaluating batch 101 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 101 out of500\n",
            "evaluating batch 102 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 102 out of500\n",
            "evaluating batch 103 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 103 out of500\n",
            "evaluating batch 104 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 104 out of500\n",
            "evaluating batch 105 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 105 out of500\n",
            "evaluating batch 106 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 106 out of500\n",
            "evaluating batch 107 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 107 out of500\n",
            "evaluating batch 108 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 108 out of500\n",
            "evaluating batch 109 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 109 out of500\n",
            "evaluating batch 110 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 110 out of500\n",
            "evaluating batch 111 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 111 out of500\n",
            "evaluating batch 112 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 112 out of500\n",
            "evaluating batch 113 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 113 out of500\n",
            "evaluating batch 114 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 114 out of500\n",
            "evaluating batch 115 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 115 out of500\n",
            "evaluating batch 116 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 116 out of500\n",
            "evaluating batch 117 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 117 out of500\n",
            "evaluating batch 118 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 118 out of500\n",
            "evaluating batch 119 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 119 out of500\n",
            "evaluating batch 120 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 120 out of500\n",
            "evaluating batch 121 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 121 out of500\n",
            "evaluating batch 122 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 122 out of500\n",
            "evaluating batch 123 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 123 out of500\n",
            "evaluating batch 124 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 124 out of500\n",
            "evaluating batch 125 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 125 out of500\n",
            "evaluating batch 126 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 126 out of500\n",
            "evaluating batch 127 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 127 out of500\n",
            "evaluating batch 128 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 128 out of500\n",
            "evaluating batch 129 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 129 out of500\n",
            "evaluating batch 130 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 130 out of500\n",
            "evaluating batch 131 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 131 out of500\n",
            "evaluating batch 132 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 132 out of500\n",
            "evaluating batch 133 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 133 out of500\n",
            "evaluating batch 134 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 134 out of500\n",
            "evaluating batch 135 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 135 out of500\n",
            "evaluating batch 136 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 136 out of500\n",
            "evaluating batch 137 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 137 out of500\n",
            "evaluating batch 138 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 138 out of500\n",
            "evaluating batch 139 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 139 out of500\n",
            "evaluating batch 140 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 140 out of500\n",
            "evaluating batch 141 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 141 out of500\n",
            "evaluating batch 142 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 142 out of500\n",
            "evaluating batch 143 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 143 out of500\n",
            "evaluating batch 144 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 144 out of500\n",
            "evaluating batch 145 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 145 out of500\n",
            "evaluating batch 146 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 146 out of500\n",
            "evaluating batch 147 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 147 out of500\n",
            "evaluating batch 148 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 148 out of500\n",
            "evaluating batch 149 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 149 out of500\n",
            "evaluating batch 150 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 150 out of500\n",
            "evaluating batch 151 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 151 out of500\n",
            "evaluating batch 152 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 152 out of500\n",
            "evaluating batch 153 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 153 out of500\n",
            "evaluating batch 154 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 154 out of500\n",
            "evaluating batch 155 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 155 out of500\n",
            "evaluating batch 156 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 156 out of500\n",
            "evaluating batch 157 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 157 out of500\n",
            "evaluating batch 158 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 158 out of500\n",
            "evaluating batch 159 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 159 out of500\n",
            "evaluating batch 160 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 160 out of500\n",
            "evaluating batch 161 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 161 out of500\n",
            "evaluating batch 162 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 162 out of500\n",
            "evaluating batch 163 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 163 out of500\n",
            "evaluating batch 164 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 164 out of500\n",
            "evaluating batch 165 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 165 out of500\n",
            "evaluating batch 166 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 166 out of500\n",
            "evaluating batch 167 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 167 out of500\n",
            "evaluating batch 168 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 168 out of500\n",
            "evaluating batch 169 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 169 out of500\n",
            "evaluating batch 170 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 170 out of500\n",
            "evaluating batch 171 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 171 out of500\n",
            "evaluating batch 172 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 172 out of500\n",
            "evaluating batch 173 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 173 out of500\n",
            "evaluating batch 174 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 174 out of500\n",
            "evaluating batch 175 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 175 out of500\n",
            "evaluating batch 176 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 176 out of500\n",
            "evaluating batch 177 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 177 out of500\n",
            "evaluating batch 178 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 178 out of500\n",
            "evaluating batch 179 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 179 out of500\n",
            "evaluating batch 180 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 180 out of500\n",
            "evaluating batch 181 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 181 out of500\n",
            "evaluating batch 182 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 182 out of500\n",
            "evaluating batch 183 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 183 out of500\n",
            "evaluating batch 184 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 184 out of500\n",
            "evaluating batch 185 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 185 out of500\n",
            "evaluating batch 186 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 186 out of500\n",
            "evaluating batch 187 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 187 out of500\n",
            "evaluating batch 188 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 188 out of500\n",
            "evaluating batch 189 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 189 out of500\n",
            "evaluating batch 190 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 190 out of500\n",
            "evaluating batch 191 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 191 out of500\n",
            "evaluating batch 192 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 192 out of500\n",
            "evaluating batch 193 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 193 out of500\n",
            "evaluating batch 194 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 194 out of500\n",
            "evaluating batch 195 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 195 out of500\n",
            "evaluating batch 196 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 196 out of500\n",
            "evaluating batch 197 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 197 out of500\n",
            "evaluating batch 198 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 198 out of500\n",
            "evaluating batch 199 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 199 out of500\n",
            "evaluating batch 200 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 200 out of500\n",
            "evaluating batch 201 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 201 out of500\n",
            "evaluating batch 202 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 202 out of500\n",
            "evaluating batch 203 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 203 out of500\n",
            "evaluating batch 204 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 204 out of500\n",
            "evaluating batch 205 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 205 out of500\n",
            "evaluating batch 206 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 206 out of500\n",
            "evaluating batch 207 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 207 out of500\n",
            "evaluating batch 208 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 208 out of500\n",
            "evaluating batch 209 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 209 out of500\n",
            "evaluating batch 210 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 210 out of500\n",
            "evaluating batch 211 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 211 out of500\n",
            "evaluating batch 212 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 212 out of500\n",
            "evaluating batch 213 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 213 out of500\n",
            "evaluating batch 214 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 214 out of500\n",
            "evaluating batch 215 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 215 out of500\n",
            "evaluating batch 216 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 216 out of500\n",
            "evaluating batch 217 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 217 out of500\n",
            "evaluating batch 218 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 218 out of500\n",
            "evaluating batch 219 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 219 out of500\n",
            "evaluating batch 220 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 220 out of500\n",
            "evaluating batch 221 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 221 out of500\n",
            "evaluating batch 222 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 222 out of500\n",
            "evaluating batch 223 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 223 out of500\n",
            "evaluating batch 224 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 224 out of500\n",
            "evaluating batch 225 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 225 out of500\n",
            "evaluating batch 226 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 226 out of500\n",
            "evaluating batch 227 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 227 out of500\n",
            "evaluating batch 228 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 228 out of500\n",
            "evaluating batch 229 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 229 out of500\n",
            "evaluating batch 230 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 230 out of500\n",
            "evaluating batch 231 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 231 out of500\n",
            "evaluating batch 232 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 232 out of500\n",
            "evaluating batch 233 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 233 out of500\n",
            "evaluating batch 234 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 234 out of500\n",
            "evaluating batch 235 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 235 out of500\n",
            "evaluating batch 236 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 236 out of500\n",
            "evaluating batch 237 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 237 out of500\n",
            "evaluating batch 238 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 238 out of500\n",
            "evaluating batch 239 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 239 out of500\n",
            "evaluating batch 240 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 240 out of500\n",
            "evaluating batch 241 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 241 out of500\n",
            "evaluating batch 242 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 242 out of500\n",
            "evaluating batch 243 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 243 out of500\n",
            "evaluating batch 244 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 244 out of500\n",
            "evaluating batch 245 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 245 out of500\n",
            "evaluating batch 246 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 246 out of500\n",
            "evaluating batch 247 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 247 out of500\n",
            "evaluating batch 248 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 248 out of500\n",
            "evaluating batch 249 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 249 out of500\n",
            "evaluating batch 250 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 250 out of500\n",
            "evaluating batch 251 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 251 out of500\n",
            "evaluating batch 252 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 252 out of500\n",
            "evaluating batch 253 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 253 out of500\n",
            "evaluating batch 254 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 254 out of500\n",
            "evaluating batch 255 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 255 out of500\n",
            "evaluating batch 256 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 256 out of500\n",
            "evaluating batch 257 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 257 out of500\n",
            "evaluating batch 258 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 258 out of500\n",
            "evaluating batch 259 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 259 out of500\n",
            "evaluating batch 260 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 260 out of500\n",
            "evaluating batch 261 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 261 out of500\n",
            "evaluating batch 262 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 262 out of500\n",
            "evaluating batch 263 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 263 out of500\n",
            "evaluating batch 264 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 264 out of500\n",
            "evaluating batch 265 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 265 out of500\n",
            "evaluating batch 266 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 266 out of500\n",
            "evaluating batch 267 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 267 out of500\n",
            "evaluating batch 268 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 268 out of500\n",
            "evaluating batch 269 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 269 out of500\n",
            "evaluating batch 270 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 270 out of500\n",
            "evaluating batch 271 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 271 out of500\n",
            "evaluating batch 272 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 272 out of500\n",
            "evaluating batch 273 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 273 out of500\n",
            "evaluating batch 274 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 274 out of500\n",
            "evaluating batch 275 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 275 out of500\n",
            "evaluating batch 276 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 276 out of500\n",
            "evaluating batch 277 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 277 out of500\n",
            "evaluating batch 278 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 278 out of500\n",
            "evaluating batch 279 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 279 out of500\n",
            "evaluating batch 280 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 280 out of500\n",
            "evaluating batch 281 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 281 out of500\n",
            "evaluating batch 282 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 282 out of500\n",
            "evaluating batch 283 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 283 out of500\n",
            "evaluating batch 284 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 284 out of500\n",
            "evaluating batch 285 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 285 out of500\n",
            "evaluating batch 286 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 286 out of500\n",
            "evaluating batch 287 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 287 out of500\n",
            "evaluating batch 288 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 288 out of500\n",
            "evaluating batch 289 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 289 out of500\n",
            "evaluating batch 290 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 290 out of500\n",
            "evaluating batch 291 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 291 out of500\n",
            "evaluating batch 292 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 292 out of500\n",
            "evaluating batch 293 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 293 out of500\n",
            "evaluating batch 294 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 294 out of500\n",
            "evaluating batch 295 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 295 out of500\n",
            "evaluating batch 296 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 296 out of500\n",
            "evaluating batch 297 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 297 out of500\n",
            "evaluating batch 298 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 298 out of500\n",
            "evaluating batch 299 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 299 out of500\n",
            "evaluating batch 300 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 300 out of500\n",
            "evaluating batch 301 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 301 out of500\n",
            "evaluating batch 302 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 302 out of500\n",
            "evaluating batch 303 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 303 out of500\n",
            "evaluating batch 304 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 304 out of500\n",
            "evaluating batch 305 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 305 out of500\n",
            "evaluating batch 306 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 306 out of500\n",
            "evaluating batch 307 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 307 out of500\n",
            "evaluating batch 308 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 308 out of500\n",
            "evaluating batch 309 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 309 out of500\n",
            "evaluating batch 310 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 310 out of500\n",
            "evaluating batch 311 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 311 out of500\n",
            "evaluating batch 312 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 312 out of500\n",
            "evaluating batch 313 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 313 out of500\n",
            "evaluating batch 314 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 314 out of500\n",
            "evaluating batch 315 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 315 out of500\n",
            "evaluating batch 316 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 316 out of500\n",
            "evaluating batch 317 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 317 out of500\n",
            "evaluating batch 318 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 318 out of500\n",
            "evaluating batch 319 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 319 out of500\n",
            "evaluating batch 320 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 320 out of500\n",
            "evaluating batch 321 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 321 out of500\n",
            "evaluating batch 322 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 322 out of500\n",
            "evaluating batch 323 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 323 out of500\n",
            "evaluating batch 324 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 324 out of500\n",
            "evaluating batch 325 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 325 out of500\n",
            "evaluating batch 326 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 326 out of500\n",
            "evaluating batch 327 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 327 out of500\n",
            "evaluating batch 328 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 328 out of500\n",
            "evaluating batch 329 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 329 out of500\n",
            "evaluating batch 330 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 330 out of500\n",
            "evaluating batch 331 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 331 out of500\n",
            "evaluating batch 332 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 332 out of500\n",
            "evaluating batch 333 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 333 out of500\n",
            "evaluating batch 334 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 334 out of500\n",
            "evaluating batch 335 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 335 out of500\n",
            "evaluating batch 336 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 336 out of500\n",
            "evaluating batch 337 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 337 out of500\n",
            "evaluating batch 338 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 338 out of500\n",
            "evaluating batch 339 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 339 out of500\n",
            "evaluating batch 340 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 340 out of500\n",
            "evaluating batch 341 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 341 out of500\n",
            "evaluating batch 342 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 342 out of500\n",
            "evaluating batch 343 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 343 out of500\n",
            "evaluating batch 344 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 344 out of500\n",
            "evaluating batch 345 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 345 out of500\n",
            "evaluating batch 346 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 346 out of500\n",
            "evaluating batch 347 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 347 out of500\n",
            "evaluating batch 348 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 348 out of500\n",
            "evaluating batch 349 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 349 out of500\n",
            "evaluating batch 350 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 350 out of500\n",
            "evaluating batch 351 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 351 out of500\n",
            "evaluating batch 352 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 352 out of500\n",
            "evaluating batch 353 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 353 out of500\n",
            "evaluating batch 354 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 354 out of500\n",
            "evaluating batch 355 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 355 out of500\n",
            "evaluating batch 356 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 356 out of500\n",
            "evaluating batch 357 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 357 out of500\n",
            "evaluating batch 358 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 358 out of500\n",
            "evaluating batch 359 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 359 out of500\n",
            "evaluating batch 360 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 360 out of500\n",
            "evaluating batch 361 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 361 out of500\n",
            "evaluating batch 362 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 362 out of500\n",
            "evaluating batch 363 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 363 out of500\n",
            "evaluating batch 364 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 364 out of500\n",
            "evaluating batch 365 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 365 out of500\n",
            "evaluating batch 366 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 366 out of500\n",
            "evaluating batch 367 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 367 out of500\n",
            "evaluating batch 368 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 368 out of500\n",
            "evaluating batch 369 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 369 out of500\n",
            "evaluating batch 370 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 370 out of500\n",
            "evaluating batch 371 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 371 out of500\n",
            "evaluating batch 372 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 372 out of500\n",
            "evaluating batch 373 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 373 out of500\n",
            "evaluating batch 374 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 374 out of500\n",
            "evaluating batch 375 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 375 out of500\n",
            "evaluating batch 376 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 376 out of500\n",
            "evaluating batch 377 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 377 out of500\n",
            "evaluating batch 378 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 378 out of500\n",
            "evaluating batch 379 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 379 out of500\n",
            "evaluating batch 380 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 380 out of500\n",
            "evaluating batch 381 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 381 out of500\n",
            "evaluating batch 382 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 382 out of500\n",
            "evaluating batch 383 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 383 out of500\n",
            "evaluating batch 384 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 384 out of500\n",
            "evaluating batch 385 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 385 out of500\n",
            "evaluating batch 386 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 386 out of500\n",
            "evaluating batch 387 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 387 out of500\n",
            "evaluating batch 388 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 388 out of500\n",
            "evaluating batch 389 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 389 out of500\n",
            "evaluating batch 390 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 390 out of500\n",
            "evaluating batch 391 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 391 out of500\n",
            "evaluating batch 392 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 392 out of500\n",
            "evaluating batch 393 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 393 out of500\n",
            "evaluating batch 394 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 394 out of500\n",
            "evaluating batch 395 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 395 out of500\n",
            "evaluating batch 396 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 396 out of500\n",
            "evaluating batch 397 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 397 out of500\n",
            "evaluating batch 398 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 398 out of500\n",
            "evaluating batch 399 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 399 out of500\n",
            "evaluating batch 400 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 400 out of500\n",
            "evaluating batch 401 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 401 out of500\n",
            "evaluating batch 402 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 402 out of500\n",
            "evaluating batch 403 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 403 out of500\n",
            "evaluating batch 404 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 404 out of500\n",
            "evaluating batch 405 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 405 out of500\n",
            "evaluating batch 406 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 406 out of500\n",
            "evaluating batch 407 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 407 out of500\n",
            "evaluating batch 408 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 408 out of500\n",
            "evaluating batch 409 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 409 out of500\n",
            "evaluating batch 410 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 410 out of500\n",
            "evaluating batch 411 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 411 out of500\n",
            "evaluating batch 412 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 412 out of500\n",
            "evaluating batch 413 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 413 out of500\n",
            "evaluating batch 414 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 414 out of500\n",
            "evaluating batch 415 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 415 out of500\n",
            "evaluating batch 416 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 416 out of500\n",
            "evaluating batch 417 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 417 out of500\n",
            "evaluating batch 418 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 418 out of500\n",
            "evaluating batch 419 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 419 out of500\n",
            "evaluating batch 420 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 420 out of500\n",
            "evaluating batch 421 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 421 out of500\n",
            "evaluating batch 422 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 422 out of500\n",
            "evaluating batch 423 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 423 out of500\n",
            "evaluating batch 424 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 424 out of500\n",
            "evaluating batch 425 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 425 out of500\n",
            "evaluating batch 426 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 426 out of500\n",
            "evaluating batch 427 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 427 out of500\n",
            "evaluating batch 428 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 428 out of500\n",
            "evaluating batch 429 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 429 out of500\n",
            "evaluating batch 430 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 430 out of500\n",
            "evaluating batch 431 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 431 out of500\n",
            "evaluating batch 432 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 432 out of500\n",
            "evaluating batch 433 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 433 out of500\n",
            "evaluating batch 434 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 434 out of500\n",
            "evaluating batch 435 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 435 out of500\n",
            "evaluating batch 436 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 436 out of500\n",
            "evaluating batch 437 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 437 out of500\n",
            "evaluating batch 438 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 438 out of500\n",
            "evaluating batch 439 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 439 out of500\n",
            "evaluating batch 440 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 440 out of500\n",
            "evaluating batch 441 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 441 out of500\n",
            "evaluating batch 442 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 442 out of500\n",
            "evaluating batch 443 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 443 out of500\n",
            "evaluating batch 444 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 444 out of500\n",
            "evaluating batch 445 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 445 out of500\n",
            "evaluating batch 446 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 446 out of500\n",
            "evaluating batch 447 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 447 out of500\n",
            "evaluating batch 448 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 448 out of500\n",
            "evaluating batch 449 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 449 out of500\n",
            "evaluating batch 450 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 450 out of500\n",
            "evaluating batch 451 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 451 out of500\n",
            "evaluating batch 452 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 452 out of500\n",
            "evaluating batch 453 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 453 out of500\n",
            "evaluating batch 454 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 454 out of500\n",
            "evaluating batch 455 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 455 out of500\n",
            "evaluating batch 456 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 456 out of500\n",
            "evaluating batch 457 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 457 out of500\n",
            "evaluating batch 458 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 458 out of500\n",
            "evaluating batch 459 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 459 out of500\n",
            "evaluating batch 460 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 460 out of500\n",
            "evaluating batch 461 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 461 out of500\n",
            "evaluating batch 462 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 462 out of500\n",
            "evaluating batch 463 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 463 out of500\n",
            "evaluating batch 464 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 464 out of500\n",
            "evaluating batch 465 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 465 out of500\n",
            "evaluating batch 466 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 466 out of500\n",
            "evaluating batch 467 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 467 out of500\n",
            "evaluating batch 468 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 468 out of500\n",
            "evaluating batch 469 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 469 out of500\n",
            "evaluating batch 470 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 470 out of500\n",
            "evaluating batch 471 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 471 out of500\n",
            "evaluating batch 472 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 472 out of500\n",
            "evaluating batch 473 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 473 out of500\n",
            "evaluating batch 474 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 474 out of500\n",
            "evaluating batch 475 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 475 out of500\n",
            "evaluating batch 476 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 476 out of500\n",
            "evaluating batch 477 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 477 out of500\n",
            "evaluating batch 478 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 478 out of500\n",
            "evaluating batch 479 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 479 out of500\n",
            "evaluating batch 480 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 480 out of500\n",
            "evaluating batch 481 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 481 out of500\n",
            "evaluating batch 482 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 482 out of500\n",
            "evaluating batch 483 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 483 out of500\n",
            "evaluating batch 484 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 484 out of500\n",
            "evaluating batch 485 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 485 out of500\n",
            "evaluating batch 486 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 486 out of500\n",
            "evaluating batch 487 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 487 out of500\n",
            "evaluating batch 488 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 488 out of500\n",
            "evaluating batch 489 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 489 out of500\n",
            "evaluating batch 490 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 490 out of500\n",
            "evaluating batch 491 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 491 out of500\n",
            "evaluating batch 492 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 492 out of500\n",
            "evaluating batch 493 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 493 out of500\n",
            "evaluating batch 494 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 494 out of500\n",
            "evaluating batch 495 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 495 out of500\n",
            "evaluating batch 496 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 496 out of500\n",
            "evaluating batch 497 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 497 out of500\n",
            "evaluating batch 498 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 498 out of500\n",
            "evaluating batch 499 out of 500\n",
            "\u001b[1A\u001b[2Ksaved batch 499 out of500\n",
            "Test Loss: 0.366352\n",
            "\n",
            "Test Accuracy of airplane: 92.6% (926/1000)\n",
            "Test Accuracy of automobile: 94.1% (941/1000)\n",
            "Test Accuracy of  bird: 83.0% (830/1000)\n",
            "Test Accuracy of   cat: 79.4% (794/1000)\n",
            "Test Accuracy of  deer: 89.3% (893/1000)\n",
            "Test Accuracy of   dog: 87.0% (870/1000)\n",
            "Test Accuracy of  frog: 90.3% (903/1000)\n",
            "Test Accuracy of horse: 88.4% (884/1000)\n",
            "Test Accuracy of  ship: 93.1% (931/1000)\n",
            "Test Accuracy of truck: 91.1% (911/1000)\n",
            "\n",
            "Test Accuracy (Overall): 88.83% (8883/10000)\n"
          ]
        }
      ]
    }
  ]
}